{"searchDocs":[{"title":"Resume Help & Guidance","type":0,"sectionRef":"#","url":"/ar/additional_resources/resume-help","content":"","keywords":"","version":"Next"},{"title":"Resourcesâ€‹","type":1,"pageTitle":"Resume Help & Guidance","url":"/ar/additional_resources/resume-help#resources","content":" Name\tLink\tDescriptionCareer Coaching &amp; Guidance (TechTual Consulting)\tLink\tTechTual Consulting is a career coaching firm that helps individuals break into and grow within the cybersecurity field through personalized guidance, resume reviews, courses, and community content. My DevSecOps &amp; Cloud Security Resume\thttps://youtu.be/TQgnBd10T0A\tResume tips and overview for Cloud Security Engineering and DevSecOps Resume Template - With Summary\tResume Template with Summary\tA resume template (docx) with a summary field associated with it - used by Damien Resume Template - Without Summary\tResume Template w/o Summary\tA resume template without (docx) a summary field associated with it - used by Damien Knock Em Dead Resumes\thttps://amzn.to/4i0lqso\tBook about writing resumes Knock Em Dead Cover Letters\thttps://amzn.to/41gjKFk\tBook about writing cover letters Cloud Resume Example (AWS)\thttps://github.com/TerminalsandCoffee/S3Resume\tA great example of a resume hosted in AWS S3 by one of the community members, for those interested in the cloud route. ","version":"Next","tagName":"h2"},{"title":"The Blueprint","type":0,"sectionRef":"#","url":"/ar/blueprint/","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"The Blueprint","url":"/ar/blueprint/#overview","content":" The Blueprint is divided into two major sections that build on each other:  Section\tDescriptionSoft Skills\tLearn how to effectively work with people, talk to them, and understand the importance of teamwork and effective communication. DevSecOps\tLearn the fundamentals of integrating security within the software development lifecycle (SDLC). Youâ€™ll explore topics like application security, CI/CD, SAST/DAST testing, and the principles of shifting security left. Cloud Security Development\tLearn how to build and secure applications within the cloud. Youâ€™ll explore identity and access management (IAM), secrets management, logging and events, APIs, serverless automation, and secure Infrastructure as Code (IaC).  Together, these two areas make up the core learning foundation for anyone looking to grow into a modern cloud security or DevSecOps role.  ","version":"Next","tagName":"h2"},{"title":"Why The Blueprint Existsâ€‹","type":1,"pageTitle":"The Blueprint","url":"/ar/blueprint/#why-the-blueprint-exists","content":" Thereâ€™s a lot of noise out there when it comes to DevSecOps and Cloud Security tools, frameworks, certifications, acronymsâ€¦ the list goes on. This Blueprint cuts through that noise, and focuses on what actually matters:  Understanding the core principles first.Learning how and why security fits into development.Building a foundation that makes every future project easier to understand.  Think of this as your on-ramp into everything else that follows, or rather the theory before the practice.  ","version":"Next","tagName":"h2"},{"title":"How to Navigateâ€‹","type":1,"pageTitle":"The Blueprint","url":"/ar/blueprint/#how-to-navigate","content":" If youâ€™re new here, start from the top:  Begin with Introduction to Soft Skills: Learn why it's important to be an effective collaborator and communicator.Then move into DevSecOps: Learn the mindset, the process, and how security fits into development.Then move into Cloud Security Development: Learn how those same principles apply in cloud environments.Apply what you learn in Projects: Get hands-on experience building pipelines, automation, and cloud-native security tools.  Each page builds on the last, introducing concepts gradually and with real-world examples to make learning easier and practical.  ","version":"Next","tagName":"h2"},{"title":"Who This Is Forâ€‹","type":1,"pageTitle":"The Blueprint","url":"/ar/blueprint/#who-this-is-for","content":" This section is for:  Developers who want to write secure code.Security engineers learning how to automate in CI/CD.Cloud engineers who want to understand secure design principles.Anyone exploring DevSecOps or Cloud Security as a career path.  ","version":"Next","tagName":"h2"},{"title":"What Youâ€™ll Gainâ€‹","type":1,"pageTitle":"The Blueprint","url":"/ar/blueprint/#what-youll-gain","content":" By the end of this section, youâ€™ll be able to:  Explain how DevSecOps integrates security into modern software development.Understand how cloud environments are structured and secured.Recognize key concepts like IAM, Secrets, Logging, and IaC.Build a mental model for secure development prior to touching a single tool.  ","version":"Next","tagName":"h2"},{"title":"The Big Pictureâ€‹","type":1,"pageTitle":"The Blueprint","url":"/ar/blueprint/#the-big-picture","content":" The Blueprint is the foundation that ties everything together:  The Soft Skills explains why you need to know how to talk and work with people.The DevSecOps section shows you how to integrate security.The Cloud Security Development section shows you how to build securely.The Projects section lets you put it all into practice.  Every part of this learning journey connects back to this. Why you may ask? Because security isnâ€™t a single tool or step. Itâ€™s a mindset, a process, and a way of building.  Ready to get started? Click the next page button on the bottom right hand side of your screen. ","version":"Next","tagName":"h2"},{"title":"Serverless and Orchestration","type":0,"sectionRef":"#","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#overview","content":" So, what is Serverless?  According to AWS, serverless computing allows you to build and run applications and services without thinking about servers. The cloud provider automatically provisions, scales, and manages the infrastructure required to run your code.  In simple terms, you write the function, and the cloud runs it.  For security engineers, this is a breakthrough. It means you can automate security actions quickly, cost-effectively, and reliably which are all triggered by real-time events.  Orchestration, on the other hand, coordinates multiple serverless functions or workflows into a single automated process. Think of it as a conductor managing a symphony of security automation.  Together, serverless and orchestration enable event-driven, continuous, and scalable security.  Ù…Ù„Ø§Ø­Ø¸Ø© Serverless automation turns detection into action, thus reducing response time from hours to seconds.  ","version":"Next","tagName":"h2"},{"title":"Why Serverless and Orchestration Matter for Securityâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#why-serverless-and-orchestration-matter-for-security","content":" Modern cloud environments generate thousands of changes every hour â€” new resources, policy updates, and access attempts. Manually investigating each one isnâ€™t sustainable. Serverless automation and orchestration workflows make it possible to:  React in Real Time: Trigger actions instantly through events and webhooks.Automate Remediation: Detect and fix issues like public S3 buckets or open ports automatically.Enforce Compliance Continuously: Check configurations against benchmarks such as CIS or NIST.Reduce Human Error: Codify standard procedures into reusable automation.Scale Effortlessly: No infrastructure to manage â€” functions scale automatically.  In short, these technologies transform cloud security from reactive to proactive automation.  ","version":"Next","tagName":"h2"},{"title":"The Serverless Security Lifecycleâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#the-serverless-security-lifecycle","content":" Like other security disciplines, serverless automation follows a lifecycle: Trigger â†’ Execute â†’ Orchestrate â†’ Monitor â†’ Improve.  ","version":"Next","tagName":"h2"},{"title":"1. Triggerâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#1-trigger","content":" A cloud event occurs, such as a resource being created, a policy changing, or a vulnerability being detected.  ","version":"Next","tagName":"h3"},{"title":"2. Executeâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#2-execute","content":" A serverless function (Lambda, Cloud Function, or Azure Function) runs code in response. Some examples are: tagging resources, revoking access, or sending alerts.  ","version":"Next","tagName":"h3"},{"title":"3. Orchestrateâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#3-orchestrate","content":" If multiple actions are required, orchestration services like AWS Step Functions, Azure Logic Apps, or GCP Workflows connect functions together into structured processes.  ","version":"Next","tagName":"h3"},{"title":"4. Monitorâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#4-monitor","content":" Each functionâ€™s activity is logged, monitored, and analyzed for performance and errors.  ","version":"Next","tagName":"h3"},{"title":"5. Improveâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#5-improve","content":" Metrics and alerts drive iteration â€” automation evolves with new risks and requirements.  ØªÙ„Ù…ÙŠØ­ Start small. Automate one task, like tagging untagged resources, before expanding to full workflows.  ","version":"Next","tagName":"h3"},{"title":"Core Conceptsâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#core-concepts","content":" ","version":"Next","tagName":"h2"},{"title":"Event-Driven Architectureâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#event-driven-architecture","content":" Serverless systems are built on events, and any action in the cloud can be a trigger.  Event Source\tExample Use CaseStorage Events\tScan uploaded files for sensitive data or malware. IAM Events\tDetect creation of risky roles or permissions. Compute Events\tQuarantine instances launched in unapproved networks. Security Alerts\tTrigger custom workflows when a vulnerability is detected.  ","version":"Next","tagName":"h3"},{"title":"Functions as a Service (FaaS)â€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#functions-as-a-service-faas","content":" Cloud Provider\tService\tPurposeAWS\tLambda\tEvent-driven compute integrated with S3, CloudWatch, and EventBridge. Azure\tFunctions\tRun code in response to HTTP requests or platform events. Google Cloud\tCloud Functions\tLightweight compute for processing cloud events and automation.  Functions handle one clear responsibility: act on an event quickly and securely.  ","version":"Next","tagName":"h3"},{"title":"Workflow Orchestrationâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#workflow-orchestration","content":" Cloud Provider\tService\tPurposeAWS\tStep Functions\tCombine multiple Lambdas into stateful workflows. Azure\tLogic Apps / Durable Functions\tChain actions and apply conditional logic for automation. Google Cloud\tWorkflows\tCoordinate multi-step processes across GCP services.  Example: Detect a public S3 bucket â†’ Remove public access â†’ Notify the security team â†’ Record results in a log. Thatâ€™s serverless orchestration in action.  ","version":"Next","tagName":"h3"},{"title":"Common Use Casesâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#common-use-cases","content":" Auto-Remediation: Fix misconfigurations automatically (for example, close open ports).Incident Response: Isolate compromised workloads or disable IAM keys instantly.Compliance Enforcement: Continuously validate configurations against policy-as-code frameworks.Threat Intelligence: Ingest feeds from EventBridge, Pub/Sub, or external APIs for analysis.Alert Routing: Forward findings to Slack, Teams, or PagerDuty automatically.Data Sanitization: Scan uploaded files for sensitive or malicious content.  Serverless automation becomes the hands of your security team.  ","version":"Next","tagName":"h2"},{"title":"Common Security Risksâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#common-security-risks","content":" Even though serverless removes infrastructure overhead, youâ€™re still responsible for securing your code and configuration.  Risk\tDescriptionOverprivileged Roles\tFunctions granted excessive IAM permissions. Unvalidated Input\tUnsanitized event payloads leading to injection or privilege escalation. Leaked Secrets\tEnvironment variables or logs exposing credentials. Insecure Dependencies\tUsing outdated or unpatched libraries in your functions. Silent Failures\tMissing error handling that hides failed remediations. Unmonitored Activity\tNo alerts or metrics tracking function performance and anomalies.  important Serverless doesnâ€™t remove responsibility. Instead, it shifts it closer to your code. You own the function logic and its security.  ","version":"Next","tagName":"h2"},{"title":"Best Practices for Secure Serverless and Orchestrationâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#best-practices-for-secure-serverless-and-orchestration","content":" Apply Least Privilege Grant functions only the permissions they need to perform their job. Validate Inputs Sanitize and verify all incoming event payloads. Use Secrets Managers Retrieve credentials dynamically from services like Secrets Manager, Key Vault, or Vault. Enable Full Logging Log invocations, errors, and security actions to your providerâ€™s monitoring service. Version and Tag Functions Use version control for rollbacks and traceability. Add Observability Monitor function duration, concurrency, and error rates. Leverage Dead Letter Queues (DLQs) Capture failed invocations for later investigation. Integrate with Orchestration Tools Build workflows that combine multiple automated security actions.  ","version":"Next","tagName":"h2"},{"title":"Practice What Youâ€™ve Learnedâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#practice-what-youve-learned","content":" Letâ€™s put this into action with a practical mini capstone.  ","version":"Next","tagName":"h2"},{"title":"Goalâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#goal","content":" Build a serverless security function that detects and responds to a cloud misconfiguration automatically.  ","version":"Next","tagName":"h3"},{"title":"Tasksâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#tasks","content":" Choose an event source â€” for example, S3 bucket creation or IAM role update.Write a function (Lambda, Azure Function, or Cloud Function) that: Parses event data.Validates the input.Takes action (for example, removes public access, tags noncompliant resources, or sends a notification). Secure the function using least privilege roles and dynamic secrets.Add orchestration (optional): Use Step Functions, Logic Apps, or Workflows to chain multiple automations (for example, remediation + alerting).  âœ… Capstone Goal: Demonstrate real-time detection and automated response to a cloud security event using serverless automation.  ØªÙ„Ù…ÙŠØ­ Add a secondary function that notifies your team when automation triggers.Observability is key to building trust in automation.  ","version":"Next","tagName":"h3"},{"title":"Recommended Resourcesâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#recommended-resources","content":" ","version":"Next","tagName":"h2"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantAWS Certified Security â€“ Specialty\tAWS\tIncludes Lambda-based automation and event-driven security. Google Professional Cloud Security Engineer\tGoogle Cloud\tFocuses on automation through Pub/Sub and Cloud Functions. Microsoft Certified: Azure Security Engineer Associate\tMicrosoft\tCovers Logic Apps, Functions, and secure orchestration patterns. Certified DevSecOps Professional (CDP)\tPractical DevSecOps\tDemonstrates real-world automation of detection and response workflows.  ","version":"Next","tagName":"h3"},{"title":"ðŸ“š Booksâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#-books","content":" Book Title\tAuthor\tLink\tWhy Itâ€™s UsefulLearning Serverless Security: Hacking and Securing Serverless Cloud Applications on AWS, Azure, and GCP\tJoshua Arvin Lat\tAmazon\tProvides hands-on guidance for identifying, exploiting, and defending against common serverless security risks across major cloud platforms.  ","version":"Next","tagName":"h3"},{"title":"ðŸŽ¥ Videosâ€‹","type":1,"pageTitle":"Serverless and Orchestration","url":"/ar/blueprint/cloud_security_development/serverless-and-orchestration#-videos","content":" AWS Serverless Security: The Most Security Way to Build In The Cloudâ€‹    AWS re:Invent 2024 - Implementing security best practices for serverless applicationsâ€‹   ","version":"Next","tagName":"h3"},{"title":"What is Cloud Security Development?","type":0,"sectionRef":"#","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#overview","content":" So, what is Cloud Security Development?  According to AWS, cloud security is â€œthe highest priority at AWS&quot; and consists of a shared responsibility model between the provider and the customer. However, Cloud Security Development takes that a step further. It focuses on engineering and building the services, tools, and automations that secure cloud environments.  This involves developing custom logic and controls through APIs and SDKs, automating guardrails, and enforcing best practices at scale using code. Think of it as the bridge between cloud engineering and security engineering, where youâ€™re not just using the cloud, youâ€™re building the systems that secure it.  In this section, weâ€™ll focus purely on theory and end with a capstone, or a challenge. The goal is to help you understand the key concepts, mental models, and design principles before you start building practical tools later.  ","version":"Next","tagName":"h2"},{"title":"Why is Cloud Security Development Important?â€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#why-is-cloud-security-development-important","content":" Cloud environments are growing fast, with new services, accounts, and permissions being created daily. With this speed, manual processes are no longer enough to keep systems secure.  Cloud Security Development enables organizations to:  Automate security enforcement: Eliminate repetitive manual steps by embedding logic directly into the cloudâ€™s control plane.Gain real-time visibility: Collect, process, and analyze logs and events to detect changes as they happen.Standardize security: Ensure consistent rules, policies, and tagging across all environments and teams.Accelerate response: Reduce time to detection and remediation during incidents.  By developing custom security capabilities, teams stay proactive rather than reactive. This practice is especially critical for large organizations that manage multiple accounts or projects across AWS, Azure, or Google Cloud.  To bring this home, I want you to think about what happens when a misconfigured cloud resource exposes data to the public internet. Without automation and guardrails, this could go unnoticed for weeks, ultimately leading to data loss, compliance violations, and reputational damage.  Hereâ€™s what that might look like in terms of risk:  Data Exposure: Sensitive data left accessible due to overly permissive storage configurations.Privilege Escalation: Overly broad IAM permissions granting users or workloads unintended access.Compliance Gaps: Inability to prove continuous enforcement of required controls.Operational Fatigue: Security teams spending hours manually auditing logs or permissions instead of improving controls.  ","version":"Next","tagName":"h2"},{"title":"Core Cloud Security Development Conceptsâ€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#core-cloud-security-development-concepts","content":" Before you start building anything, itâ€™s important to understand the main building blocks that make up cloud security development.  Identity and Access Management (IAM)Every cloud action happens under a principal (a user, role, or service). Understanding least privilege, role assumption, and permission boundaries is key. The most successful cloud security developers automate IAM validations and create small utilities that continuously check for privilege escalation paths or unused permissions. Events and LogsSecurity automation often begins with visibility. Events and logs are your sources of truth for â€œwho did what.&quot; Events represent actions that occur in near real time (e.g., an S3 bucket made public).Logs represent the historical record of all activity (e.g., CloudTrail, Audit Logs, or Activity Logs). These two together allow developers to build detection pipelines and automated responses. Guardrails and PoliciesGuardrails define what â€œgood&quot; looks like. For example, â€œno unencrypted storage buckets&quot; or â€œno public subnets in production.&quot; Cloud security developers build these policies into tools, APIs, or Lambda functions that continuously check compliance. Serverless Security FunctionsThe serverless model (e.g., AWS Lambda, Azure Functions, GCP Cloud Functions) is perfect for building lightweight controls. You can listen for specific events and immediately respond without worrying about infrastructure. Data ProtectionEncryption (at rest and in transit), tokenization, and secure key management (via KMS or CMEK) are at the core of protecting data. Cloud security developers ensure these processes are applied consistently and automatically.  ","version":"Next","tagName":"h2"},{"title":"Cloud Security Development vs. DevSecOpsâ€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#cloud-security-development-vs-devsecops","content":" You might be wondering: how is this different from DevSecOps?  DevSecOps\tCloud Security DevelopmentFocuses on securing the software delivery lifecycle\tFocuses on securing the cloud environment itself Involves scanning, testing, and shifting security left\tInvolves building tools, services, and automation for security Deals with CI/CD, IaC, and code pipelines\tDeals with API-driven detections, guardrails, and policy enforcement Example: Running SAST/DAST scans in a pipeline\tExample: Auto-remediating public resources or tagging owners automatically  Both disciplines are crucial.DevSecOps builds security into code, while Cloud Security Development builds security around the infrastructure running that code.  ","version":"Next","tagName":"h2"},{"title":"Common Cloud Security Development Use Casesâ€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#common-cloud-security-development-use-cases","content":" To better understand what cloud security development aims to achieve, here are a few common use cases that teams build toward:  Detecting Misconfigurations: Identify publicly exposed buckets or open network ports.Tag Enforcement: Automatically ensure that every resource has an owner and environment tag.IAM Auditing: Find unused permissions or risky wildcard policies.Event-Driven Response: Contain resources immediately upon suspicious activity.Compliance as Code: Codify regulatory or internal requirements to enforce automatically.  Each of these use cases can be implemented differently depending on the cloud provider and maturity of the environment, but the foundational concepts remain the same.  ","version":"Next","tagName":"h2"},{"title":"Key Takeawaysâ€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#key-takeaways","content":" Cloud Security Development is engineering .... not just configuration.Youâ€™ll spend time designing lightweight, reliable security functions.Everything begins with visibility: know who did what, when, and where.Automation is the backbone of scaling security across the cloud.Always aim for controls that are reversible, observable, and least-privileged.  ","version":"Next","tagName":"h2"},{"title":"Additional Resourcesâ€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#additional-resources","content":" To help you understand Cloud Security Development from a theoretical perspective, here are a few curated resources to explore:  ","version":"Next","tagName":"h2"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantAWS Certified Security â€“ Specialty\tAWS\tFocuses on securing AWS workloads, incident response, and best practices for cloud-native security. Google Professional Cloud Security Engineer\tGoogle Cloud\tValidates the ability to design and implement secure GCP infrastructures and automation. Microsoft Certified: Cybersecurity Architect Expert\tMicrosoft\tDemonstrates expertise in designing zero-trust and secure architectures across Azure environments. Certified Cloud Security Professional (CCSP)\t(ISC)Â²\tProvides a broad, vendor-neutral understanding of cloud security architecture and governance. Certified DevSecOps Professional (CDP)\tPractical DevSecOps\tFocuses on security automation, policy enforcement, and continuous compliance in multi-cloud CI/CD pipelines. HashiCorp Certified: Terraform Associate\tHashiCorp\tHighlights the ability to build and secure infrastructure as code across different cloud platforms.  ","version":"Next","tagName":"h3"},{"title":"Booksâ€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#books","content":" Book Title\tAuthor\tLinkSecurity Engineering: A Guide to Building Dependable Distributed Systems\tRoss J. Anderson\tAmazon Infrastructure as Code, Patterns and Practices: With examples in Python and Terraform\tRosemary Wang\tAmazon The DevOps Handbook, 2nd Edition: How to Create World-Class Agility, Reliability, &amp; Security in Technology Organizations\tGene Kim, Jez Humble, Patrick Debois, John Willis, Nicole Forsgren\tAmazon  ","version":"Next","tagName":"h3"},{"title":"YouTube Videosâ€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#youtube-videos","content":" How to ACTUALLY Learn Cloud Security?â€‹    AWS Shared Responsibility Model for Beginnersâ€‹    ","version":"Next","tagName":"h3"},{"title":"Articlesâ€‹","type":1,"pageTitle":"What is Cloud Security Development?","url":"/ar/blueprint/cloud_security_development/what-is-cloud-security-development#articles","content":" If youâ€™d like to dive deeper into the theory and concepts behind Cloud Security Development, here are a few solid reads:  https://cloud.google.com/architecture/framework/securityhttps://learn.microsoft.com/en-us/azure/security/fundamentals/overviewhttps://aws.amazon.com/architecture/security-identity-compliance/ ","version":"Next","tagName":"h3"},{"title":"FAQ","type":0,"sectionRef":"#","url":"/ar/additional_resources/faq","content":"","keywords":"","version":"Next"},{"title":"Do I need to be a strong coder in order to be a DevSecOps engineer?â€‹","type":1,"pageTitle":"FAQ","url":"/ar/additional_resources/faq#do-i-need-to-be-a-strong-coder-in-order-to-be-a-devsecops-engineer","content":" This is an interesting one. The answer to this is: Yes, to an extent.  As a DevSecOps engineer, you may be required to build custom tools from scratch or extending the functionality of these pipelines that may require an advanced understanding of programming concepts outside of looping, object oriented programming, and recursion. So it would be ideal to understand software engineering and programming concepts and best practices.  If your job won't have you doing that, then you'll need to ensure that you know scripting languages such as Bash, Powershell, and Python, and be comfortable with writing scripts that vary in complexity.  ","version":"Next","tagName":"h2"},{"title":"Do I need a degree to get into DevSecOps?â€‹","type":1,"pageTitle":"FAQ","url":"/ar/additional_resources/faq#do-i-need-a-degree-to-get-into-devsecops","content":" No. You do not need a degree to get into DevSecOps. Most of these orgs/companies really want you to have the hands-on experience, skills, and certifications that are effective for getting the job done.  Now, getting a degree will help you stand out from the competition and they do not expire or require you to recertify. The degrees that I recommend that you get for DevSecOps are highlighted below:  B.S of Computer Science with a minor in CyberSecurity (if possible)B.S of CyberSecurityM.S of CyberSecurityM.S of Computer Science  ","version":"Next","tagName":"h2"},{"title":"Do I need to learn the cloud to get a job in DevSecOps?â€‹","type":1,"pageTitle":"FAQ","url":"/ar/additional_resources/faq#do-i-need-to-learn-the-cloud-to-get-a-job-in-devsecops","content":" You don't necessary have to. However, I think that it would be of your best interest to learn one cloud service provider (CSP) of your choice very well. This could be AWS, Azure, Google Cloud, etc.  The reason why I say this is because most of the applications may end up being hosted in the cloud, and you'll have an understanding of what the cloud is and how it works. This may require you to get a few certifications as well such as the AWS Solutions Architect - Associate or the Google Cloud Associate Cloud Engineer to start. ","version":"Next","tagName":"h2"},{"title":"Secrets Management in the Cloud","type":0,"sectionRef":"#","url":"/ar/blueprint/cloud_security_development/secrets-and-config","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#overview","content":" So, what exactly is Secrets Management?  According to HashiCorp, secrets management is the practice of securely storing, accessing, and distributing sensitive credentials, such as passwords, API keys, tokens, and encryption keys, across systems.  In simpler terms:  Secrets management ensures that sensitive information doesnâ€™t end up in places where it shouldn't. Examples of this would be code, logs, or configuration files.  In modern environments powered by automation, microservices, and pipelines, this discipline isnâ€™t optional; itâ€™s foundational.  important Secrets arenâ€™t just data; theyâ€™re trust enablers. How you store and control them determines how secure your cloud really is.  ","version":"Next","tagName":"h2"},{"title":"Common Risks and Pitfallsâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#common-risks-and-pitfalls","content":" Secrets make things work, but they can also make things break â€” especially when managed poorly. Here are some of the most common pitfalls seen across cloud environments:  Risk\tDescriptionHardcoded Secrets\tCredentials left in source code or .env files. Plaintext Storage\tSecrets stored unencrypted in S3, GCS, or configuration files. Long-Lived Keys\tTokens or API keys that never expire or rotate. Overexposed Access\tMultiple users or systems sharing the same credentials. Logging Sensitive Data\tSecrets accidentally exposed in application logs or error messages.  ØªÙ„Ù…ÙŠØ­ Every leaked secret starts as a shortcut. Therefore, you should always assume that anything written down could one day be read by someone else.  ","version":"Next","tagName":"h2"},{"title":"The Four Pillars of Secrets Managementâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#the-four-pillars-of-secrets-management","content":" All effective secrets management strategies follow these core principles:  ","version":"Next","tagName":"h2"},{"title":"1. Centralizationâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#1-centralization","content":" Store secrets in a dedicated vault or managed service â€” not across config files or pipelines. Centralization provides visibility, control, and consistency.  ","version":"Next","tagName":"h3"},{"title":"2. Access Controlâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#2-access-control","content":" Restrict who (and what) can retrieve secrets using IAM roles or service accounts. Principals should only have access to the secrets tied to their role or function.  ","version":"Next","tagName":"h3"},{"title":"3. Lifecycle Managementâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#3-lifecycle-management","content":" Rotate secrets regularly, expire them automatically, and revoke them immediately after compromise. Short-lived credentials limit risk and reduce exposure time.  ","version":"Next","tagName":"h3"},{"title":"4. Auditing and Traceabilityâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#4-auditing-and-traceability","content":" Track every access request. Every retrieval should log who accessed what, when, and from where â€” if it canâ€™t be audited, it canâ€™t be trusted.  ","version":"Next","tagName":"h3"},{"title":"Secrets Management in the Cloudâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#secrets-management-in-the-cloud","content":" Each major cloud platform provides its own native tools for secrets management. While implementations differ, their design goals remain the same: control, visibility, and automation.  Provider\tService\tKey StrengthsAWS\tSecrets Manager / SSM Parameter Store\tAutomatic rotation, KMS encryption, and fine-grained IAM control. Azure\tKey Vault\tRBAC-based access, HSM-backed encryption, and comprehensive auditing. GCP\tSecret Manager\tPer-secret IAM, built-in versioning, and regional replication for availability. HashiCorp Vault\tCross-Cloud\tDynamic secrets, fine-grained policies, and lease-based access with expiration.  Ù…Ù„Ø§Ø­Ø¸Ø© Even with managed vaults, the principle remains the same: secrets should never live outside a governed boundary.  ","version":"Next","tagName":"h2"},{"title":"Best Practices for Cloud Secrets Managementâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#best-practices-for-cloud-secrets-management","content":" Centralize and Encrypt Everything Always use a dedicated vault service secured with KMS or HSM encryption. Automate Secret Rotation No secret should live longer than it needs to â€” use rotation policies or event triggers. Integrate with IAM Bind secret access to roles and identities instead of distributing static keys. Use Dynamic Secrets Where Possible Generate credentials on demand and expire them automatically. Isolate Environments Never reuse secrets across development, test, and production environments. Monitor and Audit Track access, alert on anomalies, and investigate failed retrievals. Eliminate Shared Secrets Every system, pipeline, or app should have its own unique credentials.  important When secrets are properly managed, they become invisible, ultimately working silently in the background to protect your environment.  ","version":"Next","tagName":"h2"},{"title":"Practice What Youâ€™ve Learnedâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#practice-what-youve-learned","content":" Letâ€™s put these principles into practice with a small, focused exercise.  Store a secret (like an API key or database password) in your cloud providerâ€™s secrets manager.Grant access to the secret using an IAM role or workload identity (not static credentials).Retrieve the secret securely in your application using an SDK or CLI command.Audit access logs to verify who retrieved it and when.Rotate the secret automatically to demonstrate lifecycle management.  âœ… Capstone Goal: Demonstrate how to securely store, access, and rotate secrets without ever exposing them in code or configuration.  ØªÙ„Ù…ÙŠØ­ Secrets management isnâ€™t about hiding credentials. Itâ€™s about making sure theyâ€™re used securely, automatically, and traceably.  ","version":"Next","tagName":"h2"},{"title":"Recommended Resourcesâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#recommended-resources","content":" ","version":"Next","tagName":"h2"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantAWS Certified Security â€“ Specialty\tAWS\tCovers KMS, Secrets Manager, and secure credential design. Google Professional Cloud Security Engineer\tGoogle Cloud\tDeep dive into key management and secret access policies. Microsoft SC-100: Cybersecurity Architect\tMicrosoft\tFocuses on designing vault architectures and enforcing access control. HashiCorp Certified: Vault Associate\tHashiCorp\tValidates practical understanding of Vaultâ€™s architecture and dynamic secrets. Certified DevSecOps Professional (CDP)\tPractical DevSecOps\tEmphasizes integrating secure secret management into CI/CD pipelines.  ","version":"Next","tagName":"h3"},{"title":"ðŸ“š Booksâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#-books","content":" Book Title\tAuthor\tLink\tWhy Itâ€™s UsefulCloud Native Security Cookbook: Recipes for a Secure Cloud\tJosh Armitage\tAmazon\tPractical recipes for managing secrets and encryption across multi-cloud environments.  ","version":"Next","tagName":"h3"},{"title":"ðŸŽ¥ Videosâ€‹","type":1,"pageTitle":"Secrets Management in the Cloud","url":"/ar/blueprint/cloud_security_development/secrets-and-config#-videos","content":" What is Secrets Management?â€‹    HashiCorp Vault Tutorial for Beginnersâ€‹   ","version":"Next","tagName":"h3"},{"title":"Cloud Logging and Event Visibility","type":0,"sectionRef":"#","url":"/ar/blueprint/cloud_security_development/logs-and-events","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#overview","content":" According to AWS, logging provides a record of actions taken by a user, role, or service. Events, on the other hand, represent real-time signals that something has occurred, such as a resource being created, a configuration change, or a permission update.  Together, logs and events form the observability layer of cloud security, which is the foundation for detection, response, and trust.  important Every detection, response, and compliance control depends on logs and events. Without them, youâ€™re operating blind.  ","version":"Next","tagName":"h2"},{"title":"Common Visibility Gapsâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#common-visibility-gaps","content":" Even organizations that practice strong identity and secrets management can lose sight of whatâ€™s actually happening in their environments. Here are some of the most common gaps that weaken visibility:  Gap\tDescriptionPartial Logging\tLogging isnâ€™t consistently enabled across accounts, services, or regions. Short Retention Periods\tLogs are deleted before investigations or audits can use them. Uncentralized Storage\tLogs live in separate accounts or regions without aggregation. Missing Context\tLogs lack metadata like account IDs, regions, or request origins. Dormant Events\tEvents are emitted but never acted upon or monitored.  ØªÙ„Ù…ÙŠØ­ You canâ€™t protect what you canâ€™t see. Make sure every cloud action leaves a record, and every record reaches a system that can act on it.  ","version":"Next","tagName":"h2"},{"title":"The Visibility Lifecycleâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#the-visibility-lifecycle","content":" Visibility begins with an action and ends with awareness. Each phase builds the foundation for continuous monitoring and automated defense.  ","version":"Next","tagName":"h2"},{"title":"1. Action Occursâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#1-action-occurs","content":" A user, workload, or automation makes a change (e.g modifying a policy or launching a new VM).  ","version":"Next","tagName":"h3"},{"title":"2. Log is Recordedâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#2-log-is-recorded","content":" The cloud provider captures details about the action: who performed it, what changed, and when.  ","version":"Next","tagName":"h3"},{"title":"3. Event is Emittedâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#3-event-is-emitted","content":" A real-time event signals that a notable action took place, which can trigger further processing.  ","version":"Next","tagName":"h3"},{"title":"4. Processing Happensâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#4-processing-happens","content":" Logs are stored for later analysis, while events are streamed to automation or alerting systems.  ","version":"Next","tagName":"h3"},{"title":"5. Response is Triggeredâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#5-response-is-triggered","content":" Security automations, alerts, or workflows act on suspicious activity or compliance violations.  Visibility doesnâ€™t stop with collection; it ends when your system responds intelligently.  ","version":"Next","tagName":"h3"},{"title":"Cloud-Native Visibility Servicesâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#cloud-native-visibility-services","content":" Each major cloud platform provides native logging and event capabilities. These are your most critical sources of truth for observability.  Cloud Provider\tLogging Service\tEvent Service\tPurposeAWS\tCloudTrail / CloudWatch Logs\tEventBridge\tTracks API activity and routes real-time events to automation. Azure\tActivity Logs / Diagnostic Logs\tEvent Grid\tCaptures operational data and triggers workflows or alerts. GCP\tCloud Audit Logs / Cloud Logging\tPub/Sub\tProvides centralized audit and event data for automation pipelines.  Ù…Ù„Ø§Ø­Ø¸Ø© Cloud-native visibility is your foundation, from detection to compliance builds on these core services.  ","version":"Next","tagName":"h2"},{"title":"Best Practices for Logging and Event Securityâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#best-practices-for-logging-and-event-security","content":" Enable Audit Logging Everywhere Turn on CloudTrail, Activity Logs, and Audit Logs for all accounts, regions, and critical services. Centralize and Encrypt Logs Store logs in a secured, centralized location with encryption at rest and in transit. Tag and Contextualize Include environment, region, and account identifiers for every record to improve traceability. Set Retention and Access Policies Retain logs long enough for compliance and forensics. Limit who can view or modify them. Automate Event Handling Use events to trigger real-time alerts or remediations (for example, alert on public resource creation). Monitor Access to Logs Treat log repositories like production systems â€” limit write access and track every modification. Validate the Flow Periodically test whether new events are being captured and processed as expected.  important Visibility is the foundation of trust. Without it, even the best secrets management or IAM controls lose context and meaning.  ","version":"Next","tagName":"h2"},{"title":"From Logs to Insight: The Security Value Chainâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#from-logs-to-insight-the-security-value-chain","content":" Logs and events are only valuable if they lead to insight and action. Hereâ€™s how raw telemetry becomes real security intelligence:  Collection â†’ Gather data from all cloud services.Aggregation â†’ Send logs and events to a central repository or SIEM.Correlation â†’ Connect events to users, systems, and environments.Detection â†’ Identify anomalies, misconfigurations, or policy violations.Response â†’ Automate alerts or remediations to close the loop.  Visibility transforms from passive observation into active defense.  ","version":"Next","tagName":"h2"},{"title":"Practice What Youâ€™ve Learnedâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#practice-what-youve-learned","content":" Letâ€™s apply what youâ€™ve learned with a small practical challenge.  Choose a cloud provider and enable complete audit and access logging for your environment.Route critical events â€” such as new admin creation or public resource access â€” through EventBridge, Event Grid, or Pub/Sub.Configure a simple automation (for example, a Lambda or Cloud Function) to detect and alert on those actions.Verify that all events are logged, stored, and acted upon.  âœ… Capstone Goal: Demonstrate a functioning visibility pipeline where logs provide history and events provide real-time awareness.  ØªÙ„Ù…ÙŠØ­ Think of logs as your security camera footage and events as the motion detectors that trigger alerts when something changes.  ","version":"Next","tagName":"h2"},{"title":"Recommended Resourcesâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#recommended-resources","content":" ","version":"Next","tagName":"h2"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantAWS Certified Security â€“ Specialty\tAWS\tFocuses on logging, monitoring, and event-driven detection in AWS environments. Google Professional Cloud Security Engineer\tGoogle Cloud\tEmphasizes audit logging and event automation. Microsoft SC-200: Security Operations Analyst\tMicrosoft\tHighlights visibility and response capabilities in Azure. Certified DevSecOps Professional (CDP)\tPractical DevSecOps\tDemonstrates visibility and automation in CI/CD pipelines.  ","version":"Next","tagName":"h3"},{"title":"ðŸ“š Booksâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#-books","content":" Book Title\tAuthor\tLink\tWhy Itâ€™s UsefulCloud Observability in Action\tMichael Hausenblas\tAmazon\tExplains how to design scalable log management and analysis systems.  ","version":"Next","tagName":"h3"},{"title":"ðŸŽ¥ Videosâ€‹","type":1,"pageTitle":"Cloud Logging and Event Visibility","url":"/ar/blueprint/cloud_security_development/logs-and-events#-videos","content":" What is CloudTrail?â€‹    Understanding Event-Driven Architecturesâ€‹   ","version":"Next","tagName":"h3"},{"title":"IAM Fundamentals","type":0,"sectionRef":"#","url":"/ar/blueprint/cloud_security_development/iam-fundamentals","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#overview","content":" IAM is the backbone of security in every cloud environment. It dictates who can access what, under which conditions, and from where. Misconfigurations here are one of the most common causes of security incidents in the cloud, so understanding IAM deeply is essential.  According to Microsoft, IAM is the framework that enables the right individuals or services to access the right resources at the right times for the right reasons.  In the context of cloud security, IAM provides the mechanisms that enforce authentication, authorization, and accountability across your environment. Whether youâ€™re working in AWS, Azure, or Google Cloud, the goal is the same:  Ensure that users and workloads have only the permissions they truly need.  ","version":"Next","tagName":"h2"},{"title":"Common Attack Surfacesâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#common-attack-surfaces","content":" Before we look at best practices, itâ€™s important to understand where IAM often goes wrong.  Surface\tDescriptionOverly Broad Permissions\tGranting *:* or â€œOwnerâ€ level access instead of defining specific actions. Long-Lived Credentials\tStatic access keys stored in code, scripts, or pipelines without rotation. Weak Authentication\tMissing or unenforced MFA for privileged accounts. Shared Roles\tDevelopers, admins, or CI/CD systems sharing the same identity. Unused Permissions\tIdentities retaining unnecessary access, increasing attack surface.  ØªÙ„Ù…ÙŠØ­ Most cloud breaches stem from identity misuse, not zero-day exploits. Strong IAM hygiene is your first line of defense.  ","version":"Next","tagName":"h2"},{"title":"The IAM Lifecycleâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#the-iam-lifecycle","content":" IAM security is not a one-time setup. It follows a lifecycle similar to other cloud controls: Define â†’ Enforce â†’ Monitor â†’ Improve.  ","version":"Next","tagName":"h2"},{"title":"1. Define Phaseâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#1-define-phase","content":" Identify all human and machine identities.Classify users and workloads by required access level.Establish naming and tagging conventions for traceability.  ","version":"Next","tagName":"h3"},{"title":"2. Enforce Phaseâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#2-enforce-phase","content":" Apply least privilege through roles and policies.Use conditions to restrict access (IP, time, or resource tags).Enforce MFA and federated authentication where possible.  ","version":"Next","tagName":"h3"},{"title":"3. Monitor Phaseâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#3-monitor-phase","content":" Enable access logging with AWS CloudTrail, Azure Activity Logs, or GCP Audit Logs.Detect unused permissions or suspicious behavior.Use tools like Access Analyzer, Azure PIM, or Policy Analyzer.  ","version":"Next","tagName":"h3"},{"title":"4. Improve Phaseâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#4-improve-phase","content":" Review IAM roles and permissions quarterly.Rotate and retire long-lived credentials automatically.Continuously refine policies to eliminate privilege creep.  ","version":"Next","tagName":"h3"},{"title":"Best Practices for Secure IAM Designâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#best-practices-for-secure-iam-design","content":" Apply the Principle of Least Privilege Start with no permissions and grant only whatâ€™s necessary. Use Roles, Not Users Prefer temporary credentials or federated roles over permanent users. Enable MFA Everywhere Especially for root accounts, admins, and CI/CD pipelines. Rotate Keys Frequently Automate key rotation and avoid hardcoding credentials in repositories. Audit IAM Regularly Use built-in analyzers or CSPM tools to identify misconfigurations. Separate Environments Keep IAM boundaries distinct between dev, test, and production. Tag Identities for Ownership Add metadata to roles and accounts for accountability and automation.  important IAM is not just about restricting access. Itâ€™s about granting the right access at the right time with visibility and control.  ","version":"Next","tagName":"h2"},{"title":"IAM Across Cloud Providersâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#iam-across-cloud-providers","content":" Cloud Provider\tIAM Model\tKey FeaturesAWS IAM\tPolicies, roles, users, and groups\tJSON-based policies, role assumption, temporary credentials via STS Azure IAM\tRole-Based Access Control (RBAC)\tHierarchical scope: subscription â†’ resource group â†’ resource GCP IAM\tPolicy Binding System\tResource-level bindings, inherited roles, and contextual access conditions  Each provider follows the same principle: authenticate first, authorize second.  ","version":"Next","tagName":"h2"},{"title":"Practice What Youâ€™ve Learnedâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#practice-what-youve-learned","content":" Now itâ€™s time to apply your understanding in a hands-on IAM hardening exercise.  Audit an IAM configuration for excessive permissions or weak MFA policies.Redesign policies to enforce least privilege.Implement automated analysis using AWS Access Analyzer, Azure PIM, or GCP Policy Analyzer.Write a short report documenting: Risks foundActions takenSecurity impact  âœ… Capstone Goal: Create a concise â€œIAM Hardening Reportâ€ that shows how you identified and mitigated privilege risks through automation.  Ù…Ù„Ø§Ø­Ø¸Ø© IAM automation is a journey. So make sure you review permissions frequently, track changes, and make iterative improvements over time.  ","version":"Next","tagName":"h2"},{"title":"Recommended Resourcesâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#recommended-resources","content":" ","version":"Next","tagName":"h2"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantAWS Certified Security â€“ Specialty\tAWS\tDeep dive into IAM, key management, and access control across AWS environments. Microsoft Certified: Identity and Access Administrator Associate\tMicrosoft\tFocuses on managing Azure AD, conditional access, and governance. Google Professional Cloud Security Engineer\tGoogle Cloud\tValidates knowledge of IAM, workload identity, and organization-level policies. Certified Cloud Security Professional (CCSP)\t(ISC)Â²\tProvides a vendor-neutral understanding of IAM across cloud platforms.  ","version":"Next","tagName":"h3"},{"title":"ðŸ“š Booksâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#-books","content":" Book Title\tAuthor\tLink\tWhy Itâ€™s UsefulAWS Certified Security â€“ Specialty (SCS-C02) Exam Guide\tAdam Book, Stuart Scott\tAmazon\tPrepares you for AWS IAM concepts, access management, and incident response. Microsoft Azure Security Center (IT Best Practices - Microsoft Press)\tYuri Diogenes, Tom Janetscheck\tAmazon\tIntroduces Azure IAM, policy management, and conditional access. Official Google Cloud Certified Professional Cloud Security Engineer Exam Guide\tAnkush Chowdhary, Prashant Kulkarni, Phil Venables\tAmazon\tExplains GCP IAM, auditing, and security fundamentals for developers.  ","version":"Next","tagName":"h3"},{"title":"ðŸŽ¥ Videosâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#-videos","content":" AWS Identity and Access Management (IAM) Basics | AWS Tutorials For Beginnersâ€‹    Identity &amp; Access Management (IAM)â€‹    ","version":"Next","tagName":"h3"},{"title":"Articlesâ€‹","type":1,"pageTitle":"IAM Fundamentals","url":"/ar/blueprint/cloud_security_development/iam-fundamentals#articles","content":" If you want to explore IAM theory further, check out these excellent reads:  AWS IAM FeaturesAzure Role-Based Access Control OverviewGCP IAM Overview ","version":"Next","tagName":"h3"},{"title":"What is Application Security?","type":0,"sectionRef":"#","url":"/ar/blueprint/devsecops/what-is-application-security","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#overview","content":" So, what is Application Security? According to Imperva, Application Security aims to protect software application code and data against cybersecurity threats and vulnerabilities. This includes using tools to scan your code, fixing the issues within your code, and doing a b$it of threat modeling when designing your application to ensure that you're preventing vulnerabilites from being introduced into your code.  This process really happens within phases within the Secure Software Development Life Cycle (SSDLC), which we'll define and explain in more detail in the next page. But, let's talk a little bit about why this is actually important.  ","version":"Next","tagName":"h2"},{"title":"Why is Application Security Important?â€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#why-is-application-security-important","content":" We're in the digital age where information is bought and sold for a price. The thing that I want you to understand is that most of this information is stored behind or within some type of application. I am most certain that you log into a web application to pay your bills or view what's your bank account right? So imagine if that application that you using to view that data and pay those bills is vulnerable. At this point, there is a way for someone to compromise the system, potentially your sensitive information and possibly someone elses, and then use that against you or/and sell it on the dark web.  So, to help bring this home, I want you to understand that applications are often targeted by attackers looking to exploit vulnerabilities for unauthorized access, data theft, or other malicious purposes. If an application is poorly structured, threat modeled, or secured, here are some of the underlying impacts and affects:  Data breaches: Loss or exposure of sensitive data.Compliance violations: Non-compliance with industry regulations like GDPR or HIPAA.Reputational damage: Loss of customer trust and market position.  ","version":"Next","tagName":"h2"},{"title":"Common Application Security Vulnerabilitiesâ€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#common-application-security-vulnerabilities","content":" There are a list of common security vulnerabilites that change every year. You'll want to ensure that you pay close attention to the OWASP Top 10 as it is an excellent way to stay up-to-date on the most common vulnerabilities that affect organizations in a major way.  However, I've taken the liberty of listing out some of the most common vulnerabilites that I&quot;ve experienced that are still pretty impactful that you should know:  SQL Injection: Malicious input allows attackers to manipulate queries to a database, potentially gaining access to sensitive information.Cross-Site Scripting (XSS): Attackers inject malicious scripts into web pages, potentially affecting other users.Insecure Authentication and Authorization: Weak or improperly configured access controls can allow unauthorized users to access sensitive data.Sensitive Data Exposure: Inadequate protection of sensitive information, such as encryption or tokenization, can lead to data breaches.  ","version":"Next","tagName":"h2"},{"title":"SAST and DASTâ€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#sast-and-dast","content":" There are ways mitigate and eliminate those vulnerabilites by leveraging two key testing methods: Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST). These techniques play an essential role in identifying vulnerabilities within your code or your running application.  Static Application Security Testing (SAST) is a white-box testing technique that analyzes the application's source code, bytecode, or compiled binaries for vulnerabilities. White-box testing refers to the hacker an understanding of the system prior to testing it for vulnerabilites.  SAST tools scan the code without executing the program, allowing developers to catch potential security issues early in the development process, which is nice. The benefits of SAST are:  Early Detection: Since SAST runs on the source code, it can detect security issues during development, even before the code is compiled or deployed.Comprehensive Coverage: SAST can analyze all code paths and uncover vulnerabilities such as SQL injection, XSS, buffer overflows, and insecure coding practices.Automated: SAST tools can be integrated into the CI/CD pipeline, automatically analyzing code upon each commit or build.  On the other hand, Dynamic Application Security Testing (DAST) is a black-box testing technique that examines the application's running state by simulating real-world attacks. Black-box testing refers to the tester not having any knowledge about the application or product, which is what most pentesters or hackers do anyway.  DAST tools test applications in their operational environments (e.g., web apps, APIs) to find vulnerabilities that may only become apparent during runtime, which can give you a very accurate assessment of what is truly components of your app is truly vulnerable. Here are some of the benefits of DAST:  Runtime Detection: Since DAST tests a running application, it can catch vulnerabilities like misconfigurations, authentication issues, and other flaws that arise when the application is in use.No Access to Source Code: DAST doesnâ€™t need access to the applicationâ€™s code, making it effective for identifying vulnerabilities in third-party applications and APIs.Real-World Simulation: DAST mimics the behavior of a real-world attacker, identifying potential attack vectors from an external perspective.  While SAST focuses on finding vulnerabilities in the code itself, DAST assesses how the application performs once deployed. Together, they provide a comprehensive approach to security testing:  SAST: Ideal for early-stage detection, scanning static code and catching issues before the application is deployed.DAST: Crucial for identifying vulnerabilities that can only be found in the operational environment, such as misconfigurations or runtime-specific issues.  By incorporating both SAST and DAST into the SDLC, you ensure that security testing occurs at both the code level and the operational level, minimizing the risk of vulnerabilities slipping through the cracks.  ","version":"Next","tagName":"h2"},{"title":"Tools and Projectsâ€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#tools-and-projects","content":" There are a plethora of tools that you should play around with. Some of these will really test your application penetration testing skills and help you better understand how these vulnerabilities work. I've listed them all out in a table below:  Project Name\tDescriptionOWASP Juice Shop\tA vulnerable web app for practicing web security testing with a focus on OWASP Top 10 vulnerabilities. Damn Vulnerable Web Application (DVWA)\tPHP/MySQL-based web app designed for practicing penetration testing skills. Vulnerable Node.js Application (VulnNode)\tA vulnerable Node.js application for practicing JavaScript/Node.js specific security testing. Hackazon\tAn e-commerce platform that simulates a real-world site with multiple security vulnerabilities. Mutillidae II\tA free, open-source vulnerable web application that covers over 40 vulnerabilities for testing. bWAPP\tA PHP-based vulnerable app with over 100 web vulnerabilities, covering OWASP Top 10 and beyond. OWASP WebGoat\tAn insecure web application designed for practicing web security vulnerabilities and solutions. NodeGoat\tA deliberately insecure Node.js app, focusing on vulnerabilities specific to JavaScript and Node.js. OWASP Security Shepherd\tA platform designed for learning security principles with a capture-the-flag format for both beginner and advanced users.  ","version":"Next","tagName":"h2"},{"title":"Additional Resourcesâ€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#additional-resources","content":" To help you better understand application security, here are some resources that I've hand picked out for you to review and look at:  ","version":"Next","tagName":"h2"},{"title":"Booksâ€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#books","content":" Book Title\tAuthor\tLinkAlice and Bob Learn Application Security\tTanya Janca\tAmazon The Web Application Hacker's Handbook: Finding and Exploiting Security Flaws\tDafydd Stuttard &amp; Marcus Pinto\tAmazon Hacking APIs: Breaking Web Application Programming Interfaces\tCorey J. Ball\tAmazon  ","version":"Next","tagName":"h3"},{"title":"YouTube Videosâ€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#youtube-videos","content":" Application Security 101 - What you need to know in 8 minutesâ€‹    What is SAST and DAST?â€‹    ","version":"Next","tagName":"h3"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantCompTIA Security+\tCompTIA\tBuilds foundational understanding of application and system security. Certified Application Security Engineer (CASE)\tEC-Council\tFocuses on secure coding and app testing principles. AWS Certified Developer â€“ Associate\tAWS\tIncludes knowledge of secure app deployment and API interaction. GIAC GWAPT â€“ Web Application Penetration Tester\tGIAC\tStrengthens practical knowledge of web application vulnerabilities and testing.  ","version":"Next","tagName":"h2"},{"title":"Articlesâ€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#articles","content":" If you're into blogs and want to dive more deeply into Application Security, check out the ones below:  https://www.crowdstrike.com/en-us/cybersecurity-101/application-security/https://www.ibm.com/topics/application-securityhttps://medium.com/googledeveloperseurope/what-is-application-security-all-you-need-to-know-guide-blog-3ceee69deb11  ","version":"Next","tagName":"h3"},{"title":"Practice What Youâ€™ve Learnedâ€‹","type":1,"pageTitle":"What is Application Security?","url":"/ar/blueprint/devsecops/what-is-application-security#practice-what-youve-learned","content":" Spin up one of the intentionally vulnerable applications from the list above â€” for example, OWASP Juice Shop or DVWA â€” and perform a basic SAST and DAST workflow:  Run a SAST scan on the source code using tools like SonarQube, Semgrep, or CodeQL to identify coding-level security flaws.Deploy the app locally (Docker or localhost) and run a DAST scan using OWASP ZAP or Burp Suite Community Edition to identify runtime vulnerabilities.Document the findings â€” which vulnerabilities overlap between SAST and DAST? Which ones were unique to each method?Write a short â€œsecurity summaryâ€ explaining how youâ€™d fix or mitigate the top two vulnerabilities discovered.  âœ… Capstone Goal: Demonstrate how early and continuous testing helps uncover and prevent vulnerabilities through both static and dynamic analysis. Show understanding of when to use SAST vs. DAST and why combining both leads to stronger application security.  Ù…Ù„Ø§Ø­Ø¸Ø© As you matriculate through the blueprint, you'll learn about integrating this into CI/CD pipelines and processes. ","version":"Next","tagName":"h2"},{"title":"API Patterns and SDKs","type":0,"sectionRef":"#","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#overview","content":" Every major cloud provider, AWS, Azure, and Google Cloud, exposes its services through Application Programming Interfaces (APIs). These APIs allow developers to create, manage, and secure resources programmatically.  SDKs (Software Development Kits) act as wrappers around those APIs, providing language-specific interfaces that make automation, integrations, and custom tool development much easier When used correctly, APIs and SDKs enable you to automate security, enforce compliance, and build intelligent systems that react to real-time events across your environment. However, with great power comes great responsibility, and insecure API calls or misused SDKs can introduce risk just as easily as they can remove it.    Ù…Ù„Ø§Ø­Ø¸Ø© You can find the original image here: API Gateway Security: How to Secure Your APIs with Best Practices. APIs are the foundation of cloud automation and come with several risks. Therefore, they must be designed and consumed securely to protect the control plane.  ","version":"Next","tagName":"h2"},{"title":"Common Attack Surfacesâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#common-attack-surfaces","content":" Before building with APIs and SDKs, itâ€™s important to understand where they can be most vulnerable:  Surface\tDescriptionUnsecured Endpoints\tAPIs that lack authentication or encryption expose sensitive data or control functions. Over-Privileged Tokens\tAccess keys or OAuth tokens with excessive permissions increase the blast radius of compromise. Poor Input Validation\tUnvalidated parameters can lead to injection or privilege escalation within API calls. Lack of Rate Limiting\tAPIs without throttling are vulnerable to abuse, denial-of-service attacks, or brute-force attempts. Unmonitored API Usage\tWithout logging and metrics, malicious or accidental misuse can go unnoticed.  ØªÙ„Ù…ÙŠØ­ APIs are your control planeâ€™s front door, so protect them like one. Always authenticate, authorize, and validate every request.  ","version":"Next","tagName":"h2"},{"title":"The Secure API Lifecycleâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#the-secure-api-lifecycle","content":" Just like containers or CI/CD pipelines, APIs follow a lifecycle that should include security at every step. Think of it as Design â†’ Build â†’ Consume â†’ Monitor.  ","version":"Next","tagName":"h2"},{"title":"1. Design Phaseâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#1-design-phase","content":" Start with the principle of least privilege for all service integrations.Use OpenAPI/Swagger specifications to standardize and document API behavior.Apply secure defaults â€” HTTPS only, strict authentication, and minimal scope for access tokens.  ","version":"Next","tagName":"h3"},{"title":"2. Build Phaseâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#2-build-phase","content":" Use SDKs from official cloud providers (e.g., boto3, google-cloud, or azure-identity) to ensure consistent authentication and version control.Implement parameter validation and error handling to prevent injection or data leaks.Rotate and manage credentials using tools like AWS Secrets Manager, Azure Key Vault, or GCP Secret Manager.  ","version":"Next","tagName":"h3"},{"title":"3. Consume Phaseâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#3-consume-phase","content":" Authenticate API calls using short-lived credentials (STS, OIDC, or workload identity federation).Implement retry logic with exponential backoff to handle throttling gracefully.Restrict which systems or users can make calls through IAM roles, service principals, or workload identities.  ","version":"Next","tagName":"h3"},{"title":"4. Monitor Phaseâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#4-monitor-phase","content":" Log every API interaction through services like CloudTrail, Activity Logs, or Audit Logs.Create alerts for unauthorized or unusual API activity.Analyze API traffic patterns with CloudWatch, Azure Monitor, or Cloud Logging for anomalies.  ","version":"Next","tagName":"h3"},{"title":"Best Practices for API and SDK Securityâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#best-practices-for-api-and-sdk-security","content":" Use Strong Authentication Prefer identity federation or temporary tokens over long-lived API keys. Validate Everything Validate input parameters, query strings, and payloads to prevent injection attacks. Implement Rate Limiting Throttle requests to protect APIs from abuse and denial-of-service attempts. Encrypt in Transit Enforce HTTPS/TLS for all requests. Reject any call made over plain HTTP. Rotate Keys Automatically Automate credential rotation to minimize exposure risk in case of leaks. Use Official SDKs Stick with SDKs provided by cloud vendors to ensure compatibility, reliability, and built-in security features. Enable Logging and Metrics Treat API logs like audit trails â€” essential for investigation and continuous monitoring. Document and Version APIs Clear documentation and version control prevent confusion and unsafe integrations.  ","version":"Next","tagName":"h2"},{"title":"Recommended Toolsâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#recommended-tools","content":" Tool\tPurposePostman\tTest, document, and automate API requests securely. AWS SDK (boto3)\tPython SDK for interacting with AWS services programmatically. Azure SDK for Python\tSimplifies calling Azure APIs securely using managed identities. Google Cloud SDK (gcloud / client libs)\tProvides command-line and library support for secure API interaction. Swagger / OpenAPI\tStandard framework for documenting and validating RESTful APIs. OWASP API Security Project\tOffers best practices and testing guidelines for securing APIs.  Ù…Ù„Ø§Ø­Ø¸Ø© Always test APIs in isolated environments before deploying to production. Use least privilege and separate credentials for testing and production pipelines.  ","version":"Next","tagName":"h2"},{"title":"Practice What Youâ€™ve Learnedâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#practice-what-youve-learned","content":" Now itâ€™s time to apply these concepts.  Choose one cloud provider and write a short script using its SDK (for example, boto3, google-cloud, or azure-identity).List all compute or storage resources in your account securely using temporary credentials.Implement a simple retry and rate-limiting mechanism.Add structured logging for every API call you make.  âœ… Capstone Goal: Demonstrate secure use of APIs and SDKs by automating a basic inventory or compliance task using proper authentication, error handling, and logging.  important Never hardcode credentials in your code or configuration. Always use environment variables, secrets managers, or identity federation.  ","version":"Next","tagName":"h2"},{"title":"Recommended Resourcesâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#recommended-resources","content":" ","version":"Next","tagName":"h2"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantAWS Certified Developer â€“ Associate\tAWS\tFocuses on building secure, scalable, and automated solutions using APIs and SDKs. Google Professional Cloud Developer\tGoogle Cloud\tValidates the ability to design and secure API-driven cloud applications. Microsoft Certified: Azure Developer Associate\tMicrosoft\tReinforces secure API integration and managed identity usage within Azure. Certified DevSecOps Professional (CDP)\tPractical DevSecOps\tCovers secure automation, policy enforcement, and secure coding across APIs.  ","version":"Next","tagName":"h3"},{"title":"ðŸ“š Booksâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#-books","content":" Book Title\tAuthor\tLink\tWhy Itâ€™s UsefulAPI Security in Action\tNeil Madden\tAmazon\tA practical guide to designing and securing APIs using modern authentication and encryption patterns.  ","version":"Next","tagName":"h3"},{"title":"ðŸŽ¥ Videosâ€‹","type":1,"pageTitle":"API Patterns and SDKs","url":"/ar/blueprint/cloud_security_development/api-patterns-and-sdks#-videos","content":" API Security Fundamentals - Course for Beginnersâ€‹    DevSecOps Course for Beginners - API Securityâ€‹   ","version":"Next","tagName":"h3"},{"title":"Container Security","type":0,"sectionRef":"#","url":"/ar/blueprint/devsecops/container-security","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#overview","content":" Containers have changed how we build, package, and deploy software. They make applications portable, consistent, and fast to ship, but they also introduce new attack surfaces that must be managed carefully. From outdated base images to leaked secrets and over-privileged containers, insecure configurations can make containers the easiest entry point into your environment.  In DevSecOps, container security isnâ€™t just about scanning images. Itâ€™s about embedding security throughout the entire container lifecycle... from build, to ship, to runtime.    Ù…Ù„Ø§Ø­Ø¸Ø© You can find the original image here: Securing Containers from Build to Runtime | Microsoft Defender for CloudAlso, containers give speed, but speed without security invites risk. Treat every image and runtime as part of your security perimeter.  ","version":"Next","tagName":"h2"},{"title":"Common Attack Surfacesâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#common-attack-surfaces","content":" To secure containers effectively, you need to understand where theyâ€™re most vulnerable:  Surface\tDescriptionBase Images\tOutdated or unverified base images may include known CVEs and hidden dependencies. Image Layers\tEach layer can add unnecessary files, secrets, or unpatched binaries if not reviewed. Secrets Exposure\tHardcoded credentials or unencrypted environment variables often end up baked into images. Container Runtime\tContainers running as root or with privileged access can compromise the host system. Networking\tMisconfigured networks may expose internal services to the public internet. Registry Security\tUsing unverified public images or insecure registries risks introducing malicious software.  ØªÙ„Ù…ÙŠØ­ Most container breaches stem from configuration mistakes, not advanced exploits. Start by securing the basics.  ","version":"Next","tagName":"h2"},{"title":"The Container Security Lifecycleâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#the-container-security-lifecycle","content":" Container security follows the same shift-left philosophy as DevSecOps: secure early, monitor continuously, and automate everything.  ","version":"Next","tagName":"h2"},{"title":"1. Build Phaseâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#1-build-phase","content":" Scan base images and dependencies for known vulnerabilities.Use minimal base images to reduce the attack surface.Enforce consistent image signing and tagging.Maintain a Software Bill of Materials (SBOM) to track whatâ€™s inside every image.  ","version":"Next","tagName":"h3"},{"title":"2. Ship Phaseâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#2-ship-phase","content":" Store only trusted images in private or verified registries.Apply access controls and automated scanning at the registry level.Sign and verify images before deployment.Avoid using the â€œlatestâ€ tag. Always version explicitly for traceability.  ","version":"Next","tagName":"h3"},{"title":"3. Run Phaseâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#3-run-phase","content":" Run containers with least privilege and never as root.Apply resource limits (CPU, memory) to prevent denial-of-service conditions.Enable runtime monitoring and anomaly detection.Isolate workloads using namespaces, cgroups, and sandboxing.  ","version":"Next","tagName":"h3"},{"title":"Best Practices for Container Securityâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#best-practices-for-container-security","content":" Adopt Immutable Infrastructure Treat containers as disposable. Rebuild images to patch, never modify live ones. Minimize the Attack Surface Use lightweight base images (for example Alpine or Distroless) and remove unnecessary packages. Scan Early and Often Integrate container scanning into CI/CD pipelines to catch vulnerabilities before deployment. Protect Secrets Inject secrets securely at runtime using tools like Vault or Secrets Manager. Never bake them into images. Implement Image Signing and Verification Use tools like Cosign or Notary to verify image integrity before deployment. Monitor Runtime Behavior Watch for abnormal processes, network activity, and privilege escalations. Tools like Falco can help. Secure Registries Restrict access to trusted users and enforce scanning on every pushed image.  ","version":"Next","tagName":"h2"},{"title":"Recommended Toolsâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#recommended-tools","content":" Tool\tPurposeTrivy\tScans images, file systems, and repos for vulnerabilities and secrets. Grype / Anchore Engine\tPerforms deep image analysis and compliance reporting. Clair\tScans Docker and OCI images for known vulnerabilities. Docker Scout\tIntegrates vulnerability insights directly into Docker builds. Falco\tDetects runtime anomalies and suspicious behavior. Cosign\tSigns and verifies images for integrity and provenance.  Ù…Ù„Ø§Ø­Ø¸Ø© Combine multiple tools to cover different stages of the container lifecycle. No single scanner does it all.  ","version":"Next","tagName":"h2"},{"title":"Practice What Youâ€™ve Learnedâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#practice-what-youve-learned","content":" Now itâ€™s your turn to apply what youâ€™ve learned.  Choose a small containerized app (for example, a Flask API or Node.js microservice).Scan your image for vulnerabilities using Trivy or Grype.Add runtime monitoring with Falco or a similar tool.Review your Dockerfile for security misconfigurations.  âœ… Capstone Goal: Show that you can identify and remediate vulnerabilities across the container build, ship, and run phases.  important Remember, security doesnâ€™t end at deployment. Containers must be monitored, patched, and rebuilt regularly to stay secure.  ","version":"Next","tagName":"h2"},{"title":"Recommended Resourcesâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#recommended-resources","content":" ","version":"Next","tagName":"h2"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantDocker Certified Associate (DCA)\tDocker\tValidates container lifecycle and security fundamentals. Certified Kubernetes Application Developer (CKAD)\tCNCF\tReinforces container orchestration and deployment security. CompTIA Security+\tCompTIA\tStrengthens foundational security knowledge. Certified DevSecOps Professional (CDP)\tPractical DevSecOps\tFocuses on container scanning and policy automation.  ","version":"Next","tagName":"h3"},{"title":"ðŸ“š Booksâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#-books","content":" Book Title\tAuthor\tLink\tWhy Itâ€™s UsefulContainer Security: Fundamental Technology Concepts That Protect Cloud Native Applications\tLiz Rice\tAmazon\tExplains how containers work under the hood and how to secure them effectively throughout their lifecycle. Application Container Security Guide - NIST SP 800-190\tNational Institute of Standards and Technology\tAmazon\tProvides official NIST guidance on container threats, mitigations, and best practices for secure deployments. Kubernetes Security\tLiz Rice\tAmazon\tProvides a clear, technical guide to securing Kubernetes workloads and understanding container threats.  ","version":"Next","tagName":"h3"},{"title":"ðŸŽ¥ Videosâ€‹","type":1,"pageTitle":"Container Security","url":"/ar/blueprint/devsecops/container-security#-videos","content":" What is Container Security?â€‹    How To Secure &amp; Harden Docker Containersâ€‹   ","version":"Next","tagName":"h3"},{"title":"Infrastructure as Code (IaC) Security","type":0,"sectionRef":"#","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#overview","content":" According to HashiCorp, IaC is the practice of defining and managing infrastructure through machine-readable configuration files instead of manual setup.  That means instead of clicking through a console to create an S3 bucket or VM, you describe it in code. Tools like Terraform, AWS CloudFormation, Azure Bicep, and Google Deployment Manager read those definitions and provision your environment automatically.  This shift changed everything and infrastructure became versioned, reviewed, and auditable. But that also means infrastructure code must now be treated with the same rigor and security as application code.  Ù…Ù„Ø§Ø­Ø¸Ø© You can find the original image here: HashiCorp Terraform Overview. Infrastructure as Code delivers agility, but without security guardrails, speed becomes a multiplier for risk.  ","version":"Next","tagName":"h2"},{"title":"Why IaC Security Mattersâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#why-iac-security-matters","content":" IaC acts like a conveyor belt for your cloud. Once itâ€™s in motion, every change moves fast. If a configuration is insecure, that flaw can spread across hundreds of environments in seconds.  When secured properly, IaC becomes one of the most powerful defensive tools in cloud security. It enables you to:  Catch misconfigurations prior to deployment.Enforce compliance and policy checks automatically.Audit and track infrastructure changes for accountability.Detect and remediate drift between defined and deployed states.Standardize security across multiple accounts and regions.  In other words, IaC security transforms cloud infrastructure from something thatâ€™s built to something thatâ€™s verified.  ","version":"Next","tagName":"h2"},{"title":"Core Concepts of IaC Securityâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#core-concepts-of-iac-security","content":" ","version":"Next","tagName":"h2"},{"title":"Declarative vs. Imperative Modelsâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#declarative-vs-imperative-models","content":" IaC tools fall into two broad categories:  Model\tDefinition\tExample ToolsDeclarative\tDescribes the desired state or what the environment should look like.\tTerraform, CloudFormation Imperative\tDescribes the process or how to create the environment step by step.\tAnsible, Chef  From a security perspective, declarative IaC is ideal because it exposes intent, which means that scanners and policy engines can evaluate whether configurations are safe before deployment. On the other hand, Declarative IaC is inherently auditable and predictable, making it easier to detect deviations and enforce standards.  ","version":"Next","tagName":"h3"},{"title":"Policy as Code (PaC)â€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#policy-as-code-pac","content":" If IaC defines what your cloud looks like, Policy as Code (PaC) defines whatâ€™s allowed to look like that.  Policies are written as logical rules that automatically evaluate configurations for compliance before theyâ€™re applied. Instead of relying on manual review, PaC turns governance into code.  Examples of common policies:  â€œAll storage buckets must be encrypted.â€â€œNo resource should be publicly accessible.â€â€œEvery resource must include an owner tag.â€  PaC ensures every infrastructure change meets organizational and compliance standards automatically and at scale.  ","version":"Next","tagName":"h3"},{"title":"Open Policy Agent (OPA) and Regoâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#open-policy-agent-opa-and-rego","content":" At the core of many PaC implementations is Open Policy Agent (OPA) â€” an open-source policy engine that uses a declarative logic language called Rego.  Rego defines what must be true for a configuration to be considered compliant. It doesnâ€™t build your infrastructure â€” it judges it.  Example logic:  â€œIf a resource is public, deny deployment.â€ â€œIf encryption is missing, flag as noncompliant.â€  OPA, Conftest, and Terraform Cloudâ€™s Sentinel all use Rego-like syntax to enforce these checks within CI/CD pipelines â€” ensuring every change passes through a policy gate before deployment.  ","version":"Next","tagName":"h3"},{"title":"Version Control and Drift Managementâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#version-control-and-drift-management","content":" All IaC should live in version control, not just for collaboration, but for security accountability.  This provides:  Auditability: Track who made each change and why.Rollback Capability: Revert insecure states instantly.Compliance Evidence: Document continuous governance.  But even versioned code can drift from reality. When someone makes manual console changes, thatâ€™s configuration drift, and drift creates blind spots.  Use drift detection tools (like Terraform Cloud, AWS Config, or Wiz) to continuously compare whatâ€™s deployed with whatâ€™s defined.  ","version":"Next","tagName":"h3"},{"title":"Immutable Infrastructureâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#immutable-infrastructure","content":" Instead of patching live systems, rebuild them. Immutable infrastructure replaces modification with recreation ensuring clean, versioned, and verifiable deployments.  Benefits include:  Predictable environments with no legacy misconfigurations.Consistent baselines across regions or teams.Simplified rollback and faster recovery from compromise.  Immutability isnâ€™t just an operational pattern; itâ€™s a security control that enforces consistency and hygiene.  ","version":"Next","tagName":"h3"},{"title":"Common IaC Security Risksâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#common-iac-security-risks","content":" IaC brings automation, but also new opportunities for mistakes to scale.  Risk\tImpactHardcoded Secrets\tCredentials embedded in IaC files or variables. Public Resources\tBuckets, databases, or VMs with open access. Unencrypted Storage\tMissing encryption for sensitive data at rest. Overprivileged Roles\tWildcard IAM permissions in templates. Lack of Peer Review\tIaC changes pushed directly to production. Configuration Drift\tManual updates that deviate from the defined state.  ØªÙ„Ù…ÙŠØ­ IaC accelerates both good and bad practices. So, make sure every template passes through the same security and compliance pipeline.  ","version":"Next","tagName":"h2"},{"title":"Best Practices for Securing IaCâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#best-practices-for-securing-iac","content":" Version Everything Store all IaC in Git or a similar version control system. Every change should be peer-reviewed. Scan Before You Deploy Use tools like Checkov, Tfsec, or Trivy to detect misconfigurations early. Write and Enforce Policies Use OPA, Conftest, or Sentinel to apply Policy as Code checks automatically. Integrate Secrets Management Never hardcode credentials â€” pull them dynamically from a vault. Separate Environments Keep dev, test, and production configurations isolated. Monitor for Drift Continuously detect and remediate configuration drift. Automate Everything Embed scanning and validation into your CI/CD pipeline. Enable Auditing Log every IaC execution and decision for transparency and compliance.  important IaC should build secure infrastructure by default. Security checks arenâ€™t optional; theyâ€™re part of the deployment definition.  ","version":"Next","tagName":"h2"},{"title":"Practice What Youâ€™ve Learnedâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#practice-what-youve-learned","content":" Letâ€™s apply what youâ€™ve learned with a conceptual challenge.  ","version":"Next","tagName":"h2"},{"title":"Goalâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#goal","content":" Design a policy-driven IaC security framework that ensures no insecure configuration can be deployed to production.  ","version":"Next","tagName":"h3"},{"title":"Tasksâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#tasks","content":" Definition Layer: IaC templates are written, versioned, and reviewed through pull requests.Policy Evaluation Layer: OPA or Rego-based policies automatically evaluate configurations pre-deployment.Enforcement Layer: CI/CD pipelines block or flag noncompliant changes.Observation Layer: Logging and drift detection monitor live infrastructure for deviations.  âœ… Capstone Goal: Demonstrate how to design an IaC pipeline that enforces security and compliance without slowing innovation.  ØªÙ„Ù…ÙŠØ­ Think of your pipeline as a highway, where every change is a vehicle. Policy checks are the toll gates ensuring only safe configurations reach production.  ","version":"Next","tagName":"h3"},{"title":"Recommended Resourcesâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#recommended-resources","content":" ","version":"Next","tagName":"h2"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantHashiCorp Terraform Associate\tHashiCorp\tCovers IaC principles, workflows, and secure deployments. Certified DevSecOps Professional (CDP)\tPractical DevSecOps\tEmphasizes Policy as Code, CI/CD integration, and automation. AWS Certified Security â€“ Specialty\tAWS\tFocuses on governance, compliance, and secure configurations. Microsoft SC-100: Cybersecurity Architect\tMicrosoft\tExplores cloud governance and enterprise security design.  ","version":"Next","tagName":"h3"},{"title":"ðŸ“š Booksâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#-books","content":" Book Title\tAuthor\tLink\tWhy Itâ€™s UsefulInfrastructure as Code: Designing and Delivering Dynamic Systems for the Cloud Age\tKief Morris\tAmazon\tExplains the core principles of scalable, repeatable IaC. Policy as Code: Improving Cloud Native Security\tJimmy Ray\tAmazon\tDeep dive into OPA, Rego, and governance automation. Terraform: Up and Running: Writing Infrastructure as Code\tYevgeniy Brikman\tAmazon\tPractical guide to building, testing, and securing Terraform workflows.  ","version":"Next","tagName":"h3"},{"title":"ðŸŽ¥ Videosâ€‹","type":1,"pageTitle":"Infrastructure as Code (IaC) Security","url":"/ar/blueprint/cloud_security_development/infrastructure-as-code-security#-videos","content":" IaC Security: Why, What, and Howâ€‹   ","version":"Next","tagName":"h3"},{"title":"Capstone - Event-Driven Security Automation","type":0,"sectionRef":"#","url":"/ar/blueprint/cloud_security_development/capstone","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#overview","content":" The goal of this capstone is to design and implement a self-healing cloud automation system that detects, remediates, and audits security misconfigurations automatically. Youâ€™ll simulate what real-world cloud security engineers do: connect events, policies, and automation to create a continuously secure environment.  In short, this project shows how modern cloud teams build Security as Code that's scalable, auditable, and intelligent.  ","version":"Next","tagName":"h2"},{"title":"Architecture Breakdownâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#architecture-breakdown","content":" Your automation pipeline will include four key security layers:  Detection Layer â€“ Uses cloud-native events to identify risky or noncompliant changes. Examples: Detect public storage buckets, IAM role modifications, or disabled logging. Some common tools you could/should use are: AWS CloudTrail, EventBridge, Azure Event Grid, or GCP Pub/Sub. Remediation Layer â€“ Executes secure automation in response. Build a serverless function (Lambda, Cloud Function, or Logic App) that automatically fixes, quarantines, or alerts on violations. Use least privilege IAM and secret injection from Vault or Secrets Manager. Observability Layer â€“ Tracks every action for audit and visibility. Send logs to CloudWatch, Log Analytics, or Stackdriver. Add alerts and dashboards to measure success, latency, and failed actions. Governance Layer â€“ Defines compliance and trust boundaries through Infrastructure as Code. Enforce security baselines with Terraform, OPA, or Tfsec and version everything for traceability.  You'll bring all of these layers together by completing the tasks below.  ","version":"Next","tagName":"h2"},{"title":"Capstone Tasksâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#capstone-tasks","content":" ","version":"Next","tagName":"h2"},{"title":"Phase 1 â€“ Foundation Setupâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#phase-1--foundation-setup","content":" Choose your preferred cloud provider (AWS, Azure, or GCP).Create a monitored resource (for example, an S3 bucket or Storage Account).Enable audit logging and monitoring for all resource activity.  âœ… Deliverable: A configured environment with event logging and monitoring enabled.  ","version":"Next","tagName":"h3"},{"title":"Phase 2 â€“ Event Detectionâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#phase-2--event-detection","content":" Create an event rule that captures security-relevant actions (for example, new public buckets, modified IAM roles).Route events to a notification system or directly to a function.Test to confirm events trigger consistently.  âœ… Deliverable: Working event detection that triggers automation on defined security actions.  ","version":"Next","tagName":"h3"},{"title":"Phase 3 â€“ Automated Remediationâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#phase-3--automated-remediation","content":" Build a serverless function that automatically responds to the event.Protect all credentials using a managed secrets service.Apply least privilege permissions to the function.Implement logging to track all actions taken.  âœ… Deliverable: A functioning, secure serverless automation that remediates or alerts on violations.  ","version":"Next","tagName":"h3"},{"title":"Phase 4 â€“ Observability and Reportingâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#phase-4--observability-and-reporting","content":" Forward all remediation logs to a central monitoring service.Add alerts for repeated or critical violations.Optionally, build a simple dashboard to visualize event trends.  âœ… Deliverable: End-to-end observability with traceable metrics and logs.  ","version":"Next","tagName":"h3"},{"title":"Phase 5 â€“ Policy as Codeâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#phase-5--policy-as-code","content":" Write IaC templates for your system using Terraform or CloudFormation.Add OPA or Tfsec policies to enforce compliance before deployment.Test the policies that you've developed.  âœ… Deliverable: A complete, versioned IaC setup that defines and enforces your entire automation system.  ","version":"Next","tagName":"h3"},{"title":"Deliverables Summarizedâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#deliverables-summarized","content":" Deliverable\tDescriptionIaC Templates\tTerraform or CloudFormation templates that deploy your automation system. Serverless Function\tEvent-driven function that performs remediation and logging. Secrets Integration\tDemonstration of secure secret retrieval using Vault or Secrets Manager. Logging + Alerts\tConfigured monitoring with traceable events and alert triggers. Documentation\tA README explaining architecture, IAM design, and deployment steps.  important Treat this project like a real-world system. Document your architecture, version your configs, and include diagrams in your README. When itâ€™s complete, make it a portfolio project you can showcase on GitHub or LinkedIn.  ","version":"Next","tagName":"h2"},{"title":"Learning Outcomesâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#learning-outcomes","content":" By completing this capstone, youâ€™ll demonstrate your ability to:  Automate event-driven detection and remediation in the cloud.Securely integrate secrets and IAM roles into automation workflows.Use APIs and SDKs to build security-aware serverless functions.Apply Infrastructure as Code and Policy as Code principles.Build a complete feedback loop with logging, monitoring, and alerting.Design scalable systems that respond and adapt to change in real time.  ","version":"Next","tagName":"h2"},{"title":"Stretch Goal: Continuous Compliance Engineâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#stretch-goal-continuous-compliance-engine","content":" Push your automation further by integrating with a CSPM or vulnerability management tool such as Wiz, Security Hub, or Trivy.  Ideas to explore:  Continuously ingest findings from these tools.Automate remediation for specific severity levels.Notify your team through Slack, Teams, or email for manual triage.  This turns your project into a Continuous Compliance Engine, which is the gold standard for modern cloud security operations.  ","version":"Next","tagName":"h2"},{"title":"Summaryâ€‹","type":1,"pageTitle":"Capstone - Event-Driven Security Automation","url":"/ar/blueprint/cloud_security_development/capstone#summary","content":" This capstone ties together everything youâ€™ve learned, from IAM fundamentals to IaC enforcement, into a single, cloud-native automation pipeline. Itâ€™s not just an exercise; itâ€™s your proof of skill as a Cloud Security Engineer capable of building secure, automated systems at scale.  ØªÙ„Ù…ÙŠØ­ Do us a huge favor, and make a post about your project on LinkedIn or any other social media applications with #DevSecBlueprint so that we can inspire others to build security into their pipelines too. ","version":"Next","tagName":"h2"},{"title":"DevSecOps Fundamentals","type":0,"sectionRef":"#","url":"/ar/blueprint/devsecops/devsecops-fundamentals","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#overview","content":" According to Red Hat, DevSecOps stands for development, security, and operations. It's an approach to culture, automation, and platform design that integrates security as a shared responsibility throughout the entire IT lifecycle.  What makes DevSecOps powerful is that it takes the principles of DevOps and extends them by embedding security into every phase of the software development lifecycle (SDLC). The ultimate goal is to shift security left, meaning security activities happen earlier in the process. This ensures vulnerabilities are identified and fixed before they can become critical issues.    Ù…Ù„Ø§Ø­Ø¸Ø© You can find the original image source here: Atlassian | DevSecOps Tools  Over time, DevSecOps has evolved from the limitations of traditional DevOps, where security was often treated as an afterthought. It emerged from the need to include security in agile and continuous delivery practices so that teams can reduce risk, improve reliability, and ensure compliance with industry standards.  ","version":"Next","tagName":"h2"},{"title":"Why DevSecOps Mattersâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#why-devsecops-matters","content":" Traditional security practices can create bottlenecks in fast-moving DevOps environments, since they typically occur at the end of the development cycle. DevSecOps solves this by integrating security from the start, enabling faster and more secure software releases. In short, DevSecOps is about prevention, not reaction.  ØªÙ„Ù…ÙŠØ­ The best DevSecOps teams view security as part of the delivery process, not something separate from it.  ","version":"Next","tagName":"h2"},{"title":"Core Principles of DevSecOpsâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#core-principles-of-devsecops","content":" To understand DevSecOps, you need to grasp its four core principles. Each one plays a role in creating a secure, collaborative, and efficient development culture.  ","version":"Next","tagName":"h2"},{"title":"1. Integration of Securityâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#1-integration-of-security","content":" Security is built into every phase of the SDLC. In fact, the Secure SDLC (SSDLC) is a direct precursor to DevSecOps. This holistic approach ensures that security is not an afterthought but a default part of how software is designed, developed, and deployed.  ","version":"Next","tagName":"h3"},{"title":"2. Automationâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#2-automation","content":" Automation ensures security checks happen consistently and efficiently without slowing developers down. Tools like static code analysis, dependency scanning, and container image scanning can be integrated directly into CI/CD pipelines to catch issues early.  important Do your best to ensure that your automation enhances, not hinders, the developer experience.  ","version":"Next","tagName":"h3"},{"title":"3. Collaborationâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#3-collaboration","content":" DevSecOps thrives on collaboration between development, operations, and security teams. By breaking down silos and sharing responsibility, teams create a unified approach to secure delivery. This shared culture helps teams make better decisions faster and ensures that everyone owns security.  ","version":"Next","tagName":"h3"},{"title":"4. Continuous Feedback and Monitoringâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#4-continuous-feedback-and-monitoring","content":" Continuous feedback loops provide real-time insight into the security posture of both applications and infrastructure. Monitoring tools detect misconfigurations, vulnerabilities, and anomalies as they occur, allowing teams to respond quickly and improve over time.  ØªÙ„Ù…ÙŠØ­ Think of monitoring as the â€œeyes and earsâ€ of DevSecOps. It turns lessons learned into actionable improvements.  ","version":"Next","tagName":"h3"},{"title":"Putting It All Togetherâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#putting-it-all-together","content":" When these four principles work together, DevSecOps transforms how organizations build and ship software:  Principle\tPurpose\tExample PracticeIntegration of Security\tBuild security into every SDLC phase\tThreat modeling, secure design reviews Automation\tReduce human error and speed delivery\tSAST, DAST, IaC scanning Collaboration\tAlign teams across disciplines\tShared Slack channels, joint retrospectives Continuous Feedback\tImprove continuously through visibility\tCentralized dashboards, alerts, metrics  ","version":"Next","tagName":"h2"},{"title":"Recommended Resourcesâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#recommended-resources","content":" Before you move onto the next section, here are some recommended resources to help you deepen your understanding of DevSecOps.  ","version":"Next","tagName":"h2"},{"title":"Booksâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#books","content":" Book Title\tAuthor\tLink\tWhy Itâ€™s UsefulThe Phoenix Project\tGene Kim, Kevin Behr, and George Spafford\tAmazon\tExplains the cultural and organizational transformation that drives successful DevOps and DevSecOps adoption. Continuous Delivery\tJez Humble and David Farley\tAmazon\tDemonstrates how to automate and streamline software delivery, forming the foundation of modern CI/CD pipelines. The DevOps Handbook\tGene Kim, Patrick Debois, John Willis, and Jez Humble\tAmazon\tProvides real-world practices for collaboration, automation, and continuous improvement across teams. Securing DevOps\tJulien Vehent\tAmazon\tBridges the gap between DevOps and security by focusing on practical techniques for securing cloud applications. DevSecOps: A Leaderâ€™s Guide to Producing Secure Software\tGlenn Wilson\tAmazon\tOffers a leadership perspective on building secure software pipelines without slowing development teams down. Cloud Native DevOps with Kubernetes\tJohn Arundel and Justin Domingus\tAmazon\tExplains how to apply DevOps and security principles in cloud-native environments using Kubernetes. Infrastructure as Code\tKief Morris\tAmazon\tTeaches how to manage infrastructure through code for consistent, automated, and secure deployments. Kubernetes Security\tLiz Rice\tAmazon\tProvides a clear, technical guide to securing Kubernetes workloads and understanding container threats.  ","version":"Next","tagName":"h3"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantCertified DevSecOps Professional (CDP)\tPractical DevSecOps\tFocuses on integrating security automation across CI/CD workflows. Certified Kubernetes Administrator (CKA)\tCNCF\tStrengthens container orchestration and security knowledge. AWS Certified DevOps Engineer â€“ Professional\tAWS\tValidates advanced knowledge of automated deployment and secure delivery. Microsoft Certified: DevOps Engineer Expert\tMicrosoft\tEmphasizes secure CI/CD and cultural collaboration. Google Professional Cloud DevOps Engineer\tGoogle Cloud\tCombines cloud-native DevOps and security best practices. ISCÂ² CSSLP\t(ISC)Â²\tConnects software security principles with continuous delivery pipelines.  ","version":"Next","tagName":"h3"},{"title":"ðŸŽ¥ YouTube Videosâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#-youtube-videos","content":" What is DevSecOps? DevSecOps explained in 8 Minsâ€‹    What is DevSecOps? DevSecOps explained in 7 Minsâ€‹    Accelerate Your DevSecOps Journey: 5 Key Skills in 5 Minutesâ€‹    What is DevSecOps? - Hackitect's Playgroundâ€‹    ","version":"Next","tagName":"h3"},{"title":"ðŸ“° Articlesâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#-articles","content":" IBM: What and Why of DevSecOpsMicrosoft: What is DevSecOps?Red Hat: What is DevSecOps?AWS Shared Responsibility Model  ","version":"Next","tagName":"h3"},{"title":"Practice What Youâ€™ve Learnedâ€‹","type":1,"pageTitle":"DevSecOps Fundamentals","url":"/ar/blueprint/devsecops/devsecops-fundamentals#practice-what-youve-learned","content":" Now that you understand the fundamentals, itâ€™s time to put them into practice.  Choose a small project (for example, a web app or microservice).Identify where security should be integrated into your CI/CD process.Add at least one automated security scan (SAST, dependency, or container).Write a short summary of how DevSecOps principles improved your workflow.  âœ… Capstone Goal: Demonstrate that you can apply DevSecOps principles in a real project by integrating security, automation, and collaboration into your delivery process.  important Remember, DevSecOps isnâ€™t about adding more tools. Itâ€™s about changing how teams think about security every day. ","version":"Next","tagName":"h2"},{"title":"Threat Modeling Fundamentals","type":0,"sectionRef":"#","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#overview","content":" Threat modeling is the process of identifying potential security risks within a system, understanding how those threats might be realized, and defining controls to mitigate them. Itâ€™s about anticipation rather than reaction :designing software with security built-in, not bolted on.  important Threat modeling isnâ€™t a one-time activity. Itâ€™s a living process that evolves with your architecture, your codebase, and your threat landscape.  ","version":"Next","tagName":"h2"},{"title":"The Purpose of Threat Modelingâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#the-purpose-of-threat-modeling","content":" Threat modeling allows teams to:  Identify what to protect :the assets, data, and functionality that matter most.Anticipate what could go wrong :both intentional (attacks) and accidental (misconfigurations).Prioritize mitigations :so security effort is focused where it counts.Build shared understanding :aligning developers, architects, and security engineers around real-world risks.  Ultimately, threat modeling helps you design with intent (not just &quot;make it workâ€) but &quot;make it secure by design.â€  ","version":"Next","tagName":"h2"},{"title":"Where It Fits in the Secure SDLCâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#where-it-fits-in-the-secure-sdlc","content":" Threat modeling primarily happens during the Design Phase of the Secure SDLC, right after youâ€™ve gathered requirements and before writing a single line of code.  However, in modern DevSecOps environments, itâ€™s also:  Revisited during development (when new features are added)Reassessed during testing (when vulnerabilities are found)Updated during maintenance (when new threats emerge)  important Treat threat modeling like code. Store your diagrams and notes in version control and update them when your architecture changes. You'll save yourself a LOT of time, I promise ðŸ˜„  ","version":"Next","tagName":"h2"},{"title":"The Four Core Questionsâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#the-four-core-questions","content":" Microsoftâ€™s foundational approach to threat modeling revolves around four key questions:  What are we building? Define the systemâ€™s purpose, architecture, data flows, and dependencies.What can go wrong? Identify how attackers could exploit weaknesses.What are we going to do about it? Define mitigations, compensating controls, or design changes.Did we do a good job? Review and iterate :threat modeling is never &quot;done.â€  These questions form the heartbeat of any effective threat modeling session.  ","version":"Next","tagName":"h2"},{"title":"Common Methodologiesâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#common-methodologies","content":" Different frameworks exist to structure your thinking. The three most common are:  Methodology\tFocus Area\tWhen to Use ItSTRIDE (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege)\tIdentifying threats in system design and data flows\tWhen designing applications or APIs PASTA (Process for Attack Simulation and Threat Analysis)\tEnd-to-end risk-driven approach combining business impact and attack simulation\tWhen modeling complex enterprise systems LINDDUN\tPrivacy threat modeling (Linkability, Identifiability, Non-repudiation, Detectability, Disclosure, Unawareness, Non-compliance)\tWhen focusing on user data protection and privacy laws  ØªÙ„Ù…ÙŠØ­ Most developers start with STRIDE, because itâ€™s simple, structured, and fits perfectly into early design reviews.  ","version":"Next","tagName":"h2"},{"title":"Elements of a Threat Modelâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#elements-of-a-threat-model","content":" Every good threat model includes the following building blocks. Think of these as the &quot;what,â€ &quot;how,â€ and &quot;whereâ€ of your systemâ€™s security story.  ","version":"Next","tagName":"h2"},{"title":"1. Assets â€” What are we protecting?â€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#1-assets--what-are-we-protecting","content":" These are the things that matter most. Examples include credentials, customer data, API keys, and source code. Ask yourself:  &quot;If this was stolen or changed, would it impact my users or business?â€  ","version":"Next","tagName":"h3"},{"title":"2. Data Flows â€” How does data move?â€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#2-data-flows--how-does-data-move","content":" Data flows describe how information travels through your system, such as who sends it, who receives it, and how. By mapping these flows, you can see where sensitive data is created, stored, or transmitted.  ØªÙ„Ù…ÙŠØ­ Use arrows to show direction and label where encryption or access controls apply.  ","version":"Next","tagName":"h3"},{"title":"3. Trust Boundaries â€” Where does trust change?â€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#3-trust-boundaries--where-does-trust-change","content":" A trust boundary marks a shift from one security zone to another. For example, when data leaves a userâ€™s browser and enters your backend API. These are the areas where you should apply the strongest checks, like authentication, validation, and input filtering.  ","version":"Next","tagName":"h3"},{"title":"4. Threats â€” What can go wrong?â€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#4-threats--what-can-go-wrong","content":" Once you understand your system, think like an attacker. Ask questions such as:  Could someone steal or guess credentials?Could a request be tampered with?Could data leak from logs or error messages?  Frameworks like STRIDE help you stay organized when identifying risks.  ","version":"Next","tagName":"h3"},{"title":"5. Mitigations â€” What can we do about it?â€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#5-mitigations--what-can-we-do-about-it","content":" For every threat you find, define what protects against it. Examples include:  Using encryption for data at rest and in transitValidating input and sanitizing outputEnforcing least privilege accessAdding rate limits and monitoring for anomalies  ","version":"Next","tagName":"h3"},{"title":"6. Attack Vectors â€” How could someone get in?â€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#6-attack-vectors--how-could-someone-get-in","content":" Attack vectors are the ways an attacker might reach your system. Understanding them helps you decide what controls to add.  Category\tExample\tMitigationWeb/API\tSQL injection or weak tokens\tInput validation, WAF, short-lived tokens Network\tNo TLS, exposed ports\tEnforce HTTPS, firewall rules Secrets\tKeys in code or CI logs\tStore in Vault or Secrets Manager Third-Party\tUnverified webhooks\tUse signature validation DoS\tResource exhaustion\tRate limiting, autoscaling  ","version":"Next","tagName":"h3"},{"title":"Example Data Flow Diagram (DFD)â€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#example-data-flow-diagram-dfd","content":" Below is a simple example of what a DFD might look like. It shows how users, apps, and services interact, and where trust boundaries live.    Ù…Ù„Ø§Ø­Ø¸Ø© You can find the original source for this image here. Also, Start simple. Even a hand-drawn diagram on a whiteboard can help your team understand where to focus defenses.  ","version":"Next","tagName":"h3"},{"title":"âš™ï¸ Example Threat: A Web App Loginâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#ï¸-example-threat-a-web-app-login","content":" Letâ€™s model a simple login page that sends credentials to a backend API connected to a database.  STRIDE Category\tThreat Example\tMitigationSpoofing\tAttacker impersonates a legitimate user via stolen credentials.\tImplement MFA and strong authentication. Tampering\tLogin request modified in transit.\tUse HTTPS/TLS; validate request integrity. Repudiation\tUser denies having performed a login.\tEnable detailed audit logging. Information Disclosure\tSensitive data exposed through verbose error messages.\tMask errors; avoid returning stack traces. Denial of Service\tMultiple failed logins overload the backend.\tImplement rate limiting or CAPTCHA. Elevation of Privilege\tRegular user gains admin rights.\tUse role-based access control (RBAC) and least privilege.  Ù…Ù„Ø§Ø­Ø¸Ø© You can perform this same analysis for any data flow :API requests, CI/CD pipelines, or cloud resources.  ","version":"Next","tagName":"h2"},{"title":"Threat Modeling in DevSecOpsâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#threat-modeling-in-devsecops","content":" In modern DevSecOps environments, threat modeling shouldnâ€™t be a big-bang meeting that happens once. It should be lightweight, continuous, and collaborative.  ","version":"Next","tagName":"h2"},{"title":"Practical Integration Ideasâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#practical-integration-ideas","content":" Add a threat modeling checklist to your pull requests.Conduct short &quot;threat-stormingâ€ sessions in sprint planning.Automate simple model generation using tools like Threat Dragon, IriusRisk, or Threagile.Store your models in GitHub :versioned like code.  ØªÙ„Ù…ÙŠØ­ Threat modeling is not just for architects. Developers, testers, and ops engineers should all contribute, because threats evolve with how systems are actually used.  ","version":"Next","tagName":"h3"},{"title":"ðŸ” Applied Reflection (Capstone Prompt)â€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#-applied-reflection-capstone-prompt","content":" Now that you understand the fundamentals, itâ€™s time to apply what youâ€™ve learned.  Imagine youâ€™re designing a small microservice-based application or a CI/CD pipeline for one of your own projects.  List the main components and data flows.Identify at least three threats using STRIDE categories.Define mitigations you would implement to reduce risk.(Optional) Draw a simple diagram showing your data flow and trust boundaries.  âœ… Capstone Goal: Demonstrate that you can think critically about how your system could fail and how to build resilience in by design.  Ù…Ù„Ø§Ø­Ø¸Ø© The more you practice this, the faster youâ€™ll be able to identify weak points during architecture or code reviews.  ","version":"Next","tagName":"h2"},{"title":"Recommended Resourcesâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#recommended-resources","content":" ","version":"Next","tagName":"h2"},{"title":"Booksâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#books","content":" Book Title\tAuthor\tLink\tWhy Itâ€™s UsefulThreat Modeling: Designing for Security\tAdam Shostack\tAmazon\tThe definitive guide to modern threat modeling and understanding attacker thinking during design. Securing Systems\tBrook S. E. Schoenfield\tAmazon\tProvides a structured approach to building secure architectures and embedding security into system design. Threat Modeling: A Practical Guide for Development Teams\tIzar Tarandach and Matthew Coles\tAmazon\tOffers practical, team-focused strategies for applying threat modeling consistently across real-world projects.  ","version":"Next","tagName":"h3"},{"title":"YouTube Videosâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#youtube-videos","content":" What is Threat Modeling and Why Is It Important?â€‹    STRIDE Threat Modeling for Beginnersâ€‹    ","version":"Next","tagName":"h3"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"Threat Modeling Fundamentals","url":"/ar/blueprint/devsecops/threat-modeling-fundamentals#recommended-certifications","content":" Certification\tProvider\tRelevanceCertified Threat Modeling Professional (CTMP)\tThreatModeler\tFocused on enterprise-scale modeling. CSSLP\tISCÂ²\tEmphasizes secure design and lifecycle integration. CDP (Certified DevSecOps Professional)\tPractical DevSecOps\tTies threat modeling into CI/CD and automation. ","version":"Next","tagName":"h3"},{"title":"Capstone - DevSecOps Pipeline","type":0,"sectionRef":"#","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#overview","content":" The goal of this capstone is to design and implement a fully functional, application-centric DevSecOps pipeline that enforces security at every stage, from code commit to deployment. Youâ€™ll simulate what real-world engineering teams do: integrate static analysis, dependency checks, runtime testing, and secret scanning all through CI/CD automation.  In short, this project highlights how security can be developer-friendly, automated, and actionable, ensuring vulnerabilities are caught early and fixed fast.  ","version":"Next","tagName":"h2"},{"title":"Pipeline Breakdownâ€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#pipeline-breakdown","content":" Your DevSecOps pipeline will include four key security layers:  Code Security (SAST + SCA) â€“ Detect vulnerabilities in your source code and dependencies before deployment.Runtime Security (DAST) â€“ Test your running application for real-world exploit paths.Secrets &amp; Configuration Management â€“ Prevent accidental credential exposure in your codebase and pipeline.Continuous Feedback &amp; Reporting â€“ Provide visibility to developers through reports, badges, and notifications.  You'll bring it all together by doing all of the tasks below.  ØªÙ„Ù…ÙŠØ­ If you're looking for a great place or example to start from, check out the GitHub Actions DevSecOps Pipeline that has been developed by the DSB Community.  ","version":"Next","tagName":"h2"},{"title":"Capstone Tasksâ€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#capstone-tasks","content":" ","version":"Next","tagName":"h2"},{"title":"Phase 1 â€“ Code Security (SAST + SCA)â€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#phase-1--code-security-sast--sca","content":" Focus on securing your codebase and open-source dependencies. To make this happen, you'll want to:  Integrate SAST using Semgrep, SonarQube, or CodeQL.Add SCA (Software Composition Analysis) using Trivy, Snyk, or OWASP Dependency-Check.Configure both to run automatically on each pull request or code push.  âœ… Deliverable: A CI/CD stage that fails builds on high-severity vulnerabilities and reports findings in PR comments or pipeline logs.  ","version":"Next","tagName":"h3"},{"title":"Phase 2 â€“ Runtime Security (DAST)â€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#phase-2--runtime-security-dast","content":" Simulate attacks to identify vulnerabilities in your deployed application. To make this happen, you'll want to:  Deploy your app locally or within a test container.Use OWASP ZAP, Nikto, or StackHawk for dynamic scanning.Archive results for visibility and trend tracking.  âœ… Deliverable: A DAST stage that runs after deployment and generates reports in your pipeline output.  ","version":"Next","tagName":"h3"},{"title":"Phase 3 â€“ Secrets & Configuration Managementâ€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#phase-3--secrets--configuration-management","content":" Even great code can fail if secrets leak. Secure your pipeline configurations. To make this happen, you'll want to:  Implement GitLeaks or TruffleHog for pre-commit or CI/CD secret scanning.Use GitHub Encrypted Secrets, or Vault for secure variable management.Enforce scanning policies that prevent secret commits.  âœ… Deliverable: A verified configuration that prevents credentials from being committed or printed in pipeline logs.  ","version":"Next","tagName":"h3"},{"title":"Phase 4 â€“ Continuous Feedback & Reportingâ€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#phase-4--continuous-feedback--reporting","content":" Automate visibility and make security results meaningful to your team. To make this happen, you'll want to:  Aggregate scan results into Markdown or HTML reports.Notify developers via Slack, email, or GitHub comments.Add a â€œSecurity Scans Passingâ€ badge to your README.  âœ… Deliverable: An automated reporting system that keeps developers informed and accountable for security outcomes.  ","version":"Next","tagName":"h3"},{"title":"Deliverables Summarizedâ€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#deliverables-summarized","content":" Deliverable\tDescriptionCI/CD Pipeline Configuration\tFully automated pipeline integrating SAST, SCA, DAST, and secret scanning. Reports and Artifacts\tArchived scan outputs (Markdown, HTML, or JSON). Security Badges and Notifications\tVisual pipeline feedback (e.g., &quot;Scans Passing&quot; badge). Documentation\tA concise README explaining tools, pipeline logic, and integration steps.  important Treat this project like a production system. Write documentation, version your configs, and showcase it in your portfolio. If I were you, I'd write a post about it on Hashnode or dev.to, and post about it on LinkedIn.  ","version":"Next","tagName":"h2"},{"title":"Learning Outcomesâ€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#learning-outcomes","content":" By completing this capstone, youâ€™ll demonstrate your ability to:  Automate static, dynamic, and dependency scanning in application pipelines.Implement secure CI/CD workflows using tools like Jenkins, GitHub Actions, or GitLab CI.Manage secrets and environment variables securely.Build feedback loops that make security findings visible and actionable.Apply DevSecOps principles to improve software resilience without slowing delivery.  ","version":"Next","tagName":"h2"},{"title":"Stretch Goal: Unified DevSecOps Workflowâ€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#stretch-goal-unified-devsecops-workflow","content":" Push your pipeline further by combining all scans into a single orchestrated workflow with centralized reporting. Here are some high level ideas or requirements:  Trigger all security stages from a single â€œSecurity Scanâ€ job.Aggregate results in DefectDojo, GitHub Security Dashboard, or custom dashboards.Generate consolidated risk scores for every build.  This transforms your CI/CD into a Security Quality Gate, where code quality and security share the same metrics.  ","version":"Next","tagName":"h2"},{"title":"Summaryâ€‹","type":1,"pageTitle":"Capstone - DevSecOps Pipeline","url":"/ar/blueprint/devsecops/capstone-end-to-end-application-devsecops-pipeline#summary","content":" This capstone ties together everything from secure coding to automation, giving you hands-on experience with what real DevSecOps teams do daily. When complete, youâ€™ll have a tangible, portfolio-ready project proving your ability to secure modern applications through automation and collaboration.  ØªÙ„Ù…ÙŠØ­ Do us a huge favor, and make a post about your project on LinkedIn or any other social media applications with #DevSecBlueprint so that we can inspire others to build security into their pipelines too. ","version":"Next","tagName":"h2"},{"title":"Introduction To Soft Skills","type":0,"sectionRef":"#","url":"/ar/blueprint/soft-skills","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Introduction To Soft Skills","url":"/ar/blueprint/soft-skills#overview","content":" Before we dive into the technical theory and such, there is one thing I want to get across to you all: Technical skills are just half of the battle. You need to ensure that you have the soft skills. You need to make sure you know how to effectively communicate, collaborate, and have the ability to adapt to any situation that is thrown at you.  ","version":"Next","tagName":"h2"},{"title":"Key Soft Skills & Behaviorsâ€‹","type":1,"pageTitle":"Introduction To Soft Skills","url":"/ar/blueprint/soft-skills#key-soft-skills--behaviors","content":" These are the key soft skills that you need to hone in order to be an effective professional within all of information technology, as these are in fact transferrable skills. I've outlined them and adding in some tips underneath each one.  ","version":"Next","tagName":"h2"},{"title":"1. Communicationâ€‹","type":1,"pageTitle":"Introduction To Soft Skills","url":"/ar/blueprint/soft-skills#1-communication","content":" Active Listening: Foster open discussions, ensuring all voices are heard and understood.Clear, Concise Writing: In an IT environment that often relies on asynchronous communication (emails, tickets, or documentation), well-written messages prevent miscommunication.Constructive Feedback: Provide actionable, timely, and respectful feedback, focusing on growth and improvement.  ","version":"Next","tagName":"h3"},{"title":"2. Collaborationâ€‹","type":1,"pageTitle":"Introduction To Soft Skills","url":"/ar/blueprint/soft-skills#2-collaboration","content":" Cross-Functional Teams: Collaborate across disciplines (e.g., development, operations, security, and business units) to align goals and achieve optimal outcomes.Shared Knowledge: Maintain accessible and up-to-date documentation (e.g., wikis, knowledge bases) to promote transparency and efficiency.Empathy: Understand the pressures and constraints faced by others to foster trust and mutual respect.  ","version":"Next","tagName":"h3"},{"title":"3. Adaptabilityâ€‹","type":1,"pageTitle":"Introduction To Soft Skills","url":"/ar/blueprint/soft-skills#3-adaptability","content":" Embrace Change: Stay flexible and open to evolving technologies, methodologies, and organizational needs.Continuous Learning: Commit to ongoing skill development and staying informed about industry trends.Resilience: Approach unexpected challenges with a problem-solving mindset and perseverance.  ","version":"Next","tagName":"h3"},{"title":"4. Problem-Solvingâ€‹","type":1,"pageTitle":"Introduction To Soft Skills","url":"/ar/blueprint/soft-skills#4-problem-solving","content":" Critical Thinking: Analyze complex issues systematically to identify root causes and potential solutions.Innovation: Leverage creative thinking to design novel approaches or improve existing workflows.Collaboration in Problem-Solving: Engage with diverse perspectives to tackle challenges collaboratively.  ","version":"Next","tagName":"h3"},{"title":"5. Conflict Resolutionâ€‹","type":1,"pageTitle":"Introduction To Soft Skills","url":"/ar/blueprint/soft-skills#5-conflict-resolution","content":" Focus on Common Goals: Reframe conflicts by identifying shared objectives and aligning on priorities.Encourage Healthy Debate: Create an environment where differing opinions are respected and explored constructively.Blame-Free Resolution: Shift focus from assigning blame to identifying solutions and implementing improvements.  ","version":"Next","tagName":"h3"},{"title":"Recommended Reading & Resourcesâ€‹","type":1,"pageTitle":"Introduction To Soft Skills","url":"/ar/blueprint/soft-skills#recommended-reading--resources","content":" Below are some resources to help you dive deeper into building those skills that will take you quite far. Most of these books that I've recommended are unrelated to the field, as they are focused more on how to communicate and collaborate. They'll build those inter and intra personal skills that you need.  ","version":"Next","tagName":"h2"},{"title":"Booksâ€‹","type":1,"pageTitle":"Introduction To Soft Skills","url":"/ar/blueprint/soft-skills#books","content":" Book Title\tAuthor(s)\tLinkHow to Win Friends and Influence People\tDale Carnegie\thttps://amzn.to/3DIitxx 7 Habits of Highly Effective People\tStephen R. Covey\thttps://amzn.to/4abqB5P Active Listening Techniques\tNixaly Leonardo, LCSW\thttps://amzn.to/422bee6 Read Peoplie\tRita Carter\thttps://amzn.to/3C3cumw The 3rd Alternative\tSteven Covey\thttps://amzn.to/4ijLgb1 Exactly What to Say\tPhil Jones\thttps://amzn.to/3EGcrhr How to Talk So Little Kids Will Listen\tJoanna Faber\thttps://amzn.to/4gIOX8I Never Split the Difference\tChris Voss\thttps://amzn.to/4gIMnzM  ","version":"Next","tagName":"h3"},{"title":"What You Should Have Learnedâ€‹","type":1,"pageTitle":"Introduction To Soft Skills","url":"/ar/blueprint/soft-skills#what-you-should-have-learned","content":" By the end of this section, you should:  Understand the Importance of Soft Skills: Recognize how communication, collaboration, and adaptability contribute to both individual and team success.Gain Insights into Building Trust: Learn how empathy and shared knowledge foster stronger professional relationships.Embrace Continuous Improvement: Appreciate the value of reflecting on past experiences, seeking feedback, and adapting to new challenges.Adopt Practical Strategies: Acquire actionable tips for improving communication, managing conflicts, and solving problems effectively. ","version":"Next","tagName":"h2"},{"title":"What is the Secure SDLC?","type":0,"sectionRef":"#","url":"/ar/blueprint/devsecops/what-is-the-secure-sdlc-and-sdlc","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"What is the Secure SDLC?","url":"/ar/blueprint/devsecops/what-is-the-secure-sdlc-and-sdlc#overview","content":" One of the most critical building blocks is the Secure Software Development Life Cycle (Secure SDLC). By establishing a strong understanding of Secure SDLC, you will be better equipped to comprehend how security is integrated throughout the development lifecycle.  ","version":"Next","tagName":"h2"},{"title":"What is the SDLC?â€‹","type":1,"pageTitle":"What is the Secure SDLC?","url":"/ar/blueprint/devsecops/what-is-the-secure-sdlc-and-sdlc#what-is-the-sdlc","content":"   Ù…Ù„Ø§Ø­Ø¸Ø© You can find the original image source here: Software Development Life Cycle (SDLC) | Snyk  The Software Development Life Cycle (SDLC) is a structured process used for developing software applications. To keep it short, the SDLC consists of six key phases:  Planning and Requirements Gathering: Understanding what the software needs to do and ensuring it aligns with business requirements.Design: Architecting the solution to meet functional and non-functional requirements.Development: Writing the actual code... or programming.Testing: Ensuring that the software works as intended and is free from bugs (that don't impact the features).Deployment: Releasing the software into higher environments (QA, Production)Maintenance: Ongoing updates/enhancements and fixes post-release  The downside to this process is that there is no security baked into any of phases. Formally known as the traditional SDLC, when developers follow this model, security is often treated as an afterthought and addressed after the deployment phase and well into the maintenance phase (and I do mean well into the maintanence phase). This reactive approach can result in security issues being discovered late, which can be quite costly and disruptive to fix overtime. So, when you're developing applications of any kind, pretty pretty please keep this in mind.  ","version":"Next","tagName":"h2"},{"title":"The Secure SDLCâ€‹","type":1,"pageTitle":"What is the Secure SDLC?","url":"/ar/blueprint/devsecops/what-is-the-secure-sdlc-and-sdlc#the-secure-sdlc","content":" Now that we've covered the SDLC at a high-level, let's talk about the replacement (or the better process to follow).  The Secure SDLC (SSDLC) is an evolution of the traditional SDLC model, where security is a key consideration at every phase of the process. Rather than treating security as a final step, it becomes an significant part of each phase by helping to reduce vulnerabilities and risks earlier in the lifecycle. So, when you the term &quot;shift-left&quot; or &quot;shifting-left&quot;, I want you think about the SSDLC, because that's essentially what we're doing. We are moving security from the end of the SDLC and integrating it into every phase within the SDLC.  One of the key benefits of the SSDLC is that you are finding and figuring out any security issues as you iterate through the Secure SDLC, which overtime helps save cost and eliminates the overhead and potential of releasing vulnerabilities into the wild.    Ù…Ù„Ø§Ø­Ø¸Ø© You can find the original image source here: Secure Software Development Life Cycle (SSDLC) | Snyk  ","version":"Next","tagName":"h2"},{"title":"Key Phases of the Secure SDLCâ€‹","type":1,"pageTitle":"What is the Secure SDLC?","url":"/ar/blueprint/devsecops/what-is-the-secure-sdlc-and-sdlc#key-phases-of-the-secure-sdlc","content":" There are 6 key phases that you should know:  Planning and Requirements Gathering (with Security in Mind) At this phase, itâ€™s crucial to gather both functional and security requirements. By considering security from the outset, you ensure that the software design accounts for potential threats and compliance with security standards such as GDPR, HIPPA, etc. Some example security activities that you should be aware of that happens at this phase include: Threat modeling (very... very... important)Conducting risk assessmentsComplaince mapping Design (Secure Architecture) During the design phase, architectural decisions should be made with security as a priority. This involves creating a robust design that can mitigate common security threats, which is SUPER important. Starting off with security in the design ensures that the developers code securely. Some example security activities that you should be aware of that happens at this phase include: Identifying security design patternsDefining security controlsIdentifying attack vectors and ways to mitigate them. Development (Secure Coding Practices) This is my favorite phase, because this is where the magic happens. However, you don't really get anywhere without coding securely. Secure coding is the first line of defense for your application. You're literally ensuring that you are preventing vulnerabilites by coding in secure manner and following best practices for preventing things like SQL Injection and Cross-Site Scripting (XSS). The best practices will differ based on the language that you're coding in, but the concept itself is transferable. Some example security activities that you should be aware of that happens at this phase include: Performing code reviews and pair programmingExecuting static application security testing (SAST) scansChecking your dependencies for vulnerabilities by running dependency scans against them. Testing (Security Testing) Okay... this is my second favorite phase because you can to see if what you built truly works and is secured properly by performing automated and manual security tests. To add context, automated and manual security testing should be embedded in this phase to catch vulnerabilities early. Instead of relying solely on traditional testing, specific security tests like penetration testing and dynamic analysis are key. This can be achieved by: Performing or implementing Dynamic Application Security Testing (DAST) scanningConducting penetration tests,Finding and using fuzz testing tools. Deployment (Secure Configuration and Monitoring) In this phase, security continues during deployment by ensuring that applications are deployed securely. This includes using secure configurations, Infrastructure as Code (IaC) security, and container security practices. Some example security activities that you should be aware of that happens at this phase include: The reviewal of deployment configurations to ensure that they adhere to best practicesContainer hardening and scanningEnsuring least privilege access controls for your application or infrastructure. Maintenance (Continuous Security and Monitoring) After deployment, the application enters the maintenance phase, where itâ€™s essential to continue monitoring for new vulnerabilities and regularly apply patches or updates. If you did everything correctly the first time, your application should be pretty secure. From a security standpoint though, some common activities that happen in this phase that you should be aware of is: Implementing continous monitoring solutions and processesCreating patch management processesDeveloping and implementing an incident response plan (security and operations).  ","version":"Next","tagName":"h3"},{"title":"Recommended Resourcesâ€‹","type":1,"pageTitle":"What is the Secure SDLC?","url":"/ar/blueprint/devsecops/what-is-the-secure-sdlc-and-sdlc#recommended-resources","content":" Before we move onto the next section, here are some resources that I believe you should look into to help you better understand the SDLC and SSDLC:  ","version":"Next","tagName":"h2"},{"title":"Booksâ€‹","type":1,"pageTitle":"What is the Secure SDLC?","url":"/ar/blueprint/devsecops/what-is-the-secure-sdlc-and-sdlc#books","content":" Book Title\tAuthor\tLink\tWhy Itâ€™s UsefulThreat Modeling: Designing for Security\tAdam Shostack\tAmazon\tProvides a complete, practical framework for identifying and mitigating threats during software design. Designing Secure Software: A Guide for Developers\tLoren Kohnfelder\tAmazon\tTeaches how to design systems with security in mind from the start, covering architecture, risk, and secure patterns. Clean Code: A Handbook of Agile Software Craftsmanship\tRobert C. Martin\tAmazon\tEmphasizes writing maintainable and readable code, which is essential for reducing security flaws and human error.  ","version":"Next","tagName":"h3"},{"title":"YouTube Videosâ€‹","type":1,"pageTitle":"What is the Secure SDLC?","url":"/ar/blueprint/devsecops/what-is-the-secure-sdlc-and-sdlc#youtube-videos","content":" Secure SDLCâ€‹    Introduction To The Software Development Life Cycle (SDLC)â€‹    Agile vs Waterfall Methodlogyâ€‹    What is Threat Modeling and Why Is It Important?â€‹    ","version":"Next","tagName":"h3"},{"title":"Recommended Certificationsâ€‹","type":1,"pageTitle":"What is the Secure SDLC?","url":"/ar/blueprint/devsecops/what-is-the-secure-sdlc-and-sdlc#recommended-certifications","content":" Certification\tProvider\tWhy Itâ€™s RelevantCertified Secure Software Lifecycle Professional (CSSLP)\t(ISC)Â²\tFocuses on integrating security at every phase of the software lifecycle. CompTIA Security+\tCompTIA\tEstablishes foundational security awareness applicable to the SDLC. AWS Certified DevOps Engineer â€“ Professional\tAWS\tCovers CI/CD automation, monitoring, and governance, aligning with SSDLC practices. Microsoft Certified: DevOps Engineer Expert\tMicrosoft\tDemonstrates knowledge of DevSecOps integration and secure delivery pipelines. Google Professional Cloud DevOps Engineer\tGoogle Cloud\tEmphasizes continuous delivery and secure deployment processes within SSDLC frameworks.  ","version":"Next","tagName":"h3"},{"title":"Practice What Youâ€™ve Learnedâ€‹","type":1,"pageTitle":"What is the Secure SDLC?","url":"/ar/blueprint/devsecops/what-is-the-secure-sdlc-and-sdlc#practice-what-youve-learned","content":" Design and document a Secure SDLC blueprint for a sample application of your choice â€” for example, a small e-commerce API or task manager app.  Map each SDLC phase to a corresponding security activity (like SAST, threat modeling, or IaC review).Choose at least one tool per phase (for example, OWASP Threat Dragon for threat modeling, Semgrep for code scanning, Trivy for container scanning).Create a short Secure SDLC flow diagram using Lucidchart, Excalidraw, or Draw.io that visually shows where each control fits.Bonus: integrate one step (like dependency scanning) into a CI/CD workflow to show what â€œshift-leftâ€ means in action.  âœ… Capstone Goal: Demonstrate understanding of how to integrate security throughout all stages of software delivery â€” from planning to maintenance. Your deliverable should serve as a reusable Secure SDLC template that can guide future development teams. ","version":"Next","tagName":"h2"},{"title":"DevSec Blueprint (DSB) Community Landing Page","type":0,"sectionRef":"#","url":"/ar/community/","content":"","keywords":"","version":"Next"},{"title":"Share your inspired contentâ€‹","type":1,"pageTitle":"DevSec Blueprint (DSB) Community Landing Page","url":"/ar/community/#share-your-inspired-content","content":" Have you created a project or article inspired by the DSB? We'd love to add it to this page!  ","version":"Next","tagName":"h2"},{"title":"Use our submission formâ€‹","type":1,"pageTitle":"DevSec Blueprint (DSB) Community Landing Page","url":"/ar/community/#use-our-submission-form","content":" You can submit your article or project using our Google Form:  Submit Your Contribution  You can also join our Discord server to discuss your inspired content with the community.  ","version":"Next","tagName":"h3"},{"title":"Articles and publicationsâ€‹","type":1,"pageTitle":"DevSec Blueprint (DSB) Community Landing Page","url":"/ar/community/#articles-and-publications","content":" Here's a selection of articles, posts, and publications that mention or are inspired by the DSB:  Author\tTitle\tLink\tDateOuail Ozennou\t&quot;DevSecOps Pipeline&quot;\tLinkedIn Post\tApril 2024 Gregory East\tBuilding a Cloud-Native DevSecOps Pipeline on AWS with Terraform Cloud\tMedium Article\tDecember 2024 Gregory East\tGitHub Actions - DevSecOps\tMedium Article\tApril 2025 Sam Ezebaunandu\tA crash course on OpenTelemetry\tArticle Link\tMarch 2025 Abdulhakeem Sulaiman\tHow to Build a Three-Tier End-to-End DevSecOps Pipeline\tMedium Article\tMay 2025 Abdulhakeem Sulaiman\tHow to Build an Automated AWS Incident Response Bot with Terraform, GuardDuty, EventBridge, Lambda, and Slack\tMedium Article\tAugust 4th, 2025  ","version":"Next","tagName":"h2"},{"title":"Community Impactâ€‹","type":1,"pageTitle":"DevSec Blueprint (DSB) Community Landing Page","url":"/ar/community/#community-impact","content":" We are constantly inspired by how our community uses DevSec Blueprint to learn, teach, and innovate in the field of application security. Each inspired content strengthens our mission to make DevSecOps learning accessible to everyone.  If you are using the DSB in your learning or professional journey, don't hesitate to share your experience with us! ","version":"Next","tagName":"h2"},{"title":"Projects","type":0,"sectionRef":"#","url":"/ar/projects/","content":"","keywords":"","version":"Next"},{"title":"Purposeâ€‹","type":1,"pageTitle":"Projects","url":"/ar/projects/#purpose","content":" Welcome to the Projects section - my most favorite part of the blueprint. This is where you'll be able to get the hands-on experience that's designed to helping you elevate your skills and knowledge.  Whether you're looking to build your expertise or tackle real-world challenges, these projects offer you the opportunity to apply what you've learned in a practical, impactful way.  ","version":"Next","tagName":"h2"},{"title":"List of Projectsâ€‹","type":1,"pageTitle":"Projects","url":"/ar/projects/#list-of-projects","content":" Title\tDescription\tAuthorDevSecOps Home Lab\tThis comprehensive tutorial guides you through setting up your own home lab using a variety of tools. It's a real-world solution you can implement at home or in the cloud and will provide you with the skills needed to perform in a DevSecOps role.\tDamien Burks DevSecOps Pipeline - Amazon Web Services (AWS)\tThis tutorial guides you through setting up a cloud-native DevSecOps pipeline within AWS. It's a real-world solution (technically) you can implement that will expose you and give you cloud experience for a AWS DevSecOps role, along with some software engineering and scripting experience with Python.\tDamien Burks DevSecOps Pipeline - Google Cloud Platform (GCP)\tThis tutorial guides you through setting up a cloud-native DevSecOps pipeline within GCP. It's a real-world solution (technically) you can implement that will expose you and give you cloud experience for a GCP DevSecOps role, along with some software engineering and scripting experience with Python.\tIman Crooks, Damien Burks DevSecOps Pipeline - Azure\tThis tutorial walks you through building a DevSecOps pipeline using Azure DevOps. Itâ€™s a practical, real-world setup that gives you hands-on experience in securing CI/CD workflows with tools like Trivy and OWASP ZAP, while deploying containerized applications into Azure Container Registry (ACR). You'll also gain exposure to Terraform Cloud for infrastructure automation, federated identity for passwordless authentication, and modern software delivery practices within the Azure ecosystem.\tTimothy Hogue, Damien Burks DevSecOps Pipeline - GitHub Actions\tThis tutorial walks you through building a DevSecOps pipeline using GitHub Actions. Itâ€™s a practical, real-world setup that gives you hands-on experience in securing CI/CD workflows with tools like SonarCloud, Trivy, and OWASP ZAP, which is perfect for anyone targeting a GitHub-driven or DevSecOps-focused role. You'll also gain exposure to Python-based microservices, containerization with Docker, and modern software delivery practices using GitHub Container Registry (GHCR).\tTimothy Hogue, Damien Burks ","version":"Next","tagName":"h2"},{"title":"Prerequisites","type":0,"sectionRef":"#","url":"/ar/prerequisites","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Prerequisites","url":"/ar/prerequisites#overview","content":" Before diving into the core concepts and theories of DevSecOps &amp; Cloud Security Development, it's important to build a solid foundation with some essential background knowledge. The topics/concepts that I am going to recommend will be extremely beneficial for you to understand.. It'll also help you with going through the docs and understanding the flow as well.  ","version":"Next","tagName":"h2"},{"title":"Concepts You'll Need To Knowâ€‹","type":1,"pageTitle":"Prerequisites","url":"/ar/prerequisites#concepts-youll-need-to-know","content":" Concept\tReason(s) You Need To Know It\tResourcesVersion Control System (Git)\tYou'll need to understand what Version Control is, and have a fundamental understanding of Git. Understanding this concepts will help you understand how the files are checked out from a VCS (Version Control System) during pipeline builds. The DevOps Guide - Version Control System Linux and Bash Scripting\tFamiliarity with Linux is essential, as it's a common environment in DevSecOps workflows. You'll need to know how to navigate the Linux command line, manage files and processes, and automate tasks using Bash scripting. This knowledge will be key as you begin to secure and automate processes within your infrastructure. What is Linux?Learn To Cloud - Phase 1 Programming Concepts\tA basic understanding of programming is necessary for DevSecOps. You'll be working with various scripts, tools, and automation processes that require coding skills. Whether you're familiar with Python, Java, or another language, being comfortable with programming concepts will enable you to create and manage secure, automated workflows effectively. Introduction to Programming and Computer ScienceThe Complete Python Bootcamp From Zero to Hero in PythonLearn To Cloud - Phase 2 Introduction to Programming and Computer Science - Full Course Networking\tUnderstanding the basics of networking is vital for securing and managing infrastructure. You'll need to know about IP addressing, DNS, firewalls, and common protocols (HTTP, HTTPS, FTP). This knowledge will help you configure and secure networks, which is a critical aspect of DevSecOps practices. Computer Networks: Crash Course Computer Science Introduction to Network Security Security Fundamentals\tA solid grasp of fundamental security concepts is essential. This includes knowledge of encryption, authentication, access control, and common security threats like malware and phishing. Understanding these concepts will enable you to identify potential vulnerabilities and implement security measures to protect your infrastructure and applications. What Is Cyber Security?Cybersecurity: Crash Course Computer Science #31The Complete CompTIA Security+ SY0 701 Crash Course DevOps\tDevOps is the foundation of DevSecOps. It's necessary that you understand what this means and what it implies. The DevOps Guide - What is DevOps?Learn To Cloud - Phase 4What is DevOps? REALLY understand it - DevOps vs SRE CI/CD\tYou'll need to understand what Continous Integration (CI) and Continous Delivery (CD) is before we move forward. The DevOps Guide - CI/CD Cloud Computing\tIf we wish to pursue the Cloud DevSecOps or Cloud Security Development, then you definitely need to understand the Cloud. The knowledge that you'll help you build scalable DevSecOps solutions within the Cloud by leveraging existing services to protect your applications. Learn To Cloud ","version":"Next","tagName":"h2"},{"title":"DevSecOps Home Lab","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"DevSecOps Home Lab","url":"/ar/projects/devsecops-home-lab/#overview","content":" So you decided that you want to go down the route of building your own home lab? Well, hell... welcome! This is the first project where I'm going to show you how to setup your own home lab from scratch! This will include you going in and setting up and configuring databases, installing packages, and a lot of things that System Administrators would do.  According to The Linux Handbook, a homelab is the name given to a server (or multiple server setup) that resides locally in your home and where you host several applications and virtualized systems for testing and developing or for home and functional usage.  To be more formal (in a sense), we're going to build a comprehensive DevSecOps Home Lab that simulates a real-world environment for testing, learning, and enhancing your DevSecOps skills. This lab is designed to give you hands-on experience with a variety of tools and technologies commonly used in the DevSecOps ecosystem.  ","version":"Next","tagName":"h2"},{"title":"Don't have servers at home?â€‹","type":1,"pageTitle":"DevSecOps Home Lab","url":"/ar/projects/devsecops-home-lab/#dont-have-servers-at-home","content":" This is totally okay! There are two options that I'd recommend then:  Check out this video to learn how I went out to buy my systems for a very low price:If you have experience with building things out in the cloud, you can replicate this exact architecture, although it may be a little different when connecting to your machines and vice versa.Use tools like VirtualBox to create VM's locally. This comes with a bit of a learning curve.  ","version":"Next","tagName":"h2"},{"title":"Architecture Overview and Debriefâ€‹","type":1,"pageTitle":"DevSecOps Home Lab","url":"/ar/projects/devsecops-home-lab/#architecture-overview-and-debrief","content":" My home lab consists of two servers running Ubuntu 24.04 LTS. To avoid any compatibility issues, you want to ensure that you are using this EXACT version.    The architecture is divided across two servers:  dsb-node-01: This server is responsible for hosting essential infrastructure services, including a reverse proxy, containerization engine, and monitoring stack.dsb-hub: Dedicated to handling the DevSecOps toolchain, this server focuses on source code management, security scanning, continuous integration, and continuous delivery (CI/CD).  ","version":"Next","tagName":"h2"},{"title":"Components Breakdownâ€‹","type":1,"pageTitle":"DevSecOps Home Lab","url":"/ar/projects/devsecops-home-lab/#components-breakdown","content":" 1. Server: dsb-node-01â€‹  This server lays the foundation for containerized environments and monitoring:  NGINX: NGINX acts as a web server and reverse proxy, ensuring that incoming traffic is efficiently routed to the appropriate service.Docker: Docker provides containerization capabilities, allowing applications and services to run in isolated environments.Containerized Web Application: This could be anything (Python API, Java App, etc), as long as it is in a Docker container.Prometheus: Prometheus is responsible for collecting and monitoring system and application metrics, serving as the central component for alerting and monitoring.Grafana: Integrated with Prometheus, Grafana offers visual dashboards that make it easy to observe metrics and logs, giving you insights into system health and performance.  2. Server: dsb-hubâ€‹  This server hosts the critical components of the DevSecOps toolchain, enabling secure and automated workflows:  NGINX: Similar to dsb-node-01, NGINX handles traffic management and routing for the services hosted on this server.Gitea: A lightweight, self-hosted Git service, Gitea provides version control capabilities, allowing you to manage your source code repositories.SonarQube: SonarQube is utilized for continuous code quality and security checks, detecting issues such as bugs, code smells, and vulnerabilities in the codebase.Jenkins: As a cornerstone of CI/CD, Jenkins automates the process of building, testing, and deploying applications, ensuring a streamlined development pipeline.Trivy: Trivy performs vulnerability scanning for Docker images, ensuring that containerized applications are free from known security risks.Nexus: Nexus is used for managing dependencies, artifacts, and binaries. It serves as a repository manager that integrates with Jenkins, allowing you to maintain control over project artifacts and their versions.Docker: Docker on this server continues to play a key role in containerizing applications and services, maintaining the consistency of deployment across the environment.  ","version":"Next","tagName":"h3"},{"title":"What You'll Learnâ€‹","type":1,"pageTitle":"DevSecOps Home Lab","url":"/ar/projects/devsecops-home-lab/#what-youll-learn","content":" By completing this project fully, you will gain hands-on experience (practical) on setting up a DevSecOps environment, such as:  Containerization: Deploying and managing applications in Docker containers.Monitoring and Logging: Using Prometheus to monitor application health and performance.Security Scanning: Implementing security tools like Trivy and SonarQube to identify vulnerabilities and maintain code quality.Web Traffic Management: Configuring Nginx as a reverse proxy to efficiently route and secure web traffic. ","version":"Next","tagName":"h2"},{"title":"Nginx Reverse Proxy","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/config-app-proxy","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Nginx Reverse Proxy","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/config-app-proxy#overview","content":" This installation will happen on the dsb-node-01.  In this guide, we will set up a reverse proxy for your application using Nginx. This will allow you to manage incoming traffic more effectively and forward requests to your application running on a different port.  ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"Nginx Reverse Proxy","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/config-app-proxy#installation-steps","content":" Install Nginx  To begin, you need to install Nginx on your server. This can be done using the following command:  sudo apt install nginx   This command will install Nginx along with any required dependencies.  ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Nginx Reverse Proxy","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/config-app-proxy#configuration-steps","content":" After installing Nginx, you need to configure it to act as a reverse proxy for your application.  Remove Default Configuration: First, unlink the default Nginx configuration file to avoid any conflicts: sudo unlink /etc/nginx/sites-enabled/default Create a New Configuration File: Next, create a new configuration file specifically for your reverse proxy setup: sudo nano /etc/nginx/sites-available/reverse-proxy In the file that opens, paste the following configuration: server { listen 80; server_name localhost; location / { client_max_body_size 1000M; proxy_pass http://127.0.0.1:8080; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } This configuration tells Nginx to listen on port 80 (the default HTTP port) and forward all incoming traffic to your application running on http://127.0.0.1:8080. It also sets various headers that can be useful for your application's logging and security purposes. Enable the New Configuration: Activate the new reverse proxy configuration by creating a symbolic link to the sites-enabled directory: sudo ln -s /etc/nginx/sites-available/reverse-proxy /etc/nginx/sites-enabled/ Restart Nginx: Finally, restart Nginx to apply the new configuration: sudo systemctl restart nginx   ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"Nginx Reverse Proxy","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/config-app-proxy#youre-done","content":" You've setup and configured the Nginx reverse proxy! ","version":"Next","tagName":"h2"},{"title":"Creating & Configuring Jenkins Pipeline","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-jenkins-piepline","content":"","keywords":"","version":"Next"},{"title":"Step 1: Configure Access Token in Giteaâ€‹","type":1,"pageTitle":"Creating & Configuring Jenkins Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-jenkins-piepline#step-1-configure-access-token-in-gitea","content":" In Gitea, click on your user avatar at the top right, then select Settings &gt; Applications. Enter a token name (e.g., Jenkins), set it to Private, and select all permissions as READ/WRITE. Click Generate Token. Copy the token and store it in a secure place. This token will be used for Jenkins authentication.  ","version":"Next","tagName":"h2"},{"title":"Step 2: Install Required Plugins in Jenkinsâ€‹","type":1,"pageTitle":"Creating & Configuring Jenkins Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-jenkins-piepline#step-2-install-required-plugins-in-jenkins","content":" Log into Jenkins. From the Dashboard, click Manage Jenkins &gt; Manage Plugins. Under Available Plugins, search for and install the following: GiteaSonarQube ScannerPrometheus Metrics  ","version":"Next","tagName":"h2"},{"title":"Step 3: Create Jenkins Pipelineâ€‹","type":1,"pageTitle":"Creating & Configuring Jenkins Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-jenkins-piepline#step-3-create-jenkins-pipeline","content":" From the Jenkins Dashboard, click New Item. Select Organization Folder and name it (e.g., OWASP Juice Shop Pipeline). Scroll down to Repository Sources and select Gitea Organization. Under Credentials, click Add. Select the organization folder name, set the Kind to Gitea Personal Access Token (PAT), and paste the token generated from Gitea. Set the Owner to your Gitea username, and then click Apply and Save.  ","version":"Next","tagName":"h2"},{"title":"Step 4: Configure Webhook in Giteaâ€‹","type":1,"pageTitle":"Creating & Configuring Jenkins Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-jenkins-piepline#step-4-configure-webhook-in-gitea","content":" In Jenkins, go to Manage Jenkins &gt; Configure System. Scroll down to Gitea Servers and add your Gitea server details. Discover branches: Only branches filed as PRs or master/main branch.Discover pull requests from origin: Both the current pull request revision and the pull request merged with the current target branch revision. Click Apply and Save. In Gitea, navigate to the project, click on Settings &gt; Webhooks. Click Add Webhook, then select Gitea. Fill out the form: URL: &lt;http://localhost:8080/gitea-webhook/post&gt;Method: POSTContent Type: application/jsonBranch filter: * Click Add Webhook to save the configuration.  ","version":"Next","tagName":"h2"},{"title":"Step 5: Configure SSH Keysâ€‹","type":1,"pageTitle":"Creating & Configuring Jenkins Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-jenkins-piepline#step-5-configure-ssh-keys","content":" On your local machine (DSB Hub), generate an SSH key pair: ssh-keygen -t rsa -b 4096 -C &quot;jenkins@dsb-hub.com&quot; Generating public/private rsa key pair. Enter file in which to save the key (/home/&lt;your_username&gt;/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/&lt;your_username&gt;/.ssh/id_rsa Your public key has been saved in /home/&lt;your_username&gt;/.ssh/id_rsa.pub The key fingerprint is: SHA256:iIitv6/AYHsTTND7ooLJtG0M2NZrOWMer7E0E6hZ8XI jenkins@dsb-hub.com The key's randomart image is: Copy the public key to your remote server: ssh-copy-id &lt;your_username&gt;@dsb-node-01.local In Jenkins, go to Manage Jenkins &gt; Credentials &gt; System &gt; Global Credentials (unrestricted). Select Add Credentials and choose SSH Username with private key. Fill out the form and hit Create to store your SSH credentials.  ","version":"Next","tagName":"h2"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Creating & Configuring Jenkins Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-jenkins-piepline#conclusion","content":" You've now successfully set up a Jenkins pipeline for the OWASP Juice Shop project with Gitea, SonarQube, and Docker integration. Your pipeline is configured to handle code quality checks, security scans, and deployments, ensuring that your application maintains a high standard throughout the development lifecycle. ","version":"Next","tagName":"h2"},{"title":"Nexus","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-nexus","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Nexus","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-nexus#overview","content":" This installation happens on the dsb-hub.  Nexus Repository Manager is a tool for managing components and build artifacts across various formats like Docker, Maven, and npm. We are using Docker Compose to install Nexus to avoid conflicts with SonarQube's JDK requirements. Nexus will be used to manage Docker images, allowing us to proxy images from Docker Hub, cache them locally, and securely manage retrieval.  ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"Nexus","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-nexus#installation-steps","content":" Create a new directory for Nexus: mkdir -p apps/nexus/nexus-data sudo chown -R 200 apps/nexus/nexus-data Create the Docker Compose YAML file: cd apps/nexus touch docker-compose.yml Add the following content to the docker-compose.yml: version: &quot;3&quot; services: nexus: image: sonatype/nexus3 restart: always volumes: - &quot;./nexus-data:/nexus-data&quot; ports: - &quot;8081:8081&quot; - &quot;8082:8082&quot; - &quot;8085:8085&quot; volumes: nexus-data: driver: local Run the application: docker compose up -d Confirm that the application is up and running by visiting: http://your-ip-address:8081     ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Nexus","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-nexus#configuration-steps","content":" Click the &quot;Sign In&quot; button and locate the admin password: cat nexus-data/admin.password Use the password to log in and complete the initial setup: Change your password.Enable anonymous access if desired. As admin, navigate to the UI and create a new repository: For the new repository, choose &quot;Docker proxy&quot; and input the following information: Name: docker-proxyRemote Storage Proxy URL: https://registry.hub.docker.comDocker Index: Docker HubEnable anonymous pulls.Set HTTP to 8082. Create a local user with the username nx-anonymous and complete the setup:  ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"Nexus","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-nexus#youre-done","content":" You've successfully set up your Nexus server. ","version":"Next","tagName":"h2"},{"title":"Installing a Grafana Dashboard for Nexus","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/testing-results/install-nexus-grafana-dashboard","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Installing a Grafana Dashboard for Nexus","url":"/ar/projects/devsecops-home-lab/testing-results/install-nexus-grafana-dashboard#overview","content":" This installation process is for creating and installing a Nexus dashboard within Grafana for monitoring purposes.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Installing a Grafana Dashboard for Nexus","url":"/ar/projects/devsecops-home-lab/testing-results/install-nexus-grafana-dashboard#prerequisites","content":" Set Up Nexus Repository Manager Monitoring Ensure Nexus Repository Manager is configured to expose metrics for Prometheus. Edit the nexus.properties file, typically located in the etc/ directory within your Nexus installation: sudo nano ./apps/nexus/nexus-data/etc/nexus.properties Add the following property to enable Prometheus metrics: nexus.prometheus.enabled=true Restart Nexus for the changes to take effect: docker compose down &amp;&amp; docker compose up -d Set Up Prometheus Add Nexus as a target in the Prometheus configuration (prometheus.yml) by specifying its host and port: scrape_configs: - job_name: &quot;nexus&quot; static_configs: - targets: [&quot;&lt;nexus_host&gt;:&lt;nexus_port&gt;&quot;] Replace &lt;nexus_host&gt; with your Nexus server's hostname or IP address and &lt;nexus_port&gt; with the port where Nexus is running (default is 8081). Start or restart Prometheus to begin collecting Nexus metrics.  ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"Installing a Grafana Dashboard for Nexus","url":"/ar/projects/devsecops-home-lab/testing-results/install-nexus-grafana-dashboard#installation-steps","content":" Import the Nexus Dashboard in Grafana  To visualize Nexus traffic in Grafana, follow these steps: Log in to your Grafana instance.Click on Create (the plus icon on the left) and select Import.Paste the dashboard ID: 16459, or use the URL: https://grafana.com/grafana/dashboards/16459-infra-nexus/.Click Load.Select your Prometheus data source, which scrapes metrics from Nexus.Click Import to add the dashboard to your Grafana instance.  Monitor and Refine  After importing, you can monitor Nexus traffic and performance metrics through the dashboard.Customize the dashboard as needed by refining queries or adding additional panels.Set up alerts in Grafana based on performance thresholds or traffic conditions to get notified proactively.  ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"Installing a Grafana Dashboard for Nexus","url":"/ar/projects/devsecops-home-lab/testing-results/install-nexus-grafana-dashboard#youre-done","content":" With this setup, Grafana will now display Nexus traffic and performance data, allowing you to monitor the health and usage of your Nexus Repository Manager effectively. ","version":"Next","tagName":"h2"},{"title":"Triggering Jenkins Pipeline","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/testing-results/testing-pipeline","content":"","keywords":"","version":"Next"},{"title":"Triggering the OWASP Juice Shop Pipeline in Jenkinsâ€‹","type":1,"pageTitle":"Triggering Jenkins Pipeline","url":"/ar/projects/devsecops-home-lab/testing-results/testing-pipeline#triggering-the-owasp-juice-shop-pipeline-in-jenkins","content":" Log into Jenkins Start by logging into your Jenkins server using your credentials. Navigate to the OWASP Juice Shop Pipeline From the Jenkins Dashboard, locate and select the OWASP Juice Shop pipeline from your list of projects. Select the Master Branch Inside the pipeline project, click on the master branch to view its details and available actions. Trigger the Pipeline Build Click the Build Now button to manually trigger the pipeline build for the master branch. Monitor the Pipeline Execution As the pipeline runs, you can monitor the execution of each stage in real-time. Jenkins will display logs and outputs for each step, allowing you to track the progress. Verify the Deployment Once the pipeline has successfully completed, verify the deployment by opening a web browser and navigating to: http://&lt;your_ip_address&gt;:8084/ The OWASP Juice Shop application should be accessible at this URL, confirming that the deployment was successful. Automate Pipeline Triggers Note that your Gitea repository should be configured to automatically trigger this Jenkins pipeline whenever a commit is made to the master branch. This ensures continuous integration and deployment for your project.  ","version":"Next","tagName":"h2"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Triggering Jenkins Pipeline","url":"/ar/projects/devsecops-home-lab/testing-results/testing-pipeline#conclusion","content":" Congratulations! Your Jenkins pipeline is now set up and working. Youâ€™ve successfully triggered, monitored, and verified a build and deployment for the OWASP Juice Shop application. 1 ","version":"Next","tagName":"h2"},{"title":"Jenkins","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-jenkins","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Jenkins","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-jenkins#overview","content":" This installation happens on the dsb-hub.  According to Jenkins User Documentation, Jenkins is a self-contained, open source automation server which can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software. It's a CI/CD platform.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Jenkins","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-jenkins#prerequisites","content":" Install Java  Before installing Jenkins, ensure that Java is installed on your system:  Update your package manager and install Java: sudo apt update sudo apt install fontconfig openjdk-17-jre   ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"Jenkins","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-jenkins#installation-steps","content":" Configure the Package Manager and Install Jenkins  Add Jenkins to your package manager by downloading and installing the Jenkins key: sudo wget -O /usr/share/keyrings/jenkins-keyring.asc https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key Add the Jenkins repository to your sources list: echo &quot;deb [signed-by=/usr/share/keyrings/jenkins-keyring.asc] https://pkg.jenkins.io/debian-stable binary/&quot; | sudo tee /etc/apt/sources.list.d/jenkins.list &gt; /dev/null Update your package list and install Jenkins: sudo apt-get update sudo apt-get install jenkins   Enable and Start Jenkins  Enable Jenkins to start on boot: sudo systemctl enable jenkins Start Jenkins: sudo systemctl start jenkins   Verify Jenkins is Running  Check the status of Jenkins to ensure it's active: sudo systemctl status jenkins You should see output similar to the following if Jenkins is up and running: â— jenkins.service - Jenkins Continuous Integration Server Loaded: loaded (/usr/lib/systemd/system/jenkins.service; enabled; preset: enabled) Active: active (running) since [DATE]; [TIME] ago Main PID: 9188 (java)   ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Jenkins","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-jenkins#configuration-steps","content":" Access Jenkins via Web Browser  Open your web browser and go to: http://your_ip:8080 You should see the Jenkins setup screen.  Retrieve the Initial Admin Password  To proceed with the setup, you will need the initial admin password. Retrieve it by running the following command on your server: sudo cat /var/lib/jenkins/secrets/initialAdminPassword Copy the password and enter it in the password box on the web interface.  Install Suggested Plugins  After entering the admin password, click Install suggested plugins and allow Jenkins to install the necessary plugins.  Set Up Your Admin Account  After the plugins are installed, youâ€™ll be prompted to set up your admin account. Enter your IP details and set up your Jenkins instance.  Configure Instance IP  After you enter your admin information, set the URL IP address of your instance or press Not Now.  Navigate to the home page  You should be here now.  ","version":"Next","tagName":"h2"},{"title":"Installing Jenkins Nodeâ€‹","type":1,"pageTitle":"Jenkins","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-jenkins#installing-jenkins-node","content":" This section covers installing and configuring a Jenkins node on dsb-node-01.  Create Jenkins User  Create a Jenkins user and set appropriate permissions: sudo adduser jenkins sudo chown -R jenkins:jenkins /home/jenkins sudo chmod 755 /home/jenkins sudo usermod -aG docker jenkins   Install Java  Install Java on the Jenkins node: sudo apt install openjdk-17-jre-headless   Configure the Jenkins Node in Jenkins UI  In Jenkins UI, navigate to Dashboard &gt; Manage Jenkins &gt; Nodes. Click on New Node. Select Permanent Agent and click OK. For your agent, configure the Remote root directory to /home/jenkins/agent and save the configuration.  Configure Jenkins Security for Agents  In Jenkins UI, navigate to Dashboard &gt; Manage Jenkins &gt; Security, and scroll down until you see TCP ports for inbound agents. Select Random. Apply/Save.  Set Up the Jenkins Node  On dsb-node-01, create the agent directory: cd /home/jenkins &amp;&amp; mkdir agent Download the Jenkins agent JAR file: wget http://&lt;jenkins-server-url&gt;/jnlpJars/agent.jar Run the Jenkins agent: java -jar agent.jar -jnlpUrl http://&lt;jenkins-server-url&gt;/computer/&lt;node-name&gt;/slave-agent.jnlp -secret &lt;secret-key&gt; -workDir &quot;/home/jenkins/agent&quot;   Ù…Ù„Ø§Ø­Ø¸Ø© You can find the secret key on the Jenkins node configuration page.    Create a Systemd Service for the Jenkins Agent  Create a new systemd service for the Jenkins agent: sudo nano /etc/systemd/system/jenkins-agent.service Add the following configuration to the service file: [Unit] Description=Jenkins Agent [Service] User=jenkins Group=jenkins ExecStart=/usr/bin/java -jar /home/jenkins/agent/agent.jar -jnlpUrl http://&lt;jenkins-server-url&gt;/computer/&lt;node-name&gt;/slave-agent.jnlp -secret &lt;secret-key&gt; -workDir /home/jenkins/agent Restart=always [Install] WantedBy=multi-user.target Reload systemd and start the Jenkins agent service: sudo systemctl daemon-reload sudo systemctl start jenkins-agent sudo systemctl enable jenkins-agent   ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"Jenkins","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-jenkins#youre-done","content":" You're finished setting up Jenkins on your machine! ","version":"Next","tagName":"h2"},{"title":"Docker","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-docker","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Docker","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-docker#overview","content":" This installation happens on both of the machines.  According to Docker's Website, Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker's methodologies for shipping, testing, and deploying code, you can significantly reduce the delay between writing code and running it in production.  ","version":"Next","tagName":"h2"},{"title":"Docker Installation Stepsâ€‹","type":1,"pageTitle":"Docker","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-docker#docker-installation-steps","content":" Update the Package Index First, update your existing list of packages: sudo apt-get update Install Required Packages Install the necessary packages to allow apt to use a repository over HTTPS: sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Dockerâ€™s Official GPG Key Add Dockerâ€™s official GPG key to your system: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg Set Up the Stable Repository Use the following command to set up the stable Docker repository: echo \\ &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null Install Docker Engine Update the package index again and install Docker Engine, along with containerd and docker-compose: sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-compose Verify Docker Installation After the installation, verify that Docker is installed and running correctly: sudo docker --version This command should return the Docker version installed. Start and Enable Docker Service Ensure Docker starts on boot: sudo systemctl start docker sudo systemctl enable docker Manage Docker as a Non-Root User (Optional) By default, Docker commands need to be run with sudo. If you want to run Docker commands as a non-root user, you need to add your user to the docker group: sudo usermod -aG docker $USER sudo usermod -aG docker jenkins After running this command, log out and back in, or run the following command to apply the group membership: newgrp docker Test Docker Installation Test the Docker installation by running a simple Docker container: docker run hello-world This command will download a test image, run it in a container, and print a confirmation message. (Optional) Install Additional Docker Tools  You may also want to install Docker Compose if it's not already included:  sudo apt-get install docker-compose-plugin   ","version":"Next","tagName":"h2"},{"title":"Docker Registry Installation Stepsâ€‹","type":1,"pageTitle":"Docker","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-docker#docker-registry-installation-steps","content":" Setting up your own Docker registry locally allows you to host your Docker images privately without relying on a third-party service like Docker Hub. Below are the steps to set up and use a local Docker registry on your machine or server.  Create Necessary Directories and Files Begin by navigating to your home directory and creating directories to store your Docker registry files: cd ~ mkdir -p apps/docker cd apps/docker touch docker-compose.yml Set Up docker-compose.yml Next, edit the docker-compose.yml file with the following content: version: &quot;3.8&quot; services: registry: image: registry:2.8.2 ports: - &quot;5000:5000&quot; environment: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Origin: &quot;[http://registry.example.com]&quot; REGISTRY_HTTP_HEADERS_Access-Control-Allow-Methods: &quot;[HEAD,GET,OPTIONS,DELETE]&quot; REGISTRY_HTTP_HEADERS_Access-Control-Allow-Credentials: &quot;[true]&quot; REGISTRY_HTTP_HEADERS_Access-Control-Allow-Headers: &quot;[Authorization,Accept,Cache-Control]&quot; REGISTRY_HTTP_HEADERS_Access-Control-Expose-Headers: &quot;[Docker-Content-Digest]&quot; REGISTRY_STORAGE_DELETE_ENABLED: &quot;true&quot; REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /var/lib/registry volumes: - ./registry-data:/var/lib/registry ui: image: joxit/docker-registry-ui:latest ports: - &quot;8082:80&quot; environment: - SINGLE_REGISTRY=true - REGISTRY_TITLE=Docker Registry UI - DELETE_IMAGES=true - SHOW_CONTENT_DIGEST=true - NGINX_PROXY_PASS_URL=http://&lt;your_dsb_hub_ip_address&gt;:5000 - SHOW_CATALOG_NB_TAGS=true - CATALOG_MIN_BRANCHES=1 - CATALOG_MAX_BRANCHES=1 - TAGLIST_PAGE_SIZE=100 - REGISTRY_SECURED=false - CATALOG_ELEMENTS_LIMIT=1000 depends_on: - registry volumes: registry-data: Deploy the Docker Registry Use Docker Compose to deploy the Docker registry and the UI: docker compose up -d Configure Docker Daemon To allow Docker to interact with your insecure registry, you need to update the Docker daemon configuration: sudo nano /etc/docker/daemon.json Add the following content: { &quot;insecure-registries&quot;: [&quot;&lt;your_dsb_hub_ip_address&gt;:5000&quot;] } Restart Docker Service After updating the Docker daemon configuration, restart Docker to apply the changes: sudo systemctl restart docker Configure Additional Docker Nodes (if applicable) If you're working with additional Docker nodes, such as dsb-hub-01, you'll need to apply similar settings: sudo nano /var/snap/docker/current/config/daemon.json Add the following content: { &quot;log-level&quot;: &quot;error&quot;, &quot;insecure-registries&quot;: [&quot;&lt;your_dsb_hub_ip_address&gt;:5000&quot;] } Then, restart the Docker service: sudo snap restart docker   ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"Docker","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-docker#youre-done","content":" You've completed configuring and installing Docker and the Docker Registry on your servers. ","version":"Next","tagName":"h2"},{"title":"cAdvisor","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-cadvisor","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"cAdvisor","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-cadvisor#overview","content":" This installation happens on the dsb-node-01.  According to cAdvisor's GitHub Repository, cAdvisor (Container Advisor) is an open-source tool from Google designed to provide insights into resource usage and performance characteristics of running containers. It collects, aggregates, processes, and exports information about running containers, making it a valuable tool for monitoring containerized environments. This guide will walk you through the steps to install and configure cAdvisor using Docker Compose on your system.  ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"cAdvisor","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-cadvisor#installation-steps","content":" Create Necessary Directories First, navigate to your apps directory and create a new directory for cAdvisor: cd apps mkdir cadvisor Next, create the Docker library directory if it does not already exist: cd /var/lib mkdir docker chown -R root:root docker Create Docker Compose File Navigate back to the cadvisor directory and create a docker-compose.yml file: cd ~/apps/cadvisor touch docker-compose.yml Configure Docker Compose File Edit the docker-compose.yml file and add the following configuration: version: &quot;3.8&quot; services: cadvisor: image: gcr.io/cadvisor/cadvisor container_name: cadvisor privileged: true restart: unless-stopped ports: - &quot;8080:8080&quot; volumes: - /:/rootfs:ro - /var/run:/var/run:ro - /sys:/sys:ro network_mode: host Deploy cAdvisor Finally, use Docker Compose to deploy cAdvisor: docker-compose up -d   ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"cAdvisor","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-cadvisor#youre-done","content":" Congratulations! You have successfully installed and configured cAdvisor. The cAdvisor service is now running and can be accessed via port 8080 on your host machine. This setup will allow you to monitor and visualize the performance metrics of your running containers in real-time. ","version":"Next","tagName":"h2"},{"title":"Grafana","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-grafana","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Grafana","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-grafana#overview","content":" This installation happens on the dsb-node-01.  According to Grafana's Website, Grafana is an open-source platform for monitoring and observability. It allows you to query, visualize, alert on, and understand your metrics no matter where they are stored. Grafana provides a powerful and elegant way to create, explore, and share dashboards that integrate data from various sources, making it an essential tool for DevOps teams to monitor their infrastructure, applications, and services in real time.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Grafana","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-grafana#prerequisites","content":" Install Docker Install Docker using Snap: sudo snap install docker Create Docker Group and Add User to It Create a Docker group and add your user to it: sudo groupadd docker sudo usermod -aG docker $USER sudo reboot   ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"Grafana","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-grafana#installation-steps","content":" Create a Docker-Compose File for Grafana Create the necessary directories and Docker-Compose file: mkdir -p ~/apps/grafana touch ~/apps/grafana/docker-compose.yml Copy and Save the Docker-Compose Configuration Use the following configuration in your docker-compose.yml file: version: &quot;3.7&quot; services: grafana: image: grafana/grafana:latest container_name: grafana ports: - &quot;3000:3000&quot; environment: - GF_SECURITY_ADMIN_PASSWORD=your_admin_password - GF_SECURITY_ADMIN_USER=your_admin_user volumes: - grafana_data:/var/lib/grafana restart: always network_mode: host volumes: grafana_data: Open the file with nano or your preferred text editor and paste the configuration: nano ~/apps/grafana/docker-compose.yml Save and close the file. Run Grafana with Docker-Compose Navigate to the Grafana directory and run the container: cd ~/apps/grafana docker-compose up -d Confirm Grafana is Running Check if Grafana is up and running by visiting http://localhost:3000 in your web browser.  ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Grafana","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-grafana#configuration-steps","content":" Log into the Dashboard Use your admin username and password to log into the Grafana dashboard. Update Admin Username Click on your profile and update the admin username. Update Admin Password Change your password for enhanced security.  ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"Grafana","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-grafana#youre-done","content":" Youâ€™ve successfully installed and configured Grafana on your system. ","version":"Next","tagName":"h2"},{"title":"DevSecOps Pipeline - AWS","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/","content":"","keywords":"","version":"Next"},{"title":"Know Before You Goâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - AWS","url":"/ar/projects/devsecops-pipeline-aws/#know-before-you-go","content":" This project is a little expense, and you will rack up a nice bill in AWS if you leave all your resources created. Therefore, I recommend that you TEAR IT ALL DOWN when you're done. Do not leave your EKS cluster running, you'll be charged for it.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitiesâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - AWS","url":"/ar/projects/devsecops-pipeline-aws/#prerequisities","content":" Before you begin this, you will want to have some knowledge of AWS services and how they work, as well as prior knowledge of Terraform.You will also want to ensure that you have an AWS account created. You can go through the account creation process here: AWS Account Creation ProcessMake sure you have the following installed on your local machine: PythonGitDockerTerraform CLI  ","version":"Next","tagName":"h2"},{"title":"Overviewâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - AWS","url":"/ar/projects/devsecops-pipeline-aws/#overview","content":" So you've decided to go down the path of building your own Cloud Native DevSecOps pipeline within AWS? Well, hell... welcome! This is the one of my favorite projects where I'm going to show you how to setup your own AWS pipeline using Terraform Cloud. Unlike the DevSecOps Home Lab, we're just focused on developing the pipeline and deploying an application into Elastic Kubernetes Service (EKS).  Luckily for you all, you won't need to do anything. I've taken the liberty of developing all of the code for you. These are the two GitHub repositories that you need to look at before we get started:  DevSecOps Pipeline: https://github.com/devsecblueprint/aws-devsecops-pipelineFastAPI Application: https://github.com/devsecblueprint/awsome-fastapi  ","version":"Next","tagName":"h2"},{"title":"Architecture Diagramâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - AWS","url":"/ar/projects/devsecops-pipeline-aws/#architecture-diagram","content":"   ","version":"Next","tagName":"h2"},{"title":"Architecture Breakdownâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - AWS","url":"/ar/projects/devsecops-pipeline-aws/#architecture-breakdown","content":" At a VERY high level, the architecture represents an automated CI/CD pipeline leveraging several AWS services to deploy containerized applications:  AWS CodePipeline: Manages the end-to-end flow of code changes, automating build, test, and deployment stages.AWS CodeBuild: Builds and tests the application code, generating deployable artifacts, and executing Security Scans with Snyk and Trivy.Amazon S3: Stores artifacts like build outputs and deployment files.AWS Systems Manager (SSM) Parameter Store: Securely manages configuration data and secrets used for Snyk.Amazon EKS: Serves as the deployment environment for containerized workloads, providing scalability and orchestration.  Flow Overview:  CodePipeline orchestrates the process.CodeBuild validates, scans, and compiles the code while also interacting with SSM.Artifacts are stored in S3.Applications are deployed to the EKS cluster.  This architecture ensures automation, security, and scalability for modern DevSecOps workflows.  ","version":"Next","tagName":"h3"},{"title":"What Youâ€™ll Learnâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - AWS","url":"/ar/projects/devsecops-pipeline-aws/#what-youll-learn","content":" By working through this guide, youâ€™ll gain hands-on experience building and deploying a secure, cloud-native DevSecOps pipeline on AWS. Specifically, you will learn how to:  Architect and Implement a Secure CI/CD Pipeline: Understand how AWS CodePipeline, CodeBuild, S3, SSM Parameter Store, and EKS work together in an automated, end-to-end workflow. Integrate Security Scanning into the Pipeline: Use tools like Snyk and Trivy to scan application code and container images for vulnerabilities before production deployment. Leverage Terraform for Infrastructure as Code (IaC): Employ Terraform to provision and manage AWS resources consistently and at scale. Deploy a Containerized Application to Amazon EKS: Gain confidence in running containerized workloads on Kubernetes with EKS, ensuring scalability and simplified orchestration. Securely Manage Configuration and Secrets: Store and retrieve sensitive data using AWS Systems Manager Parameter Store, following security best practices throughout your pipeline.  With all that being stated, Please follow the order of the documents, otherwise you'll most likely run into errors and get lost. ","version":"Next","tagName":"h2"},{"title":"Node Exporter","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-node-exporter","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Node Exporter","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-node-exporter#overview","content":" This installation happens on the dsb-node-01.  According to Prometheus' Website, Node Exporter is an essential tool for monitoring the hardware and OS-level metrics of your Linux systems. It is part of the Prometheus ecosystem and is widely used to gather metrics such as CPU usage, memory usage, disk I/O, and more.  ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"Node Exporter","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-node-exporter#installation-steps","content":" Create Necessary Directories  First, navigate to your apps directory and create a new directory for Node Exporter:  cd ~/apps mkdir node-exporter   Create Docker Compose File  Navigate to the node-exporter directory and create a docker-compose.yml file:  cd ~/apps/node-exporter touch docker-compose.yml   Configure Docker Compose File  Edit the docker-compose.yml file and add the following configuration:  version: '3.8' services: node-exporter: image: quay.io/prometheus/node-exporter:latest container_name: node-exporter restart: unless-stopped network_mode: &quot;host&quot; pid: &quot;host&quot; volumes: - /proc:/host/proc:ro - /sys:/host/sys:ro - /:/rootfs:ro command: - '--path.procfs=/host/proc' - '--path.sysfs=/host/sys' - '--collector.filesystem.ignored-mount-points=&quot;^/(sys|proc|dev|host|etc)($$|/)&quot;' network_mode: host   Deploy Node Exporter  Finally, use Docker Compose to deploy Node Exporter:  docker-compose up -d   ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"Node Exporter","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-node-exporter#youre-done","content":"","version":"Next","tagName":"h2"},{"title":"Setting Up Repository And Pipeline","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up Repository And Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline#overview","content":" In this section of the guide, you will learn how to set up a Jenkins pipeline for the OWASP Juice Shop project, integrating it with Gitea for version control, SonarQube for static code analysis, and Docker for containerization. The process includes cloning the codebase, configuring access tokens in Gitea, installing necessary Jenkins plugins, creating a Jenkins pipeline, and setting up webhooks and SSH keys for secure communication between your systems. By the end of this guide, your pipeline will be fully automated to handle code quality checks, security scans, and deployments.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Setting Up Repository And Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline#prerequisites","content":" Before starting, ensure that you have the following:  Access to a Gitea instanceJenkins set up with required plugins (e.g., Git, SonarQube, Docker, etc.)A SonarQube instanceDocker installed on your machine  ","version":"Next","tagName":"h2"},{"title":"Step 1: Clone the Codebaseâ€‹","type":1,"pageTitle":"Setting Up Repository And Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline#step-1-clone-the-codebase","content":" On your local machine, clone the juice-shop-sonarscanning repository:  git clone https://github.com/devsecblueprint/juice-shop-sonarscanning.git   ","version":"Next","tagName":"h2"},{"title":"Step 2: Create a New Project in Giteaâ€‹","type":1,"pageTitle":"Setting Up Repository And Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline#step-2-create-a-new-project-in-gitea","content":" Log into your Gitea instance and navigate to create a new repository. Fill out the necessary information: Repository name: owasp-juice-shopVisibility: publicDescription: (Optional)Default branch: master Click on the Create Repository button. Confirm that the repository has been created successfully.  ","version":"Next","tagName":"h2"},{"title":"Step 3: Point the Local Repository to Giteaâ€‹","type":1,"pageTitle":"Setting Up Repository And Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline#step-3-point-the-local-repository-to-gitea","content":" In your local juice-shop-sonarscanning directory, update the git origin to point to your new Gitea repository:  cd juice-shop-sonarscanning/ git remote remove origin git remote add origin http://&lt;your_gitea_server_ip&gt;/&lt;your_username&gt;/owasp-juice-shop.git git push -u origin master   ","version":"Next","tagName":"h2"},{"title":"Step 4: Authorize Your Applicationâ€‹","type":1,"pageTitle":"Setting Up Repository And Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline#step-4-authorize-your-application","content":" After pushing your code, Gitea might prompt you to authorize your application.    ","version":"Next","tagName":"h2"},{"title":"Step 5: Confirm the Changesâ€‹","type":1,"pageTitle":"Setting Up Repository And Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline#step-5-confirm-the-changes","content":" Ensure that your changes have been pushed to the Gitea repository.    ","version":"Next","tagName":"h2"},{"title":"Step 6: Remove GitHub Workflows Directoryâ€‹","type":1,"pageTitle":"Setting Up Repository And Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline#step-6-remove-github-workflows-directory","content":" Remove the GitHub workflows directory from the repository:  rm -rf .github/workflows git add . git commit -m &quot;removing workflows&quot; git push   ","version":"Next","tagName":"h2"},{"title":"Step 7: Create Jenkinsfile for CI/CD Pipelineâ€‹","type":1,"pageTitle":"Setting Up Repository And Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline#step-7-create-jenkinsfile-for-cicd-pipeline","content":" Create a Jenkinsfile in the root directory of your repository with the following content:  pipeline { agent any environment { SONAR_TOKEN = credentials('sonar-analysis') SONAR_PROJECT_KEY = 'owasp-juice-shop' DOCKER_IMAGE_NAME = 'owasp-juice-shop' NEXUS_DOCKER_REGISTRY = '&lt;your_dsb_hub_ip_address&gt;:8082' NEXUS_DOCKER_PUSH_INDEX = '&lt;your_dsb_hub_ip_address&gt;:8083' NEXUS_DOCKER_PUSH_PATH = 'repository/docker-host' } stages { stage('Clone') { steps { checkout scmGit(branches: [[name: '*/master']], extensions: [], userRemoteConfigs: [[credentialsId: 'Gitea PAT', url: 'http://&lt;your_dsb_hub_ip_address&gt;/&lt;your_username&gt;/owasp-juice-shop.git']]) } } stage('Build') { steps { sh 'docker build -t ${DOCKER_IMAGE_NAME}:${BUILD_NUMBER} .' } } stage('Security Scan'){ parallel { stage('Sonar Scan') { steps { script { try{ withSonarQubeEnv(installationName: 'Sonar Server', credentialsId: 'sonar-analysis') { sh ''' docker run --rm \\ -e SONAR_HOST_URL=&quot;${SONAR_HOST_URL}&quot; \\ -e SONAR_TOKEN=&quot;${SONAR_TOKEN}&quot; \\ -v &quot;$(pwd):/usr/src&quot; \\ sonarsource/sonar-scanner-cli \\ -Dsonar.projectKey=&quot;${SONAR_PROJECT_KEY}&quot; \\ -Dsonar.qualitygate.wait=true \\ -Dsonar.sources=. ''' } } catch (Exception e) { echo &quot;Quality Gate check has failed: ${e}&quot; currentBuild.result = 'UNSTABLE' } } } } stage('Security Scan') { steps { sh ''' # Trivy scan for HIGH and CRITICAL vulnerabilities # To fail the build on any vulnerability, add: --exit-code 1 # To generate an HTML report, add: --format html --output trivy-report.html trivy image --severity HIGH,CRITICAL ${DOCKER_IMAGE_NAME}:${BUILD_NUMBER} --format html --output trivy-report.html ''' } } } } stage('Publish Trivy Report') { steps { publishHTML([ allowMissing: false, alwaysLinkToLastBuild: true, keepAll: true, reportDir: '.', reportFiles: 'trivy-report.html', reportName: 'Trivy Vulnerability Report' ]) } } stage('Publish') { steps { script { withCredentials([usernamePassword(credentialsId: 'nexus', passwordVariable: 'NEXUS_PASSWORD', usernameVariable: 'NEXUS_USERNAME')]) { sh &quot;&quot;&quot; docker login ${NEXUS_DOCKER_PUSH_INDEX} -u $NEXUS_USERNAME -p $NEXUS_PASSWORD docker tag ${DOCKER_IMAGE_NAME}:${BUILD_NUMBER} ${NEXUS_DOCKER_PUSH_INDEX}/${NEXUS_DOCKER_PUSH_PATH}/${DOCKER_IMAGE_NAME}:latest docker push ${NEXUS_DOCKER_PUSH_INDEX}/${NEXUS_DOCKER_PUSH_PATH}/${DOCKER_IMAGE_NAME}:latest &quot;&quot;&quot; } } } } stage('Deploy') { agent { label 'dsb-node-01' } steps { script { echo 'Deploying to DSB Node 01' sh ''' docker pull ${NEXUS_DOCKER_PUSH_INDEX}/${NEXUS_DOCKER_PUSH_PATH}/${DOCKER_IMAGE_NAME}:latest docker stop ${DOCKER_IMAGE_NAME} || true docker rm ${DOCKER_IMAGE_NAME} || true docker run -d --name ${DOCKER_IMAGE_NAME} -p 8084:3000 ${NEXUS_DOCKER_PUSH_INDEX}/${NEXUS_DOCKER_PUSH_PATH}/${DOCKER_IMAGE_NAME}:latest ''' } } } } post { always { cleanWs() } } }   This pipeline performs the following steps:  Clone the Repository: Pulls the owasp-juice-shop project from a Gitea repository. Build the Application: Builds a Docker image of the application and tags it with the build number. Run Security Scans: SonarQube: Analyzes code quality and enforces a quality gate.Trivy: Scans the Docker image for vulnerabilities (HIGH and CRITICAL). Publish to Nexus: Tags and pushes the built Docker image to a Nexus Docker registry. Deploy: Pulls the latest Docker image from Nexus and deploys it to a specific server, replacing any existing instance. Cleanup: Cleans up the workspace after the build.  ","version":"Next","tagName":"h2"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Setting Up Repository And Pipeline","url":"/ar/projects/devsecops-home-lab/create-and-configure-pipeline/create-repository-pipeline#conclusion","content":" By following these steps, you've successfully set up the OWASP Juice Shop project with Sonar Scanning on Gitea and Jenkins. Your CI/CD pipeline is now ready to ensure code quality and security throughout the development process. ","version":"Next","tagName":"h2"},{"title":"Trivy","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-trivy","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Trivy","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-trivy#overview","content":" This installation happens on the dsb-hub.  According to Trivy's GitHub Repository, Trivy is a comprehensive, easy-to-use open-source vulnerability scanner. It detects vulnerabilities in OS packages, container images, file systems, and Git repositories. Additionally, Trivy can identify configuration issues and hard-coded secrets, making it an essential tool for DevSecOps practices. This guide will walk you through the steps to install and configure Trivy on your system.  ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"Trivy","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-trivy#installation-steps","content":" Configure and Install Package Install required packages and add the Trivy repository key: sudo apt-get install wget apt-transport-https gnupg wget -qO - https://aquasecurity.github.io/trivy-repo/deb/public.key | gpg --dearmor | sudo tee /usr/share/keyrings/trivy.gpg &gt; /dev/null Add the Trivy repository to your sources list: echo &quot;deb [signed-by=/usr/share/keyrings/trivy.gpg] https://aquasecurity.github.io/trivy-repo/deb generic main&quot; | sudo tee -a /etc/apt/sources.list.d/trivy.list Update your package list and install Trivy: sudo apt-get update sudo apt-get install trivy Check if Trivy is Installed Successfully Verify that Trivy is installed and running correctly by running the trivy command: trivy You should see output similar to the following, which confirms that Trivy is installed and provides usage instructions: Scanner for vulnerabilities in container images, file systems, and Git repositories, as well as for configuration issues and hard-coded secrets Usage: trivy [global flags] command [flags] target trivy [command] Examples: # Scan a container image $ trivy image python:3.4-alpine # Scan a container image from a tar archive $ trivy image --input ruby-3.1.tar # Scan local filesystem $ trivy fs . # Run in server mode $ trivy server Scanning Commands: config Scan config files for misconfigurations filesystem Scan local filesystem image Scan a container image kubernetes [EXPERIMENTAL] Scan kubernetes cluster repository Scan a repository rootfs Scan rootfs sbom Scan SBOM for vulnerabilities and licenses vm [EXPERIMENTAL] Scan a virtual machine image Management Commands: module Manage modules plugin Manage plugins vex [EXPERIMENTAL] VEX utilities Utility Commands: clean Remove cached files completion Generate the autocompletion script for the specified shell convert Convert Trivy JSON report into a different format help Help about any command server Server mode version Print the version Flags: --cache-dir string cache directory (default &quot;/home/damien/.cache/trivy&quot;) -c, --config string config path (default &quot;trivy.yaml&quot;) -d, --debug debug mode -f, --format string version format (json) --generate-default-config write the default config to trivy-default.yaml -h, --help help for trivy --insecure allow insecure server connections -q, --quiet suppress progress bar and log output --timeout duration timeout (default 5m0s) -v, --version show version   ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"Trivy","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-trivy#youre-done","content":" Trivy is now installed and ready to be used for scanning vulnerabilities in container images, file systems, and more. With Trivy, you can ensure that your applications are secure and free from known vulnerabilities before deploying them to production. ","version":"Next","tagName":"h2"},{"title":"Prometheus","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-prometheus","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Prometheus","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-prometheus#overview","content":" This installation happens on the dsb-node-01.  According to Prometheus' Website, Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. It is designed for reliability and scalability, collecting metrics from configured targets at given intervals, evaluating rule expressions, displaying the results, and triggering alerts if needed. This guide will walk you through the steps to install and configure Prometheus using Docker Compose on your system.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Prometheus","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-prometheus#prerequisites","content":" Install Docker Install Docker using Snap: sudo snap install docker Create Docker Group and Add User to It Create a Docker group and add your user to it: sudo groupadd docker sudo usermod -aG docker $USER sudo reboot   ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"Prometheus","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-prometheus#installation-steps","content":" Create a Docker-Compose File for Prometheus Create the necessary directories and Docker Compose file: mkdir -p ~/apps/prometheus touch ~/apps/prometheus/docker-compose.yml Copy and Save the Docker-Compose Configuration Use the following configuration in your docker-compose.yml file: version: &quot;3.7&quot; services: prometheus: image: prom/prometheus:latest container_name: prometheus volumes: - prometheus_data:/prometheus - ./prometheus.yml:/etc/prometheus/prometheus.yml ports: - &quot;9090:9090&quot; restart: always network_mode: host volumes: prometheus_data: Open the file with nano or your preferred text editor and paste the configuration: nano ~/apps/prometheus/docker-compose.yml Save and close the file. Create a Custom prometheus.yml File Create the prometheus.yml file in the Prometheus directory: touch ~/apps/prometheus/prometheus.yml Add the following configuration to the prometheus.yml file: global: scrape_interval: 15s evaluation_interval: 15s scrape_configs: - job_name: &quot;cadvisor&quot; static_configs: - targets: [&quot;localhost:8080&quot;] - job_name: &quot;node_exporter&quot; static_configs: - targets: [&quot;localhost:9100&quot;] - job_name: &quot;jenkins&quot; metrics_path: /prometheus/ static_configs: - targets: [&quot;&lt;your_ip_address&gt;:8080&quot;] Ù…Ù„Ø§Ø­Ø¸Ø© This configuration sets up Prometheus to scrape metrics from cAdvisor, Node Exporter, and Jenkins. Replace &lt;your_ip_address&gt; with the actual IP address of your Jenkins server. Run Prometheus with Docker-Compose Navigate to the Prometheus directory and run the container: cd ~/apps/prometheus docker-compose up -d Confirm Prometheus is Running Check if Prometheus is up and running by visiting http://localhost:9090 in your web browser.  ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"Prometheus","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/monitoring-tools/install-config-prometheus#youre-done","content":" Youâ€™ve successfully installed and configured Prometheus on your system. Your Prometheus server is now up and running, ready to scrape metrics from the configured targets and provide insights into your systemâ€™s performance and health. ","version":"Next","tagName":"h2"},{"title":"SonarQube","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-sonarqube","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"SonarQube","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-sonarqube#overview","content":" This installation happens on the dsb-hub.  According to SonarQube's Website, SonarQube is an open-source platform used to continuously inspect the quality of code in various programming languages. It is designed to detect bugs, security vulnerabilities, and code smells, providing detailed reports to help developers maintain high standards in their codebases. SonarQube is widely used in DevSecOps environments to ensure that code remains secure, maintainable, and follows industry best practices.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"SonarQube","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-sonarqube#prerequisites","content":" Note: This installation uses PostgreSQL 16.6 (Ubuntu 16.6-0ubuntu0.24.04.1)  Switch to the PostgreSQL User First, switch to the postgres user to perform database-related tasks: sudo -i -u postgres Create a Database and User for SonarQube While logged in as the postgres user, create a new PostgreSQL user and database for SonarQube: # Create the sonar user and database createuser sonar createdb sonar Set Password and Grant Privileges Still as the postgres user, start the PostgreSQL session. Set a password for the sonar user and grant the necessary privileges: # Start the PostgreSQL session psql # Set password for sonar user ALTER USER sonar WITH ENCRYPTED PASSWORD 'your_password'; # Grant initial database privileges GRANT ALL PRIVILEGES ON DATABASE sonar TO sonar; # Connect to the sonar database to grant schema privileges \\c sonar # Grant all necessary schema privileges GRANT ALL ON SCHEMA public TO sonar; GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO sonar; GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO sonar; GRANT ALL PRIVILEGES ON ALL FUNCTIONS IN SCHEMA public TO sonar; GRANT USAGE ON SCHEMA public TO sonar; GRANT CREATE ON SCHEMA public TO sonar; Exit PostgreSQL and Return to the Original User Exit from the PostgreSQL session return to the original user: # Exit PostgreSQL session \\q # Return to the original user exit Update the pg_hba.conf File Modify the pg_hba.conf file to configure authentication: sudo nano /etc/postgresql/16/main/pg_hba.conf Add the following line to enable scram-sha-256 authentication for the sonar user: local sonar sonar scram-sha-256   ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"SonarQube","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-sonarqube#installation-steps","content":" Download and Install SonarQube Download the SonarQube package and extract it: cd /opt sudo wget https://binaries.sonarsource.com/Distribution/sonarqube/sonarqube-10.6.0.92116.zip sudo apt install unzip -y sudo unzip sonarqube-10.6.0.92116.zip sudo mv sonarqube-10.6.0.92116 sonarqube Create a SonarQube User Create a dedicated user for running SonarQube and set the correct permissions: sudo adduser sonar sudo chown -R sonar:sonar /opt/sonarqube Update SonarQube Database Configuration Edit the sonar.properties file to configure SonarQube's connection to the PostgreSQL database: sudo nano /opt/sonarqube/conf/sonar.properties Update the PostgreSQL settings: # PostgreSQL settings sonar.jdbc.username=sonar sonar.jdbc.password=your_password sonar.jdbc.url=jdbc:postgresql://localhost/sonar Set Up the SonarQube Service Create a new systemd service file for SonarQube: sudo nano /etc/systemd/system/sonarqube.service Copy the following content into the file: [Unit] Description=SonarQube service After=syslog.target network.target [Service] Type=forking User=sonar Group=sonar ExecStart=/opt/sonarqube/bin/linux-x86-64/sonar.sh start ExecStop=/opt/sonarqube/bin/linux-x86-64/sonar.sh stop ExecReload=/opt/sonarqube/bin/linux-x86-64/sonar.sh restart Restart=on-failure [Install] WantedBy=multi-user.target Reload Systemd and Start SonarQube Reload the systemd daemon and start the SonarQube service: sudo systemctl daemon-reload sudo systemctl start sonarqube sudo systemctl enable sonarqube Confirm SonarQube is Running Verify that SonarQube is running by opening your web browser and navigating to: http://your_ip_address:9000   ","version":"Next","tagName":"h2"},{"title":"Configuring SonarQubeâ€‹","type":1,"pageTitle":"SonarQube","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-sonarqube#configuring-sonarqube","content":" Log into SonarQube and type in the default credentials (username: admin, password: admin). Change your password to something new after the first login. You will be directed to the dashboard. Click on 'Create Project': Create a local project and enter owasp-juice-shop as the display name and project key. Set branch = master. Hit next and set 'Use global setting', then hit 'Create Project'.  ","version":"Next","tagName":"h2"},{"title":"Jenkins Integration with SonarQubeâ€‹","type":1,"pageTitle":"SonarQube","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-sonarqube#jenkins-integration-with-sonarqube","content":" From the Jenkins Dashboard, navigate to Manage Jenkins &gt; Manage Plugins and install the SonarQube Scanner plugin. Navigate to Credentials &gt; System from the Jenkins Dashboard. Click the Global credentials (unrestricted) link in the System table. Click Add credentials and add the following information: Kind: Secret TextScope: GlobalSecret: Generate a token at User &gt; My Account &gt; Security in SonarQube, and copy and paste it here. From the Jenkins Dashboard, navigate to Manage Jenkins &gt; Configure System. In the SonarQube Servers section, click Add SonarQube. Add the following information: Name: Give a unique name to your SonarQube instance.Server URL: Your SonarQube instance URL.Credentials: Select the credentials created in step 4. Click Save to complete the integration.  ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"SonarQube","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/security-tools/install-config-sonarqube#youre-done","content":" Youâ€™ve successfully installed and configured SonarQube and integrated it with Jenkins. This setup allows you to continuously monitor code quality and security vulnerabilities. ","version":"Next","tagName":"h2"},{"title":"Installing a Grafana Dashboard for Jenkins","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/testing-results/install-grafana-jenkins-dashboard","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Installing a Grafana Dashboard for Jenkins","url":"/ar/projects/devsecops-home-lab/testing-results/install-grafana-jenkins-dashboard#overview","content":" This installation process is for setting up a Jenkins performance and health overview dashboard in Grafana for monitoring purposes.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Installing a Grafana Dashboard for Jenkins","url":"/ar/projects/devsecops-home-lab/testing-results/install-grafana-jenkins-dashboard#prerequisites","content":" Find the Dashboard: Browse various Jenkins-related Grafana dashboards here. Import the Jenkins Performance and Health Overview Dashboard: The specific dashboard can be found here. Import the Dashboard: Download or copy the dashboard JSON from the provided link.In Grafana, navigate to the dashboards section.Click on Import.Paste the JSON code or upload the JSON file.Complete the import process.  ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"Installing a Grafana Dashboard for Jenkins","url":"/ar/projects/devsecops-home-lab/testing-results/install-grafana-jenkins-dashboard#installation-steps","content":" Access Grafana: Open your Grafana instance in a web browser. Import the Dashboard: Go to the Dashboards section.Click on the Import button.Either paste the JSON code or upload the JSON file you obtained earlier.Follow the prompts to complete the import.  ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Installing a Grafana Dashboard for Jenkins","url":"/ar/projects/devsecops-home-lab/testing-results/install-grafana-jenkins-dashboard#configuration-steps","content":" Customize the Dashboard: Once imported, you can customize the dashboard to fit your specific monitoring needs. Save the Configuration: Ensure that any changes made to the dashboard are saved.  ","version":"Next","tagName":"h2"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Installing a Grafana Dashboard for Jenkins","url":"/ar/projects/devsecops-home-lab/testing-results/install-grafana-jenkins-dashboard#conclusion","content":" Your Grafana dashboard for Jenkins performance and health overview is now set up and ready for use! ","version":"Next","tagName":"h2"},{"title":"Gitea","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-gitea","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Gitea","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-gitea#overview","content":" This installation happens on the dsb-hub.  According to Gitea's documentation, Gitea is a painless self-hosted all-in-one software development service, including Git hosting, code review, team collaboration, package registry and CI/CD. It's open source under MIT license. It is designed to be lightweight, easy to use, and highly customizable, making it an ideal choice for both small teams and large organizations.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Gitea","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-gitea#prerequisites","content":" Install PostgreSQL: Run the following command to install PostgreSQL and its contrib package: sudo apt install postgresql postgresql-contrib Switch to the PostgreSQL user: sudo -i -u postgres Update the PostgreSQL configuration files: sudo nano /etc/postgresql/16/main/postgresql.conf Scroll down and uncomment the listen_addresses setting, then set it to localhost: listen_addresses = 'localhost' Scroll down and uncomment the password_encryption setting, then set it to scram-sha-256: password_encryption = scram-sha-256 Log into PostgreSQL: Log into the PostgreSQL command line as the postgres user: psql Configure the Database: Create a new role (user) for Gitea with a secure password and a new database owned by that role: CREATE ROLE gitea WITH LOGIN PASSWORD 'your_password'; CREATE DATABASE giteadb WITH OWNER gitea TEMPLATE template0 ENCODING 'UTF8' LC_COLLATE 'en_US.UTF-8' LC_CTYPE 'en_US.UTF-8'; Update the pg_hba.conf file to allow the gitea user to connect to the giteadb database using scram-sha-256: sudo nano /etc/postgresql/16/main/pg_hba.conf Add the following line: local giteadb gitea scram-sha-256 Test Database Connection: Restart the PostgreSQL service and test the connection to the Gitea database: sudo systemctl restart postgresql.service psql -U gitea -d giteadb Install Nginx: Install Nginx using the following command: sudo apt install nginx Configure Nginx: Unlink the default configuration file: sudo unlink /etc/nginx/sites-enabled/default Create a new configuration file for the reverse proxy: sudo nano /etc/nginx/sites-available/reverse-proxy Copy the following configuration into the file, then save and close it: server { listen 80; server_name localhost; location / { client_max_body_size 1000M; # This allows for large uploads up to one GB. Increase if you need more or have insanely large repositories. proxy_pass http://127.0.0.1:3000; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } } Activate the new proxy configuration and restart Nginx: sudo ln -s /etc/nginx/sites-available/reverse-proxy /etc/nginx/sites-enabled/ sudo systemctl restart nginx   ","version":"Next","tagName":"h2"},{"title":"Installation Stepsâ€‹","type":1,"pageTitle":"Gitea","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-gitea#installation-steps","content":" Install Gitea: Install Gitea using Snap: sudo snap install gitea Start the Gitea service: sudo snap start gitea Configure web hooks: sudo nano /var/snap/gitea/common/conf/app.ini # Add this inside of the file [webhook] ALLOWED_HOST_LIST = localhost, 127.0.0.1   ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Gitea","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-gitea#configuration-steps","content":" Configure Gitea: Open a web browser and navigate to your server's IP address or domain name.Follow the on-screen instructions to configure Gitea, entering your previously created PostgreSQL database credentials when prompted. Create an Account: Create an admin account to manage your Gitea instance.  ","version":"Next","tagName":"h2"},{"title":"You're Doneâ€‹","type":1,"pageTitle":"Gitea","url":"/ar/projects/devsecops-home-lab/installation-and-configuration/install-config-gitea#youre-done","content":" Gitea is now successfully installed and configured on your server. Feel free to log in and take a look around. ","version":"Next","tagName":"h2"},{"title":"Clean Up","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/cleanup","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-aws/cleanup#overview","content":" With our environments configured and secrets created, it's time to clean up the Terraform-defined DevSecOps pipeline infrastructure. This guide provides a step-by-step explanation to ensure a proper cleanup of all resources.  ","version":"Next","tagName":"h2"},{"title":"Stepsâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-aws/cleanup#steps","content":" ","version":"Next","tagName":"h2"},{"title":"1. Destroy Repository Resourcesâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-aws/cleanup#1-destroy-repository-resources","content":" Navigate to the repository Terraform directory and destroy the associated resources:  cd terraform/repositories terraform destroy --auto-approve   ","version":"Next","tagName":"h3"},{"title":"2. Destroy EKS Cluster Resourcesâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-aws/cleanup#2-destroy-eks-cluster-resources","content":" Navigate to the EKS cluster Terraform directory and destroy its resources:  cd terraform/eks-cluster terraform destroy --auto-approve   ","version":"Next","tagName":"h3"},{"title":"3. Verify Resource Deletionâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-aws/cleanup#3-verify-resource-deletion","content":" After running the destroy commands, verify that all resources have been deleted by:  Checking the AWS Management Console.Reviewing Terraform logs for confirmation.  ","version":"Next","tagName":"h3"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-aws/cleanup#conclusion","content":" Congratulations! Youâ€™ve successfully completed this project and cleaned up all resources. By properly tearing down your resources, you avoid unnecessary charges and ensure cost efficiency. ","version":"Next","tagName":"h2"},{"title":"DevSecOps Terraform Code - Explained","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/devsecops-terraform-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"DevSecOps Terraform Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/devsecops-terraform-code#overview","content":" With our environments configured and secrets created, it's time to dive into the Terraform code that defines the DevSecOps pipeline infrastructure. This guide provides a detailed explanation of the critical components so you can fully understand how the system works.  ","version":"Next","tagName":"h2"},{"title":"Code Overviewâ€‹","type":1,"pageTitle":"DevSecOps Terraform Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/devsecops-terraform-code#code-overview","content":" All relevant code is located in the terraform folder, which contains two interconnected Terraform workspaces:  EKS ClusterPipelines  ","version":"Next","tagName":"h2"},{"title":"EKS Cluster Workspaceâ€‹","type":1,"pageTitle":"DevSecOps Terraform Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/devsecops-terraform-code#eks-cluster-workspace","content":" This workspace provisions an Elastic Kubernetes Service (EKS) cluster, including node groups and essential cluster resources. While smaller in scope compared to the Pipelines workspace, it lays the foundation for Kubernetes-based deployments. Check out the codebase here.  Files: main.tf: Defines the EKS cluster, node groups, networking components, and default subnets.variables.tf: Configures input variables, including cluster name, region, and node specifications.outputs.tf: Outputs critical information such as the EKS cluster name and endpoint.  ","version":"Next","tagName":"h3"},{"title":"Pipelines Workspaceâ€‹","type":1,"pageTitle":"DevSecOps Terraform Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/devsecops-terraform-code#pipelines-workspace","content":" This workspace contains the infrastructure for setting up CI/CD pipelines. While the folder includes several files, the main.tf file is the core component. Check out the codebase here. Below are the key elements explained in detail:  GitHub Connection Configurationâ€‹  Resource: aws_codestarconnections_connectionPurpose: Establishes a secure connection between AWS CodePipeline and a GitHub repository.Utilizes a random_id resource to generate a unique connection name for traceability.Configures the provider type as &quot;GitHub.&quot;  Default S3 Bucket Configurationâ€‹  Module: default_bucketPurpose: Provisions an S3 bucket for storing CodePipeline artifacts.Standardizes bucket naming conventions using variables.Ensures secure and centralized storage for build and deployment artifacts.  EKS Cluster Configurationâ€‹  Module: cluster_authPurpose: Manages authentication and RBAC settings for the EKS cluster.Grants CodeBuild IAM roles permission to interact with the cluster by associating them with the system:masters group.Adds an IAM user (&quot;your_name&quot;) with administrative privileges to the cluster. You will want to replace this with the user name for the account.  FastAPI Pipeline Configurationâ€‹  Module: awsome_fastapi_pipelinePurpose: Establishes a CI/CD pipeline for the &quot;AWSOME FastAPI&quot; project.Leverages the GitHub connection to pull source code from the repository.Integrates the pipeline with the S3 bucket and EKS cluster for seamless deployments.  Key Pipeline Parametersâ€‹  GitHub Integration: Dynamically links the GitHub connection ARN to CodePipeline.Configures repository details: Repository: The-DevSec-Blueprint/awsome-fastapiBranch: main Build and Deployment: Buildspec: Located at buildspecs/awsome-fastapi/build.yml.Deployspec: Located at buildspecs/awsome-fastapi/deploy.yml.Build environment: Compute type: BUILD_GENERAL1_SMALLImage: aws/codebuild/standard:5.0Environment type: LINUX_CONTAINERPrivileged mode enabled for containerized builds. Security Scanning: Integrates Snyk for vulnerability scanning with SNYK_ORG_ID and SNYK_TOKEN variables.  By understanding the purpose and structure of these Terraform configurations, you'll have a clearer picture of how the DevSecOps pipeline functions, from provisioning infrastructure to enabling secure and automated CI/CD workflows. ","version":"Next","tagName":"h3"},{"title":"Application Code - Explained","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/application-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/application-code#overview","content":" This section provides a detailed explanation of the application's codebase. The project is a simple Python-based FastAPI application that can be run locally or containerized for deployment. Its primary purpose is to demonstrate a secure and automated DevSecOps pipeline while highlighting potential vulnerabilities for testing purposes.  ","version":"Next","tagName":"h2"},{"title":"Defining AWSOME-FastAPIâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/application-code#defining-awsome-fastapi","content":" The project sets up a FastAPI application inside a Docker container. It uses the official Python runtime and includes all the necessary configurations to deploy the app efficiently. Upon starting, the container automatically runs the FastAPI app, exposing it on port 80.  The goal of this project is to push it through a DevSecOps pipeline, as it intentionally contains some vulnerabilities. For more details, you can review the code in the main.py file.  ","version":"Next","tagName":"h2"},{"title":"Requirementsâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/application-code#requirements","content":" Docker: For containerizing and running the application.Python 3.12+: The latest stable version ensures compatibility with modern features.FastAPI: Framework for building the API.Uvicorn: ASGI server for running the application.  ","version":"Next","tagName":"h3"},{"title":"Featuresâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/application-code#features","content":" Dockerized Application: Simplifies deployment using containers.Python 3.12.5 Runtime: Ensures compatibility with the latest features and security patches.Optimized Dependency Installation: Leverages requirements.txt for streamlined package management.  ","version":"Next","tagName":"h3"},{"title":"Project Structureâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/application-code#project-structure","content":" awesome-fastapi/ â”œâ”€â”€ Dockerfile # Configuration for the Docker container â”œâ”€â”€ requirements.txt # Python dependencies â”œâ”€â”€ main.py # Entry point for the FastAPI app (contains sample vulnerabilities) â””â”€â”€ ...   ","version":"Next","tagName":"h3"},{"title":"Setup and Installationâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/application-code#setup-and-installation","content":" 1. Clone the Repositoryâ€‹  Clone the project repository using the following command:  git clone https://github.com/your-username/awesome-fastapi.git cd awesome-fastapi   2. Build the Docker Imageâ€‹  Run the following command in the project root to build the Docker image:  docker build -t awesome-fastapi .   3. Run the Docker Containerâ€‹  After building the image, start the container:  docker run -d -p 80:80 awesome-fastapi   This command will start the FastAPI app on port 80 of your localhost.  4. Access the Applicationâ€‹  Once the container is running, you can access the application in your browser:  http://localhost:80   ","version":"Next","tagName":"h3"},{"title":"Dependenciesâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/application-code#dependencies","content":" The application relies on the following Python packages, specified in the requirements.txt file:  fastapi: The main framework for building APIs.uvicorn: The ASGI server for running the application.  To install these dependencies locally, run:  pip install -r requirements.txt   ","version":"Next","tagName":"h3"},{"title":"Notesâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-aws/code-breakdown/application-code#notes","content":" The default entry point for the FastAPI application is main.py, where the application instance is named app. If your setup differs, update the CMD directive in the Dockerfile accordingly.By default, the container exposes the application on port 80. To use a different port, modify the EXPOSE and CMD directives in the Dockerfile as needed.  This straightforward setup ensures you can run, test, and deploy the FastAPI application with minimal effort while integrating it into a secure DevSecOps pipeline. ","version":"Next","tagName":"h3"},{"title":"Deploying and Configuring Your DevSecOps Pipeline","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/deploying-infrastructure-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/deploying-infrastructure-code#overview","content":" We've finally reached the stage where we deploy our infrastructure using Terraform Cloud. This guide will walk you through creating, configuring, and deploying the necessary DevSecOps pipelines for your project.  ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/deploying-infrastructure-code#configuration-steps","content":" ","version":"Next","tagName":"h2"},{"title":"Terraform Cloud Setupâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/deploying-infrastructure-code#terraform-cloud-setup","content":" Log into Terraform Cloud and select the DSB organization.Click the New button to create a new project. Provide a name and description as needed.Navigate to Workspace, select the project you created, and click Continue.Choose CLI-Driven Workflow (required for GitHub Actions).Enter the workspace name as dsb-aws-devsecops-eks-cluster. Add an optional description and click Create.Repeat the same steps for another workspace named dsb-aws-devsecops-pipelines.  At the end of this process, you should have two workspaces created. Hereâ€™s an example of how they should appear in your organization (without the Run Status applied):    Configuring Secrets in Terraform Cloudâ€‹  Navigate to the dsb-aws-devsecops-pipelines workspace and select Variables. Under Workspace Variables, create two sensitive variables: SNYK_ORG_IDSNYK_TOKEN Populate these variables with the respective values. The final setup should resemble this:  ","version":"Next","tagName":"h3"},{"title":"Deploying Changes via GitHub Actionsâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/deploying-infrastructure-code#deploying-changes-via-github-actions","content":" With the workspaces configured, you can now deploy changes using GitHub Actions.  Log into GitHub and open your forked project: aws-devsecops-pipelines. Navigate to Actions and click on .github/workflows/main.yml. On the right-hand side, select the Run Workflow dropdown and click Run Workflow. This triggers the pipeline to: Checkout the repository.Plan and apply changes in Terraform Cloud.Deploy the EKS Cluster and DevSecOps pipeline for AWSOME-FastAPI. Ù…Ù„Ø§Ø­Ø¸Ø© This process may take around 30 minutes. Feel free to step away during this time as the EKS Cluster creation is time-intensive. Confirm that the plans have been applied successfully. You should see successful builds in both GitHub and Terraform Cloud. Example results are shown below: GitHub Pipeline Execution: Terraform Cloud Deployment: AWS Overview:  ","version":"Next","tagName":"h3"},{"title":"Configuring and Testing CodeStar Connectionâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/deploying-infrastructure-code#configuring-and-testing-codestar-connection","content":" With your infrastructure deployed, the next step is configuring the CodeStar Connection to link AWS with GitHub. This ensures automatic detection and deployment of changes to your Python project, AWSOME-FastAPI.  Navigate to the CodePipeline Dashboard in AWS. Click Settings &gt; Connections and select the dsb-github-connection name. Its status will likely be Pending, which explains why the pipeline is in a failed state. The connection needs to be Active. Click Update Pending Connection. A browser pop-up will appear. Click Install a New App. Select your GitHub username to install the AWS Connector for GitHub. Once redirected back to the Connect to GitHub dashboard, click Connect. The connection status should now display as Available.  With these steps completed, your pipeline is fully operational and ready to detect and deploy changes from your GitHub repository. ","version":"Next","tagName":"h3"},{"title":"Setting Up Snyk Account","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-snyk","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up Snyk Account","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-snyk#overview","content":" This guide will help you create a Snyk account and generate the required token and organization ID for scanning purposes. If you're following along with the video, this document covers the steps that were not detailed due to time constraints.  ","version":"Next","tagName":"h2"},{"title":"Account Creation Processâ€‹","type":1,"pageTitle":"Setting Up Snyk Account","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-snyk#account-creation-process","content":" Navigate to the Snyk Sign-Up Page. Sign up using the GitHub option and follow the prompts to create your account. Note: The sign-up process is straightforward and does not require a credit card initially. Once registration is complete, you will be redirected to your Snyk dashboard:  ","version":"Next","tagName":"h2"},{"title":"Obtaining Tokens and Organization IDâ€‹","type":1,"pageTitle":"Setting Up Snyk Account","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-snyk#obtaining-tokens-and-organization-id","content":" Click on your name at the bottom-left corner of the page and select Account Settings. In the General section, locate the Auth Token field. Click Generate to create a token and make note of it. This token will be required later for integration. Navigate to the Settings page, scroll down to find the Organization ID, and note this ID as well. Youâ€™ll need it for configuring your environment.  With these steps completed, your Snyk account is ready to use! ","version":"Next","tagName":"h2"},{"title":"Configuring Deployment Role in AWS","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/setup/configuring-deployment-role","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Configuring Deployment Role in AWS","url":"/ar/projects/devsecops-pipeline-aws/setup/configuring-deployment-role#overview","content":" This guide walks you through setting up an IAM role in AWS that leverages OpenID Connect (OIDC), allowing Terraform Cloud to assume the role and deploy infrastructure changes on your behalf. Before continuing, ensure you have an AWS account ready for deployment.  ","version":"Next","tagName":"h2"},{"title":"Step 1: Add Terraform Cloud as an OIDC Providerâ€‹","type":1,"pageTitle":"Configuring Deployment Role in AWS","url":"/ar/projects/devsecops-pipeline-aws/setup/configuring-deployment-role#step-1-add-terraform-cloud-as-an-oidc-provider","content":" Log in to your AWS account. Navigate to the IAM Dashboard, go to Identity Providers, and click Add Provider. Complete the form with the following details: Provider Type: OpenID ConnectProvider URL: https://app.terraform.ioAudience: aws.workload.identity  ","version":"Next","tagName":"h2"},{"title":"Step 2: Create an IAM Role for Terraformâ€‹","type":1,"pageTitle":"Configuring Deployment Role in AWS","url":"/ar/projects/devsecops-pipeline-aws/setup/configuring-deployment-role#step-2-create-an-iam-role-for-terraform","content":" In the IAM Dashboard, go to Roles and click Create Role. Under Trusted Entity Type, select Web Identity. Choose the app.terraform.io identity provider you just created. Fill out the trust relationship using the fields below. These settings define which Terraform Cloud runs are allowed to assume this role, based on specific workload identity attributes: Workload Type: Workspace Run This indicates that only actual Terraform runs (not agents or other services) will be able to assume the role. Organization: DSB This should match the name of your Terraform Cloud organization. It restricts access to only runs that originate from this specific org. Project Name: AWS If you're using Terraform Cloud projects, this narrows the access scope to a particular project. You can update this to match the exact name you're using, or leave it open-ended depending on your structure. Workspace Name: * Using an asterisk allows any workspace within the specified organization and project to assume the role. If you prefer to scope this more tightly, you can replace the * with a specific workspace name. Run Phase: * This allows the role to be assumed during any phase of a run (plan, apply, etc). You can scope this more tightly if needed, but * is most flexible during development. ðŸ’¡ These trust conditions form the basis of your IAM roleâ€™s assume role policy. It ensures that only authorized Terraform runs from specific contexts can use this role to deploy resources into AWS. Attach the AdministratorAccess policy (or a scoped-down policy as needed for your environment). Name the role terraform-cloud-deployer-oidc and create it.  After creating the role, note down the Role ARN. Youâ€™ll need this when configuring your Terraform Cloud workspace to assume the role using OIDC. ","version":"Next","tagName":"h2"},{"title":"Setting Up GitHub Repositories","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-github-repos","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-github-repos#overview","content":" This guide will walk you through the process of setting up GitHub repositories in your personal account. If you're unfamiliar with GitHub, it is a web-based platform that leverages Git, a version control system, to help developers manage and track changes in their code. It also facilitates collaboration on projects, tracks revisions, and enables code contributions from anywhere in the world. For more details, check out this article named introduction to GitHub on GeeksforGeeks. Plus, GitHub offers free accounts, which is always a bonus!  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-github-repos#prerequisites","content":" Before proceeding, ensure you have a GitHub account. If you don't already have one, follow this guide to create a GitHub account.  ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-github-repos#configuration-steps","content":" ","version":"Next","tagName":"h2"},{"title":"Forking Repositoriesâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-github-repos#forking-repositories","content":" To begin, you'll need to fork the repositories into your personal GitHub account:  Log in to your GitHub account. Navigate to the landing page of the first project: AWS DevSecOps Pipeline. Click the Fork button in the top-right corner. Select your personal account as the Owner and click Create Fork. Ensure the Copy the main branch only option is enabled. Repeat the above steps for the second project: Awesome FastAPI. Clone both repositories onto your local machine using the following command, as an example: git clone https://github.com/devsecblueprint/awsome-fastapi.git  ","version":"Next","tagName":"h3"},{"title":"Running the Pipeline and Analyzing Outputs","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/executing-pipeline","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Running the Pipeline and Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/executing-pipeline#overview","content":" With the infrastructure deployed and verified, the next step is to execute the pipeline and analyze its outputs. This guide will walk you through running the pipeline, reviewing security scan results, and testing the deployed application.  ","version":"Next","tagName":"h2"},{"title":"Running the Pipelineâ€‹","type":1,"pageTitle":"Running the Pipeline and Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/executing-pipeline#running-the-pipeline","content":" Open the CodePipeline Dashboard and navigate to the awsome-fastapi pipeline. Click the pipeline name. Click Release Change, then confirm by clicking Release. This action triggers the pipeline to: Pull the latest code from the GitHub repository.Build the project.Run tests and security scans.Deploy the application into the EKS Cluster. Ù…Ù„Ø§Ø­Ø¸Ø© The pipeline process may take 10-30 minutes to complete. Use this time to take a break and return once it finishes.  ","version":"Next","tagName":"h2"},{"title":"Reviewing Resultsâ€‹","type":1,"pageTitle":"Running the Pipeline and Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/executing-pipeline#reviewing-results","content":" After the pipeline completes, review the results of the security scans. Below are examples from Snyk and Trivy:    Snyk Results    Trivy Results  The Trivy scan results are extensive and might be challenging to address comprehensively. Focus on the most critical issues first. If you want the pipeline to fail for certain vulnerabilities, you can configure the buildspec.yml file in the AWSOME-FastAPI repository accordingly. Ù…Ù„Ø§Ø­Ø¸Ø© Vulnerabilities may evolve over time, so periodic reviews and updates are essential.  ","version":"Next","tagName":"h2"},{"title":"Testing the API Applicationâ€‹","type":1,"pageTitle":"Running the Pipeline and Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-aws/deployment-and-testing/executing-pipeline#testing-the-api-application","content":" Open the EKS Clusters Dashboard and select the dsb-devsecops-cluster. Under Resources, navigate to Service and Networking &gt; Services and locate the awsome-fastapi service. This will display the following screen: Copy the provided URL and paste it into your web browser. It should resemble the following:  http://aacbaaa4740274a1a83351e8723871d7-2065184365.us-east-1.elb.amazonaws.com/    ","version":"Next","tagName":"h2"},{"title":"Application Code - Explained","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/application-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/application-code#overview","content":" This section provides a detailed explanation of the application's codebase. The project is a simple Python-based FastAPI application that can be run locally or containerized for deployment. Its primary purpose is to demonstrate a secure and automated DevSecOps pipeline while highlighting potential vulnerabilities for testing purposes.  ","version":"Next","tagName":"h2"},{"title":"Defining Azure-FastAPIâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/application-code#defining-azure-fastapi","content":" The project sets up a FastAPI application inside a Docker container. It uses the official Python runtime and includes all the necessary configurations to deploy the app efficiently. Upon starting, the container automatically runs the FastAPI app, exposing it on port 80.  The goal of this project is to push it through a DevSecOps pipeline, as it intentionally contains some vulnerabilities. For more details, you can review the code in the main.py file.  ","version":"Next","tagName":"h2"},{"title":"Requirementsâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/application-code#requirements","content":" Docker: For containerizing and running the application.Python 3.12+: The latest stable version ensures compatibility with modern features.FastAPI: Framework for building the API.Uvicorn: ASGI server for running the application.  ","version":"Next","tagName":"h3"},{"title":"Featuresâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/application-code#features","content":" Dockerized Application: Simplifies deployment using containers.Python 3.12.5 Runtime: Ensures compatibility with the latest features and security patches.Optimized Dependency Installation: Leverages requirements.txt for streamlined package management.  ","version":"Next","tagName":"h3"},{"title":"Project Structureâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/application-code#project-structure","content":" azure-python-fastapi/ â”œâ”€â”€ Dockerfile # Configuration for the Docker container â”œâ”€â”€ requirements.txt # Python dependencies â”œâ”€â”€ main.py # Entry point for the FastAPI app (contains sample vulnerabilities) â””â”€â”€ ...   ","version":"Next","tagName":"h3"},{"title":"Setup and Installationâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/application-code#setup-and-installation","content":" 1. Clone the Repositoryâ€‹  Clone the project repository using the following command:  git clone https://github.com/your-username/azure-python-fastapi.git cd azure-python-fastapi   2. Build the Docker Imageâ€‹  Run the following command in the project root to build the Docker image:  docker build -t azure-python-fastapi .   3. Run the Docker Containerâ€‹  After building the image, start the container:  docker run -d -p 80:80 azure-python-fastapi   This command will start the FastAPI app on port 80 of your localhost.  4. Access the Applicationâ€‹  Once the container is running, you can access the application in your browser:  http://localhost:80   ","version":"Next","tagName":"h3"},{"title":"Dependenciesâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/application-code#dependencies","content":" The application relies on the following Python packages, specified in the requirements.txt file:  fastapi: The main framework for building APIs.uvicorn: The ASGI server for running the application.  To install these dependencies locally, run:  pip install -r requirements.txt   ","version":"Next","tagName":"h3"},{"title":"Notesâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/application-code#notes","content":" The default entry point for the FastAPI application is main.py, where the application instance is named app. If your setup differs, update the CMD directive in the Dockerfile accordingly.By default, the container exposes the application on port 80. To use a different port, modify the EXPOSE and CMD directives in the Dockerfile as needed.  This straightforward setup ensures you can run, test, and deploy the FastAPI application with minimal effort while integrating it into a secure DevSecOps pipeline. ","version":"Next","tagName":"h3"},{"title":"DevSecOps Pipeline - Azure","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/","content":"","keywords":"","version":"Next"},{"title":"Know Before You Goâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - Azure","url":"/ar/projects/devsecops-pipeline-azure/#know-before-you-go","content":" This project is a little expense, and you will rack up a nice bill in Azure if you leave all your resources created. Therefore, I recommend that you TEAR IT ALL DOWN when you're done.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitiesâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - Azure","url":"/ar/projects/devsecops-pipeline-azure/#prerequisities","content":" Before you begin this, you will want to have some knowledge of Azure services and how they work, as well as prior knowledge of Terraform.You will also want to ensure that you have an Azure project created. You can go through the account creation process here: Azure Project Creation ProcessMake sure you have the following installed on your local machine: PythonGitDockerTerraform CLIAzure CLI  ","version":"Next","tagName":"h2"},{"title":"Overviewâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - Azure","url":"/ar/projects/devsecops-pipeline-azure/#overview","content":" So you've decided to go down the path of building your own Cloud Native DevSecOps pipeline within Azure? If so, you've come to the right place! We are going to show you how to setup your own Azure pipeline using Terraform Cloud. Unlike the DevSecOps Home Lab, we're just focused on developing the pipeline and deploying our containerized application into the Azure Container Registry.  Luckily for you all, you won't need to do anything. we've taken the liberty of developing all of the code for you. These are the two GitHub repositories that you need to look at before we get started:  DevSecOps Pipeline Infrastructure: https://github.com/devsecblueprint/azure-devsecops-pipelineFastAPI Application with Pipeline Definition: https://github.com/devsecblueprint/azure-python-fastapi  ","version":"Next","tagName":"h2"},{"title":"Architecture Diagramâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - Azure","url":"/ar/projects/devsecops-pipeline-azure/#architecture-diagram","content":"   ","version":"Next","tagName":"h2"},{"title":"Architecture Breakdownâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - Azure","url":"/ar/projects/devsecops-pipeline-azure/#architecture-breakdown","content":" At a VERY high level, this architecture briefly covers the services that we will be leveraging for the DevSecOps Pipelines. Here are the descriptions with intent of each service:  Azure DevOps â€“ Provides end-to-end CI/CD pipelines and project management tools to automate builds, tests, and deployments in the cloud.Azure Container Registry â€“ A secure, private registry for storing and managing Docker container images used in your deployments.Federated Identity / User Assigned Identity â€“ Enables workloads to authenticate securely to Azure resources without embedding secrets, by leveraging managed or federated identities.Resource Group â€“ A logical container that organizes and manages related Azure resources as a single unit for easier governance and lifecycle management.Azure Resource Manager Service Connection (AzureRM) â€“ Connects Azure DevOps pipelines to Azure subscriptions, enabling deployments and resource management through secure federated authentication.  ","version":"Next","tagName":"h3"},{"title":"Flow Diagramâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - Azure","url":"/ar/projects/devsecops-pipeline-azure/#flow-diagram","content":" Now that we've covered the architecture diagram, let's put this together so you can understand the flow and how everything is supposed to work.    Flow Diagram Explainedâ€‹  A developer writes code and commits the changes to GitHub. This action triggers the Azure DevOps pipeline.The Azure DevOps pipeline runs according to the stages defined in its YAML configuration file stored in GitHub. These stages include building, testing, scanning, and deploying the applicationThe Python application image is built. In addition, the source code is checked for any formatting and linting errors.The Python source code, dependencies and the containerized application will be scanned for any security vulnerabilities using ZAP by Checkmarx and Trivy.Upon successful completion of all pipeline stages, the containerized application is checked into Azure Container Registry.  ","version":"Next","tagName":"h3"},{"title":"What Youâ€™ll Learnâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - Azure","url":"/ar/projects/devsecops-pipeline-azure/#what-youll-learn","content":" By working through this guide, youâ€™ll gain hands-on experience building and deploying a secure, cloud-native DevSecOps pipeline on Azure. Specifically, you will learn how to:  Configure and manage Azure resources using Terraform Cloud.Integrate GitHub with Azure DevOps for version control and pipeline triggers.Use Azure DevOps to automate CI/CD processes, including build, test, and deployment stages.Perform security scans on code and dependencies using tools like Trivy and ZAP by Checkmarx.Deploy containerized application or image into Azure Container Registry.  With all that being stated, Please follow the order of the documents, otherwise you'll most likely run into errors and get lost. ","version":"Next","tagName":"h2"},{"title":"Setting Up Terraform Cloud","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-terraform-cloud","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up Terraform Cloud","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-terraform-cloud#overview","content":" Terraform Cloud is an essential tool that simplifies infrastructure management and will quickly become a key part of your workflow. If youâ€™re unfamiliar with Terraform, itâ€™s highly recommended that you take some time to study it before proceeding. This guide will walk you through creating a Terraform Cloud account and setting up your first organization.  ","version":"Next","tagName":"h2"},{"title":"Creating Your Account and Organizationâ€‹","type":1,"pageTitle":"Setting Up Terraform Cloud","url":"/ar/projects/devsecops-pipeline-aws/setup/setup-terraform-cloud#creating-your-account-and-organization","content":" Navigate to the Terraform Cloud Sign-Up Page. Click Continue with HCP account to proceed with registration. Select Sign Up at the bottom of the page, then click Continue with GitHub to link your account. Note: If you are redirected back to the sign-in page, click Continue with HCP and then choose Sign in with GitHub. Once logged in, you should arrive at the Terraform Cloud dashboard. If the &quot;DSB&quot; organization is not already created, it will appear empty: Click Create Organization, and for the organization name, enter DSB. You can rename this organization later as needed after configuring the code. ","version":"Next","tagName":"h2"},{"title":"Deploying and Configuring Your DevSecOps Pipeline","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/deploying-infrastructure-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/deploying-infrastructure-code#overview","content":" We've finally reached the stage where we deploy our infrastructure using Terraform Cloud. This guide will walk you through creating, configuring, and deploying the necessary DevSecOps pipelines for your project.  ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/deploying-infrastructure-code#configuration-steps","content":" ","version":"Next","tagName":"h2"},{"title":"Deploying Changes via GitHub Actionsâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/deploying-infrastructure-code#deploying-changes-via-github-actions","content":" With the workspaces configured, you can now deploy changes using GitHub Actions.  Log into GitHub and open your forked project: azure-devsecops-pipeline. Navigate to Actions and click on .github/workflows/main.yml. On the right-hand side, select the Run Workflow dropdown and click Run Workflow. This triggers the pipeline to: Checkout the repository.Plan and apply changes in Terraform Cloud.Create Cloud Build pipeline and any additional resources. Confirm that the plans have been applied successfully. You should see successful builds in both GitHub and Terraform Cloud. Example results are shown below: GitHub Pipeline Execution: Terraform Cloud Deployment:  With these steps completed, your pipeline is fully operational and ready to detect and deploy changes from your GitHub repository. ","version":"Next","tagName":"h3"},{"title":"Configuring Secrets and Environment Variables","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-aws/setup/config-secrets","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Configuring Secrets and Environment Variables","url":"/ar/projects/devsecops-pipeline-aws/setup/config-secrets#overview","content":" Now that the foundational setup is complete, this guide will walk you through configuring secrets and environment variables within both GitHub and Terraform Cloud.  ","version":"Next","tagName":"h2"},{"title":"Terraform Cloud Configurationâ€‹","type":1,"pageTitle":"Configuring Secrets and Environment Variables","url":"/ar/projects/devsecops-pipeline-aws/setup/config-secrets#terraform-cloud-configuration","content":" Log in to Terraform Cloud and select the DSB organization. On the left-hand menu, click Settings &gt; Variable Sets. You should see a screen similar to this: Click Create Organization Variable Set, and fill in the following details: Name: Provide a meaningful name for the variable set.Description: Add a brief description for clarity.Variable Set Scope: Select Apply to all projects and workspaces. (You can modify this later if needed.) Scroll down to the Variables section and click Add Variable. Add the following keys, marking them as Environment Variables: TFC_AWS_PROVIDER_AUTH: Set this to true.TFC_AWS_RUN_ROLE_ARN: Paste the Role ARN that you saved earlier here. After adding the variables, your variable set should look similar to this:  ","version":"Next","tagName":"h2"},{"title":"GitHub Configurationâ€‹","type":1,"pageTitle":"Configuring Secrets and Environment Variables","url":"/ar/projects/devsecops-pipeline-aws/setup/config-secrets#github-configuration","content":" After forking the repositories, you need to configure the necessary secrets for GitHub Actions in the aws-devsecops-pipeline repository. These secrets will enable automated deployments when updates are pushed to the main branch.  Log in to GitHub and open the aws-devsecops-pipeline repository.Navigate to Settings &gt; Secrets and Variables under the Security section.Click Actions, then select New Repository Secret.Create a secret with the name TF_API_TOKEN and paste the Terraform Cloud token you generated earlier.  With these steps completed, your repositories and environment are fully configured and ready for use. ","version":"Next","tagName":"h2"},{"title":"Creating & Configuring Deployment Principals","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/setup/creating-deployment-principals","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Creating & Configuring Deployment Principals","url":"/ar/projects/devsecops-pipeline-azure/setup/creating-deployment-principals#overview","content":" Before your pipeline can deploy infrastructure and containerized applications, it needs a way to authenticate securely with Azure and GitHub. This section walks you through creating and configuring the deployment principals required for Terraform, Azure Container Registry (ACR), and GitHub.  Youâ€™ll create:  A Terraform Service Principal to provision resources in Azure.A Container Registry Service Principal to manage images in ACR/AKS.A GitHub Personal Access Token (PAT) to integrate with your repository.  ","version":"Next","tagName":"h2"},{"title":"Terraform Service Principalâ€‹","type":1,"pageTitle":"Creating & Configuring Deployment Principals","url":"/ar/projects/devsecops-pipeline-azure/setup/creating-deployment-principals#terraform-service-principal","content":" This principal gives Terraform the necessary rights to deploy and manage Azure resources.  Head to App registrations in the Azure portal and click New registration. Enter a name (e.g., test-registration) and click Register. Navigate to your Subscription in the Azure portal. Go to Access Control (IAM) â†’ Role assignments.Click + Add â†’ Add role assignment.Assign the Contributor role to your service principal. Repeat the process and also assign the User Access Administrator role. Back in your app registration, go to Certificates &amp; secrets. Click + New client secret. Set the expiry to 180 days. Save the generated Client ID and Client Secret â€” youâ€™ll need them later. important These credentials are sensitive. Store them securely in a secret manager or someplace safe locally.  ","version":"Next","tagName":"h2"},{"title":"Container Registry Service Principalâ€‹","type":1,"pageTitle":"Creating & Configuring Deployment Principals","url":"/ar/projects/devsecops-pipeline-azure/setup/creating-deployment-principals#container-registry-service-principal","content":" This principal will be used for authentication with ACR or AKS.  Navigate again to App registrations. Click New registration and provide a name that indicates its purpose (e.g., test-registration-CR or test-registration-AKS). Once created, go to Certificates &amp; secrets. Click + New client secret.Set the expiry to 180 days.Save the Client ID and Client Secret securely for later use.  ","version":"Next","tagName":"h2"},{"title":"GitHub Personal Access Token (PAT)â€‹","type":1,"pageTitle":"Creating & Configuring Deployment Principals","url":"/ar/projects/devsecops-pipeline-azure/setup/creating-deployment-principals#github-personal-access-token-pat","content":" A GitHub PAT allows your pipeline to push and pull code securely from your repository.  Log in to GitHub and head to: Personal Access Tokens. Select Fine-grained tokens.Click Generate new token and choose your forked repository (e.g., azure-python-fastapi).Assign the required repository-level scopes. Copy the token and store it securely (e.g., GitHub Secrets, Azure Key Vault). Your screen should look similar to this once generated:  ","version":"Next","tagName":"h2"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Creating & Configuring Deployment Principals","url":"/ar/projects/devsecops-pipeline-azure/setup/creating-deployment-principals#conclusion","content":" Thatâ€™s it! Youâ€™ve set up all the deployment principals needed for your DevSecOps pipeline. With these in place, Terraform can provision resources, your pipeline can interact with ACR/AKS, and GitHub can authenticate workflows securely. ","version":"Next","tagName":"h2"},{"title":"Clean Up","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/cleanup","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-azure/cleanup#overview","content":" Once youâ€™ve finished working with the pipeline, itâ€™s important to clean up your environment. This prevents unnecessary costs and ensures you leave no lingering resources in your Azure subscription.  This guide walks you through destroying the Terraform-defined infrastructure and verifying that everything has been removed.  ","version":"Next","tagName":"h2"},{"title":"Stepsâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-azure/cleanup#steps","content":" ","version":"Next","tagName":"h2"},{"title":"1. Destroy Terraform Resourcesâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-azure/cleanup#1-destroy-terraform-resources","content":" From the repository root, navigate into the Terraform directory and run the following command:  cd terraform/ terraform destroy --auto-approve   This will deprovision all resources that were created during the setup phase, including the Azure DevOps project and associated service connections.  important Always double-check that youâ€™re operating in the correct environment before running destroy. Running this in production could remove critical infrastructure.  ","version":"Next","tagName":"h3"},{"title":"2. Verify Resource Deletionâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-azure/cleanup#2-verify-resource-deletion","content":" After Terraform completes, verify that resources have been properly deleted:  Log in to the Azure Portal and confirm that the project, pipelines, and container registry no longer exist.Review the Terraform destroy logs to ensure there were no errors or skipped resources.  Ù…Ù„Ø§Ø­Ø¸Ø© Sometimes resource deletions may take a few minutes to fully propagate in the Azure Portal.  ","version":"Next","tagName":"h3"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Clean Up","url":"/ar/projects/devsecops-pipeline-azure/cleanup#conclusion","content":" Congratulations! Youâ€™ve successfully completed the project and cleaned up all resources.  By tearing down your environment at the end, you:  Avoid unexpected Azure billing charges.Keep your environment clean for future projects.Practice good cloud resource management habits. ","version":"Next","tagName":"h2"},{"title":"Configuring Secrets & Environment Variables","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/setup/config-secrets","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Configuring Secrets & Environment Variables","url":"/ar/projects/devsecops-pipeline-azure/setup/config-secrets#overview","content":" With your deployment principals created, the next step is wiring up secrets and environment variables so Terraform Cloud and GitHub Actions can authenticate securely. This ensures your pipeline can provision resources and trigger deployments without exposing sensitive credentials.  In this section, youâ€™ll configure:  Terraform Cloud Variable Sets for managing environment variables at the organization level.GitHub Repository Secrets for authenticating pipelines.  ","version":"Next","tagName":"h2"},{"title":"Terraform Cloud Configurationâ€‹","type":1,"pageTitle":"Configuring Secrets & Environment Variables","url":"/ar/projects/devsecops-pipeline-azure/setup/config-secrets#terraform-cloud-configuration","content":" Log in to Terraform Cloud and select your DSB organization. From the left-hand menu, go to Settings â†’ Variable Sets. Youâ€™ll see a page like this: Click Create Organization Variable Set and fill out the details: Name: Something descriptive (e.g., Azure Deployment Variables).Description: Add a short explanation for clarity.Variable Set Scope: Select Apply to all projects and workspaces. (This can be narrowed later if needed.) Under the Variables section, click Add Variable and define the following keys. Be sure to set each one as an Environment Variable: TFC_AZ_CLIENT_ID: Application ID of the Terraform Deployment Service PrincipalTFC_AZ_CLIENT_PASSWORD: Client Secret value of the Terraform Deployment Service PrincipalTFC_AZ_DEVOPS_GITHUB_PAT: GitHub PAT generated earlierTFC_AZ_DEVOPS_ORG_SERVICE_URL: https://dev.azure.com/your_organizationTFC_AZ_DEVOPS_PAT: Azure DevOps PAT you created earlierTFC_AZ_SUBSCRIPTION_NAME: Subscription NameTFC_AZ_SUBSCRIPTION_ID: Subscription ID of your default subscriptionTFC_AZ_TENANT_ID: Directory (Tenant) ID of the Terraform Service Principal Once complete, your variable set should look something like this:  warning These values are sensitive. Store them securely and rotate them regularly to maintain security best practices.  ","version":"Next","tagName":"h2"},{"title":"GitHub Configurationâ€‹","type":1,"pageTitle":"Configuring Secrets & Environment Variables","url":"/ar/projects/devsecops-pipeline-azure/setup/config-secrets#github-configuration","content":" Next, configure GitHub to store the secrets required by your pipeline. This will allow GitHub Actions to securely connect to Terraform Cloud.  Log in to GitHub and open your fork of the azure-devsecops-pipeline repository. Navigate to Settings â†’ Secrets and Variables under the Security section. Click Actions, then select New Repository Secret. Create a secret named: TF_API_TOKEN â†’ Paste in the Terraform Cloud API token you generated earlier.  ","version":"Next","tagName":"h2"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Configuring Secrets & Environment Variables","url":"/ar/projects/devsecops-pipeline-azure/setup/config-secrets#conclusion","content":" Thatâ€™s it! Youâ€™ve successfully configured your secrets in Terraform Cloud and GitHub. With this step complete, your pipeline is now ready to authenticate, provision resources, and automate deployments securely. ","version":"Next","tagName":"h2"},{"title":"Setting Up Azure DevOps","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-azure-devops","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up Azure DevOps","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-azure-devops#overview","content":" Before wiring up your pipeline, youâ€™ll need an Azure DevOps organization and a Personal Access Token (PAT). This section walks you through creating your DevOps organization, setting its visibility, and generating a PAT that will be used later in your pipeline configuration.  ","version":"Next","tagName":"h2"},{"title":"Creating an Azure DevOps Organizationâ€‹","type":1,"pageTitle":"Setting Up Azure DevOps","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-azure-devops#creating-an-azure-devops-organization","content":" While logged into the Azure Portal, open a new tab and go to Azure DevOps. Fill out the form with your details and click Continue. Make sure to set the Visibility to Private. Once complete, navigate back to the Azure DevOps homepage. Click New organization and choose a unique name (example: devsecblueprint).  ","version":"Next","tagName":"h2"},{"title":"Generating a Personal Access Token (PAT)â€‹","type":1,"pageTitle":"Setting Up Azure DevOps","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-azure-devops#generating-a-personal-access-token-pat","content":" In Azure DevOps, click the gear icon in the top-right corner and select Personal access tokens. Click New Token. Set the scope to Full access.Generate and save the token securely â€” youâ€™ll use this later in your Terraform and pipeline setup.  ","version":"Next","tagName":"h2"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Setting Up Azure DevOps","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-azure-devops#conclusion","content":" Thatâ€™s it! ðŸŽ‰ You now have an Azure DevOps organization and a Personal Access Token ready to go. With this in place, youâ€™ll be able to integrate Azure DevOps into your pipeline workflows in the next steps. ","version":"Next","tagName":"h2"},{"title":"Terraform and Pipeline Code - Explained","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#overview","content":" With your environment configured and secrets set up, itâ€™s time to look under the hood at the Terraform code that provisions the DevSecOps pipeline on Azure. This guide breaks down the structure of the Terraform configuration and explains the purpose of each file so you can clearly understand how the infrastructure is deployed and managed.  ","version":"Next","tagName":"h2"},{"title":"Code Overviewâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#code-overview","content":" All Terraform code is located in the terraform folder. This folder contains a single configuration that provisions the Azure DevOps project, service connections, and pipelines needed to build and deploy your FastAPI application securely.  Hereâ€™s a breakdown of the key files:  ","version":"Next","tagName":"h2"},{"title":"main.tfâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#maintf","content":" This is the core file that provisions all Azure DevOps resources.  Azure DevOps Projectâ€‹  Resource: azuredevops_project Purpose: Creates a new Azure DevOps project (python-fastapi).Configures features such as pipelines (enabled) and disables unused ones (artifacts, boards, test plans, repositories).Sets the project visibility to private.  important Always keep your project visibility set to private for security reasons.  GitHub Service Connectionâ€‹  Resource: azuredevops_serviceendpoint_github Purpose: Establishes a secure service connection between Azure DevOps and GitHub.Uses the TFC_AZ_DEVOPS_GITHUB_PAT variable for authentication. Benefit: Enables Azure DevOps to fetch code directly from your forked repository.  Build Definitionâ€‹  Resource: azuredevops_build_definition Purpose: Defines a build pipeline in Azure DevOps.Delegates build logic to the YAML file stored in the repository (.azdo-pipelines/build.yml).Configures CI triggers to automatically run on commits to the main branch. Repository Details: Source: devsecblueprint/azure-python-fastapi (your forked repo).Branch: refs/heads/main. Advantage: Keeps pipeline logic versioned and controlled alongside your code.  Ù…Ù„Ø§Ø­Ø¸Ø© By storing pipeline definitions in your repo, you ensure all changes are tracked and version-controlled.  ","version":"Next","tagName":"h3"},{"title":"variables.tfâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#variablestf","content":" Defines input variables for sensitive and configurable values, including: GitHub PAT (TFC_AZ_DEVOPS_GITHUB_PAT)Azure Subscription ID, Tenant ID, Client ID, and Client Secret Encourages reusability and environment-specific customization.  important Sensitive values should always be injected via Terraform Cloud variable sets, never hardcoded.  ","version":"Next","tagName":"h3"},{"title":"providers.tfâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#providerstf","content":" Configures the Azure DevOps provider.Authenticates using your Azure DevOps PAT (TFC_AZ_DEVOPS_PAT).Ensures Terraform can provision and manage DevOps projects, pipelines, and connections.  ØªÙ„Ù…ÙŠØ­ Lock provider versions to prevent unexpected changes in behavior during upgrades.  ","version":"Next","tagName":"h3"},{"title":"data.tfâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#datatf","content":" Used to fetch or reference existing Azure DevOps and Azure resources.Keeps the configuration modular by reusing values instead of hardcoding them.  ","version":"Next","tagName":"h3"},{"title":"outputs.tfâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#outputstf","content":" Provides outputs such as project IDs, pipeline IDs, or service endpoint IDs after a successful apply.Makes it easier to reference critical resources when wiring up other configurations.  Ù…Ù„Ø§Ø­Ø¸Ø© Use outputs to pass important values into other systems, but never expose sensitive data like secrets or tokens.  ","version":"Next","tagName":"h3"},{"title":"FastAPI Projectâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#fastapi-project","content":" Alongside the Terraform infrastructure, the pipeline integrates with a FastAPI application that will be scanned, built, and deployed. The application repository includes an .azdo-pipelines folder, which contains the pipeline definitions and templates.  ","version":"Next","tagName":"h2"},{"title":"Pipeline Entry Pointâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#pipeline-entry-point","content":" azure-pipelines.yml Main entry point for Azure DevOps.References modular templates for each stage of the pipeline.Controls the build, scan, and deployment lifecycle of the FastAPI app.  ","version":"Next","tagName":"h3"},{"title":"Pipeline Templatesâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#pipeline-templates","content":" The pipeline_templates/ folder contains reusable steps broken down by function:  build-image.yml: Builds the FastAPI Docker image.push-image.yml: Pushes the built image to the container registry.linting.yml: Runs code quality checks (linting) against the FastAPI project.unit-sec-scan.yml: Executes unit tests and security scans against the codebase.deploy.yml: Handles deployment of the FastAPI image to the target environment (such as AKS).  important These templates are modular, which means you can reuse them across multiple pipelines or projects without duplicating logic.  ","version":"Next","tagName":"h3"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Terraform and Pipeline Code - Explained","url":"/ar/projects/devsecops-pipeline-azure/code-breakdown/devsecops-terraform-code#conclusion","content":" This Terraform configuration defines the backbone of your Azure DevSecOps pipeline, while the FastAPI project provides the application layer that is continuously scanned, built, and deployed.  Together, they enable a secure and automated workflow where:  Terraform provisions the Azure DevOps infrastructure.Pipelines orchestrate builds, tests, scans, and deployments.The FastAPI app serves as the workload being protected and delivered through DevSecOps practices. ","version":"Next","tagName":"h2"},{"title":"Setting Up Terraform Cloud","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-terraform-cloud","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up Terraform Cloud","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-terraform-cloud#overview","content":" Terraform Cloud is an essential tool that simplifies infrastructure management and will quickly become a key part of your workflow. If youâ€™re unfamiliar with Terraform, itâ€™s highly recommended that you take some time to study it before proceeding. This guide will walk you through creating a Terraform Cloud account and setting up your first organization.  ","version":"Next","tagName":"h2"},{"title":"Creating Your Account and Organizationâ€‹","type":1,"pageTitle":"Setting Up Terraform Cloud","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-terraform-cloud#creating-your-account-and-organization","content":" Navigate to the Terraform Cloud Sign-Up Page. Click Continue with HCP account to proceed with registration. Select Sign Up at the bottom of the page, then click Continue with GitHub to link your account. Note: If you are redirected back to the sign-in page, click Continue with HCP and then choose Sign in with GitHub. Once logged in, you should arrive at the Terraform Cloud dashboard. If the &quot;DSB&quot; organization is not already created, it will appear empty: Click Create Organization, and for the organization name, enter DSB. You can rename this organization later as needed after configuring the code. ","version":"Next","tagName":"h2"},{"title":"Running the Pipeline & Analyzing Outputs","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/executing-pipeline","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Running the Pipeline & Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/executing-pipeline#overview","content":" With the infrastructure deployed and verified, the next step is to execute the pipeline and analyze its outputs. This guide will walk you through running the pipeline, reviewing the results of integrated security scans, and validating the deployed container image.  ","version":"Next","tagName":"h2"},{"title":"Running the Pipelineâ€‹","type":1,"pageTitle":"Running the Pipeline & Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/executing-pipeline#running-the-pipeline","content":" Navigate to the Azure DevOps project called python-fastapi. Click the project, then select Pipelines. Under the All tab, click the pipeline named Default. Click Run pipeline and wait for the results. This process typically takes 15â€“20 minutes.  important If you encounter an exception related to enabling parallelism, youâ€™ll need to fill out this request form: Azure DevOps Parallelism Request. Approval takes about 3 business days, and no notification will be sent once your request is approved.  ","version":"Next","tagName":"h2"},{"title":"Reviewing Resultsâ€‹","type":1,"pageTitle":"Running the Pipeline & Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/executing-pipeline#reviewing-results","content":" Once the pipeline completes, you can review the outputs from multiple security scans.  ","version":"Next","tagName":"h2"},{"title":"Trivy Dependency Scanâ€‹","type":1,"pageTitle":"Running the Pipeline & Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/executing-pipeline#trivy-dependency-scan","content":"   Detects vulnerabilities in application dependencies.Focus on addressing critical and high-severity issues first.  ","version":"Next","tagName":"h3"},{"title":"Trivy Image Scanâ€‹","type":1,"pageTitle":"Running the Pipeline & Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/executing-pipeline#trivy-image-scan","content":"   Scans the built Docker image for OS-level vulnerabilities.Ensures your container base image and packages are hardened before deployment.  ","version":"Next","tagName":"h3"},{"title":"OWASP ZAP Scanâ€‹","type":1,"pageTitle":"Running the Pipeline & Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/executing-pipeline#owasp-zap-scan","content":"   Runs a dynamic application security test (DAST) against the deployed FastAPI service.Detects web vulnerabilities such as injection flaws, insecure headers, and weak authentication.  ØªÙ„Ù…ÙŠØ­ Trivy scan results can be extensive. If you want the pipeline to fail on specific vulnerabilities, you can modify the unit-sec-test.yml file in the azure-python-fastapi repository to enforce stricter thresholds.  ","version":"Next","tagName":"h3"},{"title":"Validating the Imageâ€‹","type":1,"pageTitle":"Running the Pipeline & Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/executing-pipeline#validating-the-image","content":" Log in to the Azure Portal and search for your container registry (DSBContainerRegistry). Navigate to Repositories â†’ python-fastapi. Click the repository link and verify that a new image tag has been published.  This confirms that your pipeline not only scanned the application but also successfully pushed the image to your Azure Container Registry.  ","version":"Next","tagName":"h2"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Running the Pipeline & Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-azure/deployment-and-testing/executing-pipeline#conclusion","content":" Youâ€™ve now executed the Azure DevOps pipeline and validated its outputs:  Security scans (Trivy + OWASP ZAP) provide visibility into vulnerabilities.Scan thresholds can be tuned to enforce stricter build gates.The FastAPI Docker image is built, scanned, and pushed into Azure Container Registry.  Youâ€™re done! Your DevSecOps pipeline is fully operational, combining automated builds, security checks, and deployments. ","version":"Next","tagName":"h2"},{"title":"Deploying and Configuring Your DevSecOps Pipeline","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/deployment-and-testing/deploying-infrastructure-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-gcp/deployment-and-testing/deploying-infrastructure-code#overview","content":" We've finally reached the stage where we deploy our infrastructure using Terraform Cloud. This guide will walk you through creating, configuring, and deploying the necessary DevSecOps pipelines for your project.  ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-gcp/deployment-and-testing/deploying-infrastructure-code#configuration-steps","content":" ","version":"Next","tagName":"h2"},{"title":"Deploying Changes via GitHub Actionsâ€‹","type":1,"pageTitle":"Deploying and Configuring Your DevSecOps Pipeline","url":"/ar/projects/devsecops-pipeline-gcp/deployment-and-testing/deploying-infrastructure-code#deploying-changes-via-github-actions","content":" With the workspaces configured, you can now deploy changes using GitHub Actions.  Log into GitHub and open your forked project: gcp-dsb-devsecops-infra. Navigate to Actions and click on .github/workflows/main.yml. On the right-hand side, select the Run Workflow dropdown and click Run Workflow. This triggers the pipeline to: Checkout the repository.Plan and apply changes in Terraform Cloud.Create Cloud Build pipeline and any additional resources. Confirm that the plans have been applied successfully. You should see successful builds in both GitHub and Terraform Cloud. Example results are shown below: GitHub Pipeline Execution: Terraform Cloud Deployment:  With these steps completed, your pipeline is fully operational and ready to detect and deploy changes from your GitHub repository. ","version":"Next","tagName":"h3"},{"title":"Setting Up GitHub Repositories","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-github-repos","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-github-repos#overview","content":" Your pipeline relies on GitHub repositories to host both the application code and the infrastructure definitions. In this section, youâ€™ll fork the required repositories into your personal GitHub account so you can make changes, push updates, and run the pipeline independently.  If youâ€™re new to GitHub, itâ€™s a web-based platform built on top of Git (a distributed version control system). It allows developers to manage code, track changes, collaborate, and contribute from anywhere in the world. For a quick primer, check out this Introduction to GitHub article.  ØªÙ„Ù…ÙŠØ­ GitHub offers free accounts, so you can get started without any upfront cost.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-github-repos#prerequisites","content":" Before proceeding, make sure you have a GitHub account.  If you donâ€™t already have one, follow this guide to create a GitHub account.  ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-github-repos#configuration-steps","content":" ","version":"Next","tagName":"h2"},{"title":"Forking Repositoriesâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-github-repos#forking-repositories","content":" Log in to your GitHub account. Navigate to the first project:Azure DevSecOps Infrastructure Click the Fork button in the top-right corner. Select your personal account as the Owner and click Create Fork. Make sure the option Copy the main branch only is enabled. Repeat the same steps for the second project: Azure FastAPI Once forked, clone both repositories to your local machine. For example: git clone https://github.com/&lt;your-username&gt;/azure-python-fastapi git clone https://github.com/&lt;your-username&gt;/azure-devsecops-pipeline   ","version":"Next","tagName":"h3"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-azure/setup/setup-github-repos#conclusion","content":" Thatâ€™s it! ðŸŽ‰ You now have your own copies of the infrastructure and application repositories inside your GitHub account. With these set up, youâ€™re ready to configure secrets and wire up your DevSecOps pipeline in the next steps. ","version":"Next","tagName":"h2"},{"title":"DevSecOps Pipeline - GCP","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/","content":"","keywords":"","version":"Next"},{"title":"Know Before You Goâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GCP","url":"/ar/projects/devsecops-pipeline-gcp/#know-before-you-go","content":" This project is a little expense, and you will rack up a nice bill in GCP if you leave all your resources created. Therefore, I recommend that you TEAR IT ALL DOWN when you're done.  ","version":"Next","tagName":"h2"},{"title":"Prerequisitiesâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GCP","url":"/ar/projects/devsecops-pipeline-gcp/#prerequisities","content":" Before you begin this, you will want to have some knowledge of GCP services and how they work, as well as prior knowledge of Terraform.You will also want to ensure that you have an GCP project created. You can go through the account creation process here: GCP Project Creation ProcessMake sure you have the following installed on your local machine: PythonGitDockerTerraform CLIgcloud CLI  ","version":"Next","tagName":"h2"},{"title":"Overviewâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GCP","url":"/ar/projects/devsecops-pipeline-gcp/#overview","content":" So you've decided to go down the path of building your own Cloud Native DevSecOps pipeline within GCP? If so, you've come to the right place! We are going to show you how to setup your own GCP pipeline using Terraform Cloud. Unlike the DevSecOps Home Lab, we're just focused on developing the pipeline and deploying our application onto a Cloud Run resource.  Luckily for you all, you won't need to do anything. we've taken the liberty of developing all of the code for you. These are the two GitHub repositories that you need to look at before we get started:  DevSecOps Pipeline Infrastructure: https://github.com/devsecblueprint/dsb-gcp-devsecops-infraFastAPI Application with Pipeline Definition: https://github.com/devsecblueprint/gcp-python-fastapi  ","version":"Next","tagName":"h2"},{"title":"Architecture Diagramâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GCP","url":"/ar/projects/devsecops-pipeline-gcp/#architecture-diagram","content":"   ","version":"Next","tagName":"h2"},{"title":"Architecture Breakdownâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GCP","url":"/ar/projects/devsecops-pipeline-gcp/#architecture-breakdown","content":" At a VERY high level, this architecture briefly covers the services that we will be leveraging for the DevSecOps Pipelines. Here are the descriptions with intent of each service:  Artifact Registry: Stores container images and application artifacts for deployments.Cloud IAM: Provides secure identity and access management service accounts with roles for pipeline operations.Cloud Build: Automates build, test, and deployment processes within CI/CD workflows. This is defined by within the FastAPI Application Project.Cloud Storage: Stores build artifacts and logs generated during pipeline execution.Secret Manager: Securely manages sensitive data like API keys and credentials for pipelines.  ","version":"Next","tagName":"h3"},{"title":"Flow Diagramâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GCP","url":"/ar/projects/devsecops-pipeline-gcp/#flow-diagram","content":" Now that we've covered the architecture diagram, let's put this together so you can understand the flow and who everything is supposed to work.    Flow Diagram Explainedâ€‹  A developer writes code and commits the changes to GitHub. This action triggers the Cloud Build pipeline.The Cloud Build pipeline runs according to the stages defined in its YAML configuration file stored in GitHub. These stages include building, testing, scanning, and deploying the application, with all necessary secrets securely retrieved from Secrets Manager.The pipeline performs comprehensive security scans on the code, including both source code and dependency analysis.Trivy executes container scans to ensure security and compliance.Upon successful completion of all pipeline stages, the containerized application is deployed to a Cloud Run resource.  ","version":"Next","tagName":"h3"},{"title":"What Youâ€™ll Learnâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GCP","url":"/ar/projects/devsecops-pipeline-gcp/#what-youll-learn","content":" By working through this guide, youâ€™ll gain hands-on experience building and deploying a secure, cloud-native DevSecOps pipeline on GCP. Specifically, you will learn how to:  Configure and manage GCP resources using Terraform Cloud.Integrate GitHub for version control and pipeline triggers.Use Cloud Build to automate CI/CD processes, including build, test, and deployment stages.Securely manage sensitive information with GCP Secret Manager.Perform security scans on code and dependencies using tools like Trivy.Deploy containerized applications to Cloud Run for scalable, serverless execution.  With all that being stated, Please follow the order of the documents, otherwise you'll most likely run into errors and get lost. ","version":"Next","tagName":"h2"},{"title":"Application Code - Explained","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/application-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/application-code#overview","content":" This section provides a detailed explanation of the application's codebase. The project is a simple Python-based FastAPI application that can be run locally or containerized for deployment. Its primary purpose is to demonstrate a secure and automated DevSecOps pipeline while highlighting potential vulnerabilities for testing purposes.  ","version":"Next","tagName":"h2"},{"title":"Defining GCP-FastAPIâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/application-code#defining-gcp-fastapi","content":" The project sets up a FastAPI application inside a Docker container. It uses the official Python runtime and includes all the necessary configurations to deploy the app efficiently. Upon starting, the container automatically runs the FastAPI app, exposing it on port 80.  The goal of this project is to push it through a DevSecOps pipeline, as it intentionally contains some vulnerabilities. For more details, you can review the code in the main.py file.  ","version":"Next","tagName":"h2"},{"title":"Requirementsâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/application-code#requirements","content":" Docker: For containerizing and running the application.Python 3.12+: The latest stable version ensures compatibility with modern features.FastAPI: Framework for building the API.Uvicorn: ASGI server for running the application.  ","version":"Next","tagName":"h3"},{"title":"Featuresâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/application-code#features","content":" Dockerized Application: Simplifies deployment using containers.Python 3.12.5 Runtime: Ensures compatibility with the latest features and security patches.Optimized Dependency Installation: Leverages requirements.txt for streamlined package management.  ","version":"Next","tagName":"h3"},{"title":"Project Structureâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/application-code#project-structure","content":" gcp-fastapi/ â”œâ”€â”€ Dockerfile # Configuration for the Docker container â”œâ”€â”€ requirements.txt # Python dependencies â”œâ”€â”€ main.py # Entry point for the FastAPI app (contains sample vulnerabilities) â””â”€â”€ ...   ","version":"Next","tagName":"h3"},{"title":"Setup and Installationâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/application-code#setup-and-installation","content":" 1. Clone the Repositoryâ€‹  Clone the project repository using the following command:  git clone https://github.com/your-username/gcp-fastapi.git cd gcp-fastapi   2. Build the Docker Imageâ€‹  Run the following command in the project root to build the Docker image:  docker build -t gcp-fastapi .   3. Run the Docker Containerâ€‹  After building the image, start the container:  docker run -d -p 80:80 gcp-fastapi   This command will start the FastAPI app on port 80 of your localhost.  4. Access the Applicationâ€‹  Once the container is running, you can access the application in your browser:  http://localhost:80   ","version":"Next","tagName":"h3"},{"title":"Dependenciesâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/application-code#dependencies","content":" The application relies on the following Python packages, specified in the requirements.txt file:  fastapi: The main framework for building APIs.uvicorn: The ASGI server for running the application.  To install these dependencies locally, run:  pip install -r requirements.txt   ","version":"Next","tagName":"h3"},{"title":"Notesâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/application-code#notes","content":" The default entry point for the FastAPI application is main.py, where the application instance is named app. If your setup differs, update the CMD directive in the Dockerfile accordingly.By default, the container exposes the application on port 80. To use a different port, modify the EXPOSE and CMD directives in the Dockerfile as needed.  This straightforward setup ensures you can run, test, and deploy the FastAPI application with minimal effort while integrating it into a secure DevSecOps pipeline. ","version":"Next","tagName":"h3"},{"title":"DevSecOps Terraform Code - Explained","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/devsecops-terraform-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"DevSecOps Terraform Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/devsecops-terraform-code#overview","content":" With our environments configured and secrets created, it's time to dive into the Terraform code that defines the DevSecOps pipeline infrastructure. This guide provides a detailed explanation of the critical components so you can fully understand how the system works.  ","version":"Next","tagName":"h2"},{"title":"Code Overviewâ€‹","type":1,"pageTitle":"DevSecOps Terraform Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/devsecops-terraform-code#code-overview","content":" All relevant code is located in the terraform folder, which contains multiple Terraform modules:  Core InfrastructureCI/CD Pipelines  ","version":"Next","tagName":"h2"},{"title":"Core Infrastructureâ€‹","type":1,"pageTitle":"DevSecOps Terraform Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/devsecops-terraform-code#core-infrastructure","content":" This module provisions foundational infrastructure components such as storage, artifact registry, and secret management. It ensures that essential resources are available for secure DevSecOps operations.  Files: main.tf: Defines storage buckets, artifact registries, and secret management resources.variables.tf: Configures input variables, including project ID and region.provider.tf: Configures the Google Cloud provider settings.  ","version":"Next","tagName":"h3"},{"title":"CI/CD Pipelinesâ€‹","type":1,"pageTitle":"DevSecOps Terraform Code - Explained","url":"/ar/projects/devsecops-pipeline-gcp/code-breakdown/devsecops-terraform-code#cicd-pipelines","content":" This module sets up Cloud Build pipelines, IAM roles, and GitHub integration for CI/CD automation. Below are the key elements explained in detail:  Artifact Registry Configurationâ€‹  Resource: google_artifact_registry_repositoryPurpose: Provisions a Docker artifact repository for storing container images.Ensures that all built images are stored securely and version-controlled.Enables seamless integration with Google Cloud Build for CI/CD pipelines.  Cloud Storage Bucket Configurationâ€‹  Resource: google_storage_bucketPurpose: Provisions a Cloud Storage bucket for storing Cloud Build artifacts.Standardizes bucket naming conventions using variables.Ensures secure and centralized storage for build and deployment artifacts.  Secret Management Configurationâ€‹  Resource: google_secret_manager_secretPurpose: Stores sensitive information such as API tokens securely.Manages access control for secrets using IAM policies.Ensures integration with Cloud Build and other services.  FastAPI Pipeline Configurationâ€‹  Module: gcp_python_fastapi_pipelinePurpose: Establishes a CI/CD pipeline for the &quot;GCP FastAPI&quot; project.Leverages the GitHub connection to pull source code from the repository.Integrates the pipeline with the Cloud Storage bucket and Artifact Registry for seamless deployments.  Key Pipeline Parametersâ€‹  GitHub Integration: Dynamically links the GitHub connection to Cloud Build triggers.Configures repository details: Repository: The-DevSec-Blueprint/gcp-python-fastapiBranch: main Build and Deployment: Buildspec: Located at cloudbuild.yaml.Build environment: Machine type: E2_SMALLImage: gcr.io/cloud-builders/gcloudPrivileged mode enabled for containerized builds. Security Scanning: Integrates Snyk for container security scanning.Uses SNYK_TOKEN and project_id variables for authentication.  By understanding the purpose and structure of these Terraform configurations, you'll have a clearer picture of how the DevSecOps pipeline functions, from provisioning infrastructure to enabling secure and automated CI/CD workflows. ","version":"Next","tagName":"h3"},{"title":"Setting Up Terraform Cloud","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-terraform-cloud","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up Terraform Cloud","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-terraform-cloud#overview","content":" Terraform Cloud is an essential tool that simplifies infrastructure management and will quickly become a key part of your workflow. If youâ€™re unfamiliar with Terraform, itâ€™s highly recommended that you take some time to study it before proceeding. This guide will walk you through creating a Terraform Cloud account and setting up your first organization.  ","version":"Next","tagName":"h2"},{"title":"Creating Your Account and Organizationâ€‹","type":1,"pageTitle":"Setting Up Terraform Cloud","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-terraform-cloud#creating-your-account-and-organization","content":" Navigate to the Terraform Cloud Sign-Up Page. Click Continue with HCP account to proceed with registration. Select Sign Up at the bottom of the page, then click Continue with GitHub to link your account. Note: If you are redirected back to the sign-in page, click Continue with HCP and then choose Sign in with GitHub. Once logged in, you should arrive at the Terraform Cloud dashboard. If the &quot;DSB&quot; organization is not already created, it will appear empty: Click Create Organization, and for the organization name, enter DSB. You can rename this organization later as needed after configuring the code. Inside of the organization, create an API-Driven Workspace called dsb-gcp-devsecops-infra. ","version":"Next","tagName":"h2"},{"title":"Setting Up Snyk Account","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-snyk","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up Snyk Account","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-snyk#overview","content":" This guide will help you create a Snyk account and generate the required token and organization ID for scanning purposes. If you're following along with the video, this document covers the steps that were not detailed due to time constraints.  ","version":"Next","tagName":"h2"},{"title":"Account Creation Processâ€‹","type":1,"pageTitle":"Setting Up Snyk Account","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-snyk#account-creation-process","content":" Navigate to the Snyk Sign-Up Page. Sign up using the GitHub option and follow the prompts to create your account. Note: The sign-up process is straightforward and does not require a credit card initially. Once registration is complete, you will be redirected to your Snyk dashboard:  ","version":"Next","tagName":"h2"},{"title":"Obtaining Tokens and Organization IDâ€‹","type":1,"pageTitle":"Setting Up Snyk Account","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-snyk#obtaining-tokens-and-organization-id","content":" Click on your name at the bottom-left corner of the page and select Account Settings. In the General section, locate the Auth Token field. Click Generate to create a token and make note of it. This token will be required later for integration. Navigate to the Settings page, scroll down to find the Organization ID, and note this ID as well. Youâ€™ll need it for configuring your environment.  With these steps completed, your Snyk account is ready to use! ","version":"Next","tagName":"h2"},{"title":"Configuring Secrets and Environment Variables","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/setup/config-secrets-gcp","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Configuring Secrets and Environment Variables","url":"/ar/projects/devsecops-pipeline-gcp/setup/config-secrets-gcp#overview","content":" Now that the foundational setup is complete, this guide will walk you through configuring secrets and environment variables within both Google Cloud Platform (GCP) and Terraform Cloud.  ","version":"Next","tagName":"h2"},{"title":"Terraform Cloud Configurationâ€‹","type":1,"pageTitle":"Configuring Secrets and Environment Variables","url":"/ar/projects/devsecops-pipeline-gcp/setup/config-secrets-gcp#terraform-cloud-configuration","content":" Log in to Terraform Cloud and select the DSB organization. On the left-hand menu, click Settings &gt; Variable Sets. You should see a screen similar to this: Click Create Organization Variable Set, and fill in the following details: Name: Provide a meaningful name for the variable set.Description: Add a brief description for clarity.Variable Set Scope: Select Apply to all projects and workspaces. (You can modify this later if needed.) Scroll down to the Variables section and click Add Variable. Add the following keys, marking them as Sensitive except for TFC_GCP_PROVIDER_AUTH: Variable Name\tDescriptionTFC_GCP_RUN_SERVICE_ACCOUNT_EMAIL\tThe email of the GCP service account that Terraform Cloud will impersonate. TFC_GCP_PROJECT_NUMBER\tThe numeric project ID of your GCP project. TFC_GCP_PROVIDER_AUTH\tSet to true to enable Workload Identity Federation for authentication. TFC_GCP_WORKLOAD_POOL_ID\tThe ID of the Workload Identity Pool created in GCP. TFC_GCP_WORKLOAD_PROVIDER_ID\tThe ID of the OIDC Provider associated with the identity pool. Navigate to the workspace, and click on Variables, and create a Workspace variable named SNYK_TOKEN, making it sensitive. Paste the value of the API Key or token in it and save it. After adding the variables, your variable set should look similar to this:  ","version":"Next","tagName":"h2"},{"title":"GitHub Configurationâ€‹","type":1,"pageTitle":"Configuring Secrets and Environment Variables","url":"/ar/projects/devsecops-pipeline-gcp/setup/config-secrets-gcp#github-configuration","content":" After forking the repositories, you need to configure the necessary secrets for GitHub Actions in the gcp-devsecops-pipeline repository. These secrets will enable automated deployments when updates are pushed to the main branch.  Log in to GitHub and open the gcp-devsecops-pipeline repository.Navigate to Settings &gt; Secrets and Variables under the Security section.Click Actions, then select New Repository Secret.Add the following secrets: GOOGLE_CREDENTIALS: Paste the contents of your GCP service account JSON key file.TF_API_TOKEN: Your Terraform Cloud API token.  With these steps completed, your secrets and environment variables are fully configured for GCP and Terraform Cloud. ","version":"Next","tagName":"h2"},{"title":"Setting Up GitHub Repositories","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-github-repos","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-github-repos#overview","content":" This guide will walk you through the process of setting up GitHub repositories in your personal account. If you're unfamiliar with GitHub, it is a web-based platform that leverages Git, a version control system, to help developers manage and track changes in their code. It also facilitates collaboration on projects, tracks revisions, and enables code contributions from anywhere in the world. For more details, check out this article named Introduction to GitHub on GeeksforGeeks. Plus, GitHub offers free accounts, which is always a bonus!  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-github-repos#prerequisites","content":" Before proceeding, ensure you have a GitHub account. If you don't already have one, follow this guide to create a GitHub account.  ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-github-repos#configuration-steps","content":" ","version":"Next","tagName":"h2"},{"title":"Forking Repositoriesâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-github-repos#forking-repositories","content":" To begin, you'll need to fork the repositories into your personal GitHub account:  Log in to your GitHub account. Navigate to the landing page of the first project: GCP DevSecOps Infrastructure. Click the Fork button in the top-right corner. Select your personal account as the Owner and click Create Fork. Ensure the Copy the main branch only option is enabled. Repeat the above steps for the second project: GCP FastAPI Clone both repositories onto your local machine using the following command, as an example: git clone https://github.com/devsecblueprint/gcp-python-fastapi  ","version":"Next","tagName":"h3"},{"title":"Running the Pipeline and Analyzing Outputs","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/deployment-and-testing/executing-pipeline","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Running the Pipeline and Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-gcp/deployment-and-testing/executing-pipeline#overview","content":" With the infrastructure deployed and verified, the next step is to execute the pipeline and analyze its outputs. This guide will walk you through running the pipeline, reviewing security scan results, and testing the deployed application.  ","version":"Next","tagName":"h2"},{"title":"Running the Pipelineâ€‹","type":1,"pageTitle":"Running the Pipeline and Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-gcp/deployment-and-testing/executing-pipeline#running-the-pipeline","content":" Open the Cloud Build Dashboard, click Triggers, and find to the gh-trigger-gcp-python-fastapi pipeline. Click Run Trigger, then confirm by clicking Run. This action triggers the pipeline to: Pull the latest code from the GitHub repository.Build the project.Run tests and security scans.Deploy the application into the GKE Cluster. Ù…Ù„Ø§Ø­Ø¸Ø© The pipeline process may take 10-30 minutes to complete. Use this time to take a break and return once it finishes.  ","version":"Next","tagName":"h2"},{"title":"Reviewing Resultsâ€‹","type":1,"pageTitle":"Running the Pipeline and Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-gcp/deployment-and-testing/executing-pipeline#reviewing-results","content":" After the pipeline completes, review the results of the security scans. Below are examples from Snyk and Trivy:    Snyk Results    Trivy Results  The Trivy scan results are extensive and might be challenging to address comprehensively. Focus on the most critical issues first. If you want the pipeline to fail for certain vulnerabilities, you can configure the cloudbuild.yaml file in the GCP-FastAPI repository accordingly. Ù…Ù„Ø§Ø­Ø¸Ø© Vulnerabilities may evolve over time, so periodic reviews and updates are essential.  ","version":"Next","tagName":"h2"},{"title":"Testing the API Applicationâ€‹","type":1,"pageTitle":"Running the Pipeline and Analyzing Outputs","url":"/ar/projects/devsecops-pipeline-gcp/deployment-and-testing/executing-pipeline#testing-the-api-application","content":" Open the Cloud Run dashboard and select the gcp-python-fastapi-service. At the top of the screen, you should see the URL to the running service. Copy the provided URL and paste it into your web browser. It should resemble the following: https://gcp-python-fastapi-service-724455289756.us-central1.run.app  ","version":"Next","tagName":"h2"},{"title":"Setting Up Repository Triggers","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-repository-triggers","content":"","keywords":"","version":"Next"},{"title":"Purposeâ€‹","type":1,"pageTitle":"Setting Up Repository Triggers","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-repository-triggers#purpose","content":" Automating the build and deployment process is a critical aspect of modern DevSecOps workflows. By setting up repository triggers in Google Cloud Build, you can ensure that changes to your codebase automatically trigger builds, tests, and deployments, improving efficiency and security. This guide will walk you through configuring repository triggers in Google Cloud to streamline your DevSecOps pipeline.  ","version":"Next","tagName":"h2"},{"title":"Setting Up Repository Triggersâ€‹","type":1,"pageTitle":"Setting Up Repository Triggers","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-repository-triggers#setting-up-repository-triggers","content":" Follow these steps to configure repository triggers in Google Cloud Build:  ","version":"Next","tagName":"h2"},{"title":"1. Connect Your Repositoryâ€‹","type":1,"pageTitle":"Setting Up Repository Triggers","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-repository-triggers#1-connect-your-repository","content":" Log into Google Cloud and navigate to the Cloud Build dashboard.Click on Repositories and select Create a new host connection.Choose GitHub as the host provider and fill out the necessary details:If you are a new user, click Install in a new account to authorize access:Select your GitHub namespace or organization. Once done, your repositories and connections should appear:  ","version":"Next","tagName":"h3"},{"title":"2. Link Your Repositoryâ€‹","type":1,"pageTitle":"Setting Up Repository Triggers","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-repository-triggers#2-link-your-repository","content":" Click the three-dot menu (â‹®) on the right-hand side of the connected repository.Select Link Repositories.Choose the repository you want to create a trigger for and click Link:  ","version":"Next","tagName":"h3"},{"title":"3. Configure Build Triggersâ€‹","type":1,"pageTitle":"Setting Up Repository Triggers","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-repository-triggers#3-configure-build-triggers","content":" Navigate to the Triggers dashboard.Click Connect Repository.Fill out the required details to link your repository.Skip creating a trigger at this stage if you want to manually configure it later:  ","version":"Next","tagName":"h3"},{"title":"Next Stepsâ€‹","type":1,"pageTitle":"Setting Up Repository Triggers","url":"/ar/projects/devsecops-pipeline-gcp/setup/setup-repository-triggers#next-steps","content":" Once your repository is connected, you can define build triggers to automate deployments based on branch updates, pull requests, or tag creations. Fine-tune your configurations to align with your security and compliance requirements.  By implementing repository triggers, you enhance your CI/CD pipeline's efficiency, security, and reliabilityâ€”key principles of a DevSecOps approach. ","version":"Next","tagName":"h2"},{"title":"DevSecOps Pipeline - GitHub Actions","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gha/","content":"","keywords":"","version":"Next"},{"title":"Prerequisitiesâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GitHub Actions","url":"/ar/projects/devsecops-pipeline-gha/#prerequisities","content":" Before you begin this, you will want to have some knowledge of Terraform.You will also want to ensure that you have an GitHub account created. If you don't have a GitHub account created, you can follow the documentation here: Creating A GitHub AccountMake sure you have the following installed on your local machine: PythonGitDocker  ","version":"Next","tagName":"h2"},{"title":"Overviewâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GitHub Actions","url":"/ar/projects/devsecops-pipeline-gha/#overview","content":" So you've decided to go down the path of building a DevSecOps pipeline within GitHub? If so, you've come to the right place! We are going to show you how to build your own DevSecOps pipeline using GitHub Actions. Unlike the any of the other pipelines, we are not going to build and our own infrastructure. We are going to leverage the infrastructure that GitHub has that their Actions rely on.  Luckily for you all, you won't need to do anything. we've taken the liberty of developing all of the code for you. These are the one GitHub repositories that you need to look at before we get started:  DevSecOps Pipeline Infrastructure (Python FastAPI): https://github.com/devsecblueprint/python-fastapi/.github/workflows  ","version":"Next","tagName":"h2"},{"title":"Architecture Diagramâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GitHub Actions","url":"/ar/projects/devsecops-pipeline-gha/#architecture-diagram","content":"     ","version":"Next","tagName":"h2"},{"title":"Architecture Breakdownâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GitHub Actions","url":"/ar/projects/devsecops-pipeline-gha/#architecture-breakdown","content":" At a VERY high level, this architecture briefly covers the services that we will be leveraging for the DevSecOps Pipelines. Here are the descriptions with intent of each service:  GitHub: Acts as the central version control system and CI/CD trigger point. Engineers push changes and raise pull requests here, which kick off the pipeline.GitHub Actions: Orchestrates all automation in the pipeline. It handles build, test, static/dynamic scanning, and image publishing workflows.SonarCloud: Performs Static Application Security Testing (SAST) on the codebase for quality issues and security vulnerabilities before merging.Trivy: Scans the Docker image and dependencies for known vulnerabilities (CVEs) during the build process.ZAP by Checkmarx: Executes Dynamic Application Security Testing (DAST) against the running container to catch runtime vulnerabilities such as XSS, injection flaws, and misconfigurations.Docker: Serves as the containerization platform for building and packaging the application, which is used both for testing and deployment.GitHub Container Registry (GHCR): Stores the final, security-validated Docker image that can be pulled into downstream environments.  ","version":"Next","tagName":"h3"},{"title":"Flow Diagramâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GitHub Actions","url":"/ar/projects/devsecops-pipeline-gha/#flow-diagram","content":" Now that we've covered the architecture diagram, let's put this together so you can understand the flow and who everything is supposed to work.      Flow Diagram Explainedâ€‹  An engineer pushes their changes to the repository and opens a Pull Request (PR).Once the PR is created, SonarCloud automatically scans the codebase for bugs, code smells, and security vulnerabilities. If any critical issues are detected, the PR check will fail.In parallel, GitHub Actions is triggered to build the project or Docker image.Unit tests are executed to validate the functionality of the changes.Upon successful completion of unit tests: An ZAP by Checkmarx scan runs against the running Docker container to detect common web application vulnerabilities.A Trivy scan is also performed to identify vulnerabilities in dependencies and the container image. If all checks pass, a reviewer merges the PR. This triggers the pipeline to run again for the main branch, and the final, verified Docker image is published to GitHub Container Registry.  ","version":"Next","tagName":"h3"},{"title":"What Youâ€™ll Learnâ€‹","type":1,"pageTitle":"DevSecOps Pipeline - GitHub Actions","url":"/ar/projects/devsecops-pipeline-gha/#what-youll-learn","content":" By working through this guide, youâ€™ll gain hands-on experience building and deploying a secure DevSecOps pipeline using GitHub Actions. Specifically, you will learn how to:  Build a secure DevSecOps pipeline using GitHub ActionsAutomate builds, tests, and security scans in pull request workflowsRun SAST with SonarCloudScan containers and dependencies using TrivyPerform DAST with ZAP by CheckmarxPackage applications with Docker and publishing to GHCREnforce security gates before merging codeLeverage GitHubâ€™s hosted infrastructure for CI/CD  With all that being stated, Please follow the order of the documents, otherwise you'll most likely run into errors and get lost. ","version":"Next","tagName":"h2"},{"title":"DevSecOps GitHub Actions Code - Explained","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/devsecops-gha-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"DevSecOps GitHub Actions Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/devsecops-gha-code#overview","content":" Alright, now that youâ€™ve seen how the pipeline flows and what tools weâ€™re using, letâ€™s take a step back and walk through how this thing is actually put together. This section gives you a breakdown of each workflow file so you know whatâ€™s going on under the hood, and how everything ties together to give us a secure, automated CI/CD setup using GitHub Actions.  ","version":"Next","tagName":"h2"},{"title":"Code Overviewâ€‹","type":1,"pageTitle":"DevSecOps GitHub Actions Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/devsecops-gha-code#code-overview","content":" All of the workflow logic lives inside the .github/workflows folder. Each file in this directory is responsible for a specific part of the pipelineâ€”building images, running tests, scanning for vulnerabilities, and pushing to registries. The real magic happens in the main.yml and pr.yml files, which orchestrate the order of operations depending on whether code is being pushed to main or coming in through a pull request.  ","version":"Next","tagName":"h2"},{"title":"Main Workflowâ€‹","type":1,"pageTitle":"DevSecOps GitHub Actions Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/devsecops-gha-code#main-workflow","content":" This is the primary CI/CD pipeline that runs on every push to the main branch or when manually triggered. It chains together the full DevSecOps flowâ€”building the image, checking code quality, running tests and security scans, and pushing the Docker image.  File: main.yml Triggers on push to main or manual invocationSequentially calls build-image, lint-format, unit-sec-test, and push-docker-image  ","version":"Next","tagName":"h3"},{"title":"PR Workflowâ€‹","type":1,"pageTitle":"DevSecOps GitHub Actions Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/devsecops-gha-code#pr-workflow","content":" This workflow runs when a pull request is opened, edited, or synchronized. It runs a subset of the pipeline to validate incoming changes before merging.  File: pr.yml Triggers on pull request eventsExecutes build-image, followed by lint-format, and then unit-sec-test  ","version":"Next","tagName":"h3"},{"title":"Build Image Workflowâ€‹","type":1,"pageTitle":"DevSecOps GitHub Actions Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/devsecops-gha-code#build-image-workflow","content":" This workflow builds the Docker image from the current application source. It is invoked by both the main and PR workflows as an early validation step.  File: build-image.yml Builds a Docker image using the application codeTags the image with the commit SHA  ","version":"Next","tagName":"h3"},{"title":"Linting and Formatting Workflowâ€‹","type":1,"pageTitle":"DevSecOps GitHub Actions Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/devsecops-gha-code#linting-and-formatting-workflow","content":" This workflow ensures Python code quality and consistency by running pylint and black. It enforces coding standards before proceeding to tests or deployment.  File: lint-format.yml Runs pylint on the codebaseRuns black to check for proper code formatting  ","version":"Next","tagName":"h3"},{"title":"Unit & Security Test Workflowâ€‹","type":1,"pageTitle":"DevSecOps GitHub Actions Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/devsecops-gha-code#unit--security-test-workflow","content":" This workflow runs unit tests and performs two types of container security scansâ€”Trivy for static analysis and ZAP by Checkmarx for dynamic application testing.  File: unit-sec-test.yml Unit tests: Executes pytest after installing dependenciesTrivy scans: Detects high/critical vulnerabilities in the built Docker imageZAP by Checkmarx: Runs a DAST scan against the running container  ","version":"Next","tagName":"h3"},{"title":"Push Docker Image Workflowâ€‹","type":1,"pageTitle":"DevSecOps GitHub Actions Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/devsecops-gha-code#push-docker-image-workflow","content":" This workflow builds and tags a Docker image, then pushes it to GitHub Container Registry (GHCR). It's triggered after all validations have passed.  File: push-docker-image.yml Builds the imageTags it with the commit SHA, which is latestPushes to GHCR  Once you understand the purpose and layout of these Terraform configs, you'll have a solid grasp on how the DevSecOps pipeline works from spinning up infrastructure to running secure, automated CI/CD workflows. ","version":"Next","tagName":"h3"},{"title":"Setting Up SonarCloud","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gha/setup/setup-sonarcloud","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up SonarCloud","url":"/ar/projects/devsecops-pipeline-gha/setup/setup-sonarcloud#overview","content":" This guide will walk you through the process of integrating SonarCloud into your GitHub repository. If you're unfamiliar with SonarCloud, it's a cloud-based code quality and security service that performs Static Application Security Testing (SAST). It helps identify bugs, vulnerabilities, and code smells in your applicationâ€”before they make it to production. SonarCloud seamlessly integrates with GitHub and supports over 25 programming languages. For more background, check out this article: SonarCloud Documentation. Plus, SonarCloud is free, which is a HUGE plus.  ","version":"Next","tagName":"h2"},{"title":"Instructionsâ€‹","type":1,"pageTitle":"Setting Up SonarCloud","url":"/ar/projects/devsecops-pipeline-gha/setup/setup-sonarcloud#instructions","content":" Go to the SonarCloud Login Page. Click Sign in with GitHub to create your SonarCloud account. After signing in, youâ€™ll be prompted to install the SonarCloud GitHub App. Select your GitHub account or organization and proceed with the installation. Once installed, youâ€™ll land on the Analyze Projects screen. Select the python-fastapi repository to import it into SonarCloud. After importing, youâ€™ll be redirected to your project dashboard. It may look empty at firstâ€”but once your pipeline runs, itâ€™ll populate with results like this:  ","version":"Next","tagName":"h2"},{"title":"Configuring Deployment Service Account in GCP","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gcp/setup/configuring-deployment-user-gcp","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Configuring Deployment Service Account in GCP","url":"/ar/projects/devsecops-pipeline-gcp/setup/configuring-deployment-user-gcp#overview","content":" This guide walks you through setting up a Google Cloud Platform (GCP) Service Account for Terraform Cloud deployments using OIDC (OpenID Connect) via Workload Identity Federation (WIF). This setup allows Terraform Cloud to securely authenticate to GCP without relying on long-lived service account keys, ultimately enabling short-lived, scoped credentials per workspace.  âœ… Before you begin: Ensure you have a GCP project available and sufficient permissions to create service accounts and manage IAM.  ","version":"Next","tagName":"h2"},{"title":"Step 1: Create and Configure the Service Accountâ€‹","type":1,"pageTitle":"Configuring Deployment Service Account in GCP","url":"/ar/projects/devsecops-pipeline-gcp/setup/configuring-deployment-user-gcp#step-1-create-and-configure-the-service-account","content":" Log in to your GCP account. In the left-hand menu, go to: IAM &amp; Admin â†’ Service Accounts Click Create Service Account, and fill in: Name: terraform-deployerDescription: (optional)Click Create and Continue Grant the following roles to the service account: EditorProject IAM AdminRole AdministratorSecret Manager AdminSecret Manager Secret AccessorIAM Service Account Token Creator Under Grant users access to this service account, leave it blank and click Done.  ","version":"Next","tagName":"h2"},{"title":"Step 2: Configure Workload Identity Federationâ€‹","type":1,"pageTitle":"Configuring Deployment Service Account in GCP","url":"/ar/projects/devsecops-pipeline-gcp/setup/configuring-deployment-user-gcp#step-2-configure-workload-identity-federation","content":" In the GCP Console, go to IAM &amp; Admin â†’ Workload Identity Federation Click Create Pool and enter the following: Pool Name: Terraform CloudPool ID: terraform-cloudâœ… Check the box: Enabled Provider In the same flow, add a new provider: Provider Type: OIDCProvider Name: defaultIssuer URL: https://app.terraform.ioAudiences: Select Default Audience Configure Attribute Mappings: Google Attribute\tOIDC Assertionattribute.terraform_full_workspace\tassertion.terraform_full_workspace google.sub\tassertion.sub attribute.terraform_workspace\tassertion.terraform_workspace_id Add Attribute Condition: assertion.terraform_organization_name == &quot;DSB&quot; Click Create to finalize the pool and provider.  ","version":"Next","tagName":"h2"},{"title":"Step 3: Grant Access via Impersonationâ€‹","type":1,"pageTitle":"Configuring Deployment Service Account in GCP","url":"/ar/projects/devsecops-pipeline-gcp/setup/configuring-deployment-user-gcp#step-3-grant-access-via-impersonation","content":" After the pool and provider are created, click Grant Access. Choose Grant access using service account impersonation. Select the previously created service account (terraform-deployer). Add the Terraform Workspace Principal: Use your workspace ID to define the principal.Example format: principalSet://iam.googleapis.com/projects/&lt;project-number&gt;/locations/global/workloadIdentityPools/terraform-cloud/attribute.terraform_workspace_id/&lt;workspace-id&gt;   With this setup complete, Terraform Cloud will now be able to authenticate to GCP using OIDC and impersonate the terraform-deployer service account during runs â€” without the need for storing or rotating service account keys.  You can now move on to configuring your Terraform provider and environment variables. ","version":"Next","tagName":"h2"},{"title":"Pipeline Execution & Results","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gha/execution-pipeline-results","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Pipeline Execution & Results","url":"/ar/projects/devsecops-pipeline-gha/execution-pipeline-results#overview","content":" With your infrastructure deployed and everything wired up, itâ€™s time to put the pipeline to work and see it in action. This section walks you through running the pipeline, checking the results from the various security scans, and verifying that your Docker image has been successfully published to GitHub Container Registry (GHCR).  ","version":"Next","tagName":"h2"},{"title":"Running the Pipelineâ€‹","type":1,"pageTitle":"Pipeline Execution & Results","url":"/ar/projects/devsecops-pipeline-gha/execution-pipeline-results#running-the-pipeline","content":" Head over to your python-fastapi repository on GitHub and click the Actions tab at the top. In the left-hand sidebar, select Main Workflow, then click Run workflow on the right. Once triggered, your pipeline will kick off and begin executing. It should look something like this: Note: This job takes about 5â€“7 minutes to complete, so go grab a coffee and check back in a bit.  ","version":"Next","tagName":"h2"},{"title":"Reviewing the Resultsâ€‹","type":1,"pageTitle":"Pipeline Execution & Results","url":"/ar/projects/devsecops-pipeline-gha/execution-pipeline-results#reviewing-the-results","content":" After the workflow finishes running, hereâ€™s how to review the key outputs from your DevSecOps pipeline:  ","version":"Next","tagName":"h2"},{"title":"GitHub Container Registry (GHCR)â€‹","type":1,"pageTitle":"Pipeline Execution & Results","url":"/ar/projects/devsecops-pipeline-gha/execution-pipeline-results#github-container-registry-ghcr","content":" Back on your repositoryâ€™s homepage, scroll down to the Packages section. You should see your Docker image listed there. Click the image name to view details, including how to pull it using Docker.     ","version":"Next","tagName":"h3"},{"title":"SonarCloud Analysisâ€‹","type":1,"pageTitle":"Pipeline Execution & Results","url":"/ar/projects/devsecops-pipeline-gha/execution-pipeline-results#sonarcloud-analysis","content":" If youâ€™ve properly integrated SonarCloud, your repository should be scanned automatically as part of the pipeline. Navigate to your SonarCloud dashboard to explore:  Code SmellsVulnerabilitiesSecurity Hotspots  Feel free to experiment by adding some insecure code or edge cases to test the scanner. You can also customize your Quality Gates directly from the SonarCloud interface.     ","version":"Next","tagName":"h3"},{"title":"ZAP by Checkmarx Scanâ€‹","type":1,"pageTitle":"Pipeline Execution & Results","url":"/ar/projects/devsecops-pipeline-gha/execution-pipeline-results#zap-by-checkmarx-scan","content":" The results of the ZAP by Checkmarx scan can be found directly in the GitHub Actions logs. This scan runs against your running Docker container to detect common web vulnerabilities like injection flaws, insecure headers, and more.  Hereâ€™s an example of what it looks like in the workflow logs:   ","version":"Next","tagName":"h3"},{"title":"Trivy Scan Resultsâ€‹","type":1,"pageTitle":"Pipeline Execution & Results","url":"/ar/projects/devsecops-pipeline-gha/execution-pipeline-results#trivy-scan-results","content":" Trivy scan results are automatically uploaded to GitHub under the Security tab â†’ Code scanning alerts. From there, youâ€™ll be able to view any critical or high-severity vulnerabilities identified in your image.    ","version":"Next","tagName":"h3"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"Pipeline Execution & Results","url":"/ar/projects/devsecops-pipeline-gha/execution-pipeline-results#conclusion","content":" You're done!!! Youâ€™ve successfully executed your GitHub Actions DevSecOps pipeline. Youâ€™ve built, scanned, tested, and pushed a containerized app with security built in from the start. ","version":"Next","tagName":"h2"},{"title":"Setting Up GitHub Repositories","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gha/setup/setup-github-repos","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-gha/setup/setup-github-repos#overview","content":" This guide will walk you through the process of setting up GitHub repositories in your personal account. If you're unfamiliar with GitHub, it is a web-based platform that leverages Git, a version control system, to help developers manage and track changes in their code. It also facilitates collaboration on projects, tracks revisions, and enables code contributions from anywhere in the world. For more details, check out this article named Introduction to GitHub on GeeksforGeeks. Plus, GitHub offers free accounts, which is always a bonus!  ","version":"Next","tagName":"h2"},{"title":"Prerequisitesâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-gha/setup/setup-github-repos#prerequisites","content":" Before proceeding, ensure you have a GitHub account. If you don't already have one, follow this guide to create a GitHub account.  ","version":"Next","tagName":"h2"},{"title":"Configuration Stepsâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-gha/setup/setup-github-repos#configuration-steps","content":" ","version":"Next","tagName":"h2"},{"title":"Forking Repositoriesâ€‹","type":1,"pageTitle":"Setting Up GitHub Repositories","url":"/ar/projects/devsecops-pipeline-gha/setup/setup-github-repos#forking-repositories","content":" To begin, you'll need to fork the repositories into your personal GitHub account:  Log in to your GitHub account. Navigate to the landing page of the project: python-fastapi. Click the Fork button in the top-right corner. Select your personal account as the Owner and click Create Fork. Ensure the Copy the main branch only option is enabled. Clone the repository onto your local machine using the following command, as an example: git clone https://github.com/devsecblueprint/python-fastapi  ","version":"Next","tagName":"h3"},{"title":"Application Code - Explained","type":0,"sectionRef":"#","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/application-code","content":"","keywords":"","version":"Next"},{"title":"Overviewâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/application-code#overview","content":" This section provides a detailed explanation of the application's codebase. The project is a simple Python-based FastAPI application that can be run locally or containerized for deployment. Its primary purpose is to demonstrate a secure and automated DevSecOps pipeline while highlighting potential vulnerabilities for testing purposes.  ","version":"Next","tagName":"h2"},{"title":"Defining python-fastapiâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/application-code#defining-python-fastapi","content":" The project sets up a FastAPI application inside a Docker container. It uses the official Python runtime and includes all the necessary configurations to deploy the app efficiently. Upon starting, the container automatically runs the FastAPI app, exposing it on port 80.  The goal of this project is to push it through a DevSecOps pipeline, as it intentionally contains some vulnerabilities. For more details, you can review the code in the main.py file.  ","version":"Next","tagName":"h2"},{"title":"Requirementsâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/application-code#requirements","content":" Docker: For containerizing and running the application.Python 3.12+: The latest stable version ensures compatibility with modern features.FastAPI: Framework for building the API.Uvicorn: ASGI server for running the application.  ","version":"Next","tagName":"h3"},{"title":"Featuresâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/application-code#features","content":" Dockerized Application: Simplifies deployment using containers.Python 3.12.5 Runtime: Ensures compatibility with the latest features and security patches.Optimized Dependency Installation: Leverages requirements.txt for streamlined package management.  ","version":"Next","tagName":"h3"},{"title":"Project Structureâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/application-code#project-structure","content":" python-fastapi/ â”œâ”€â”€ Dockerfile # Configuration for the Docker container â”œâ”€â”€ requirements.txt # Python dependencies â”œâ”€â”€ main.py # Entry point for the FastAPI app (contains sample vulnerabilities) â””â”€â”€ ...   ","version":"Next","tagName":"h3"},{"title":"Setup and Installationâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/application-code#setup-and-installation","content":" 1. Clone the Repositoryâ€‹  Clone the project repository using the following command:  git clone https://github.com/your-username/python-fastapi.git cd python-fastapi   2. Build the Docker Imageâ€‹  Run the following command in the project root to build the Docker image:  docker build -t python-fastapi .   3. Run the Docker Containerâ€‹  After building the image, start the container:  docker run -d -p 80:80 python-fastapi   This command will start the FastAPI app on port 80 of your localhost.  4. Access the Applicationâ€‹  Once the container is running, you can access the application in your browser:  http://localhost:80   ","version":"Next","tagName":"h3"},{"title":"Dependenciesâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/application-code#dependencies","content":" The application relies on the following Python packages, specified in the requirements.txt file:  fastapi: The main framework for building APIs.uvicorn: The ASGI server for running the application.  To install these dependencies locally, run:  pip install -r requirements.txt   ","version":"Next","tagName":"h3"},{"title":"Notesâ€‹","type":1,"pageTitle":"Application Code - Explained","url":"/ar/projects/devsecops-pipeline-gha/code-breakdown/application-code#notes","content":" The default entry point for the FastAPI application is main.py, where the application instance is named app. If your setup differs, update the CMD directive in the Dockerfile accordingly.By default, the container exposes the application on port 80. To use a different port, modify the EXPOSE and CMD directives in the Dockerfile as needed.  This straightforward setup ensures you can run, test, and deploy the FastAPI application with minimal effort while integrating it into a secure DevSecOps pipeline. ","version":"Next","tagName":"h3"}],"options":{"id":"default"}}